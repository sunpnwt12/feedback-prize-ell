{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHIRa3vWKyBe",
        "outputId": "1209e635-ace1-4341-a237-175409b4adef"
      },
      "id": "DHIRa3vWKyBe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 16 12:56:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q transformers==4.20.1\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q kaggle --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka6Aoj2GGP_B",
        "outputId": "ae9edd64-7aec-4591-816c-4d7837f9228b"
      },
      "id": "ka6Aoj2GGP_B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[K     |████████████████▎               | 834.1 MB 1.2 MB/s eta 0:10:46tcmalloc: large alloc 1147494400 bytes == 0x3a528000 @  0x7f8953856615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████▋           | 1055.7 MB 1.2 MB/s eta 0:07:49tcmalloc: large alloc 1434370048 bytes == 0x7eb7e000 @  0x7f8953856615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |██████████████████████████▏     | 1336.2 MB 1.3 MB/s eta 0:03:58tcmalloc: large alloc 1792966656 bytes == 0x39b0000 @  0x7f8953856615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████████████████| 1636.9 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1636958208 bytes == 0x6e798000 @  0x7f89538551e7 0x4b2590 0x4b261c 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6\n",
            "tcmalloc: large alloc 2046197760 bytes == 0xd00b8000 @  0x7f8953856615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x4bad99 0x4d3249\n",
            "\u001b[K     |████████████████████████████████| 1637.0 MB 6.7 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.12.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 4.0 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.11.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0+cu113) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (2.10)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.1+cu113\n",
            "    Uninstalling torchaudio-0.12.1+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0+cu113 torchaudio-0.11.0+cu113 torchvision-0.12.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 36.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 67.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 52.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data = True\n",
        "import os\n",
        "if os.path.exists('/content/data'):\n",
        "    prepare_data = False"
      ],
      "metadata": {
        "id": "VILXvrhfLNjY"
      },
      "id": "VILXvrhfLNjY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if prepare_data:    \n",
        "    from google.colab import files, drive\n",
        "    \n",
        "    # Upload your kaggle.json (kaggle api)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for fn in uploaded.keys():\n",
        "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "            name=fn, length=len(uploaded[fn])))\n",
        "    \n",
        "    # Then move kaggle.json into the folder where the API expects to find it.\n",
        "    !mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    !mkdir data && cd data && kaggle competitions download -c feedback-prize-english-language-learning\n",
        "    !unzip /content/data/feedback-prize-english-language-learning.zip -d /content/data/\n",
        "\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "gs8PdbNdMAbu",
        "outputId": "b047a081-22f9-4d70-cb9e-d93a7aaac58c"
      },
      "id": "gs8PdbNdMAbu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-05cd313c-0fbb-475c-94d8-6b64d633b693\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-05cd313c-0fbb-475c-94d8-6b64d633b693\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 65 bytes\n",
            "Downloading feedback-prize-english-language-learning.zip to /content/data\n",
            "100% 2.80M/2.80M [00:01<00:00, 2.21MB/s]\n",
            "100% 2.80M/2.80M [00:01<00:00, 1.71MB/s]\n",
            "Archive:  /content/data/feedback-prize-english-language-learning.zip\n",
            "  inflating: /content/data/sample_submission.csv  \n",
            "  inflating: /content/data/test.csv  \n",
            "  inflating: /content/data/train.csv  \n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087ef4e2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:27.143155Z",
          "iopub.status.busy": "2022-10-18T08:02:27.142438Z",
          "iopub.status.idle": "2022-10-18T08:02:45.652436Z",
          "shell.execute_reply": "2022-10-18T08:02:45.650933Z"
        },
        "papermill": {
          "duration": 18.519494,
          "end_time": "2022-10-18T08:02:45.654625",
          "exception": false,
          "start_time": "2022-10-18T08:02:27.135131",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087ef4e2",
        "outputId": "d1e4e3f5-6309-45f9-eee1-d0aca654fc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python version: 3.7.15 (default, Oct 12 2022, 19:14:55) \n",
            "[GCC 7.5.0]\n",
            "iterstart version: 0.1.6\n",
            "torch version: 1.11.0+cu113\n",
            "transfromers version: 4.20.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import pytz\n",
        "import gc; gc.enable()\n",
        "import random\n",
        "import warnings\n",
        "import yaml\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "from tabulate import tabulate\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(f'python version: {sys.version}') \n",
        "\n",
        "os.system('pip install -q iterative-stratification==0.1.7')\n",
        "import iterstrat\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "print(f'iterstart version: {iterstrat.__version__}')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "\n",
        "print(f'torch version: {torch.__version__}')\n",
        "\n",
        "from torchmetrics.functional import mean_squared_error\n",
        "\n",
        "# os.system('pip install --root-user-action=ignore --force-reinstall transformers==4.22.1')\n",
        "import transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorWithPadding\n",
        "print(f'transfromers version: {transformers.__version__}')\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e29ee0",
      "metadata": {
        "papermill": {
          "duration": 0.006228,
          "end_time": "2022-10-18T08:02:45.878028",
          "exception": false,
          "start_time": "2022-10-18T08:02:45.871800",
          "status": "completed"
        },
        "tags": [],
        "id": "07e29ee0"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608cdbbd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:45.892325Z",
          "iopub.status.busy": "2022-10-18T08:02:45.891376Z",
          "iopub.status.idle": "2022-10-18T08:02:45.956165Z",
          "shell.execute_reply": "2022-10-18T08:02:45.955230Z"
        },
        "papermill": {
          "duration": 0.074274,
          "end_time": "2022-10-18T08:02:45.958422",
          "exception": false,
          "start_time": "2022-10-18T08:02:45.884148",
          "status": "completed"
        },
        "tags": [],
        "id": "608cdbbd"
      },
      "outputs": [],
      "source": [
        "class BASICCONF:\n",
        "    seed = 12\n",
        "    \n",
        "    data_path = '/content/data'\n",
        "    \n",
        "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    num_labels = 6\n",
        "    num_folds = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c985864",
      "metadata": {
        "papermill": {
          "duration": 0.005939,
          "end_time": "2022-10-18T08:02:45.970719",
          "exception": false,
          "start_time": "2022-10-18T08:02:45.964780",
          "status": "completed"
        },
        "tags": [],
        "id": "0c985864"
      },
      "source": [
        "# Seeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce80e59e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:45.984077Z",
          "iopub.status.busy": "2022-10-18T08:02:45.983777Z",
          "iopub.status.idle": "2022-10-18T08:02:45.991572Z",
          "shell.execute_reply": "2022-10-18T08:02:45.990678Z"
        },
        "papermill": {
          "duration": 0.016748,
          "end_time": "2022-10-18T08:02:45.993528",
          "exception": false,
          "start_time": "2022-10-18T08:02:45.976780",
          "status": "completed"
        },
        "tags": [],
        "id": "ce80e59e"
      },
      "outputs": [],
      "source": [
        "#https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n",
        "def seed_everything(seed: int):    \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything(BASICCONF.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3fe388",
      "metadata": {
        "papermill": {
          "duration": 0.005947,
          "end_time": "2022-10-18T08:02:46.005518",
          "exception": false,
          "start_time": "2022-10-18T08:02:45.999571",
          "status": "completed"
        },
        "tags": [],
        "id": "ec3fe388"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb756b2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:45.670194Z",
          "iopub.status.busy": "2022-10-18T08:02:45.668343Z",
          "iopub.status.idle": "2022-10-18T08:02:45.862831Z",
          "shell.execute_reply": "2022-10-18T08:02:45.861842Z"
        },
        "papermill": {
          "duration": 0.204007,
          "end_time": "2022-10-18T08:02:45.865207",
          "exception": false,
          "start_time": "2022-10-18T08:02:45.661200",
          "status": "completed"
        },
        "tags": [],
        "id": "9eb756b2"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = f'{BASICCONF.data_path}/train.csv'\n",
        "TEST_PATH = f'{BASICCONF.data_path}/test.csv'\n",
        "SAMP_SUB = f'{BASICCONF.data_path}/sample_submission.csv'\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "samp_sup = pd.read_csv(SAMP_SUB)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc83bcf4",
      "metadata": {
        "papermill": {
          "duration": 0.005878,
          "end_time": "2022-10-18T08:02:46.017627",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.011749",
          "status": "completed"
        },
        "tags": [],
        "id": "bc83bcf4"
      },
      "source": [
        "## split CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf7df74a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.032077Z",
          "iopub.status.busy": "2022-10-18T08:02:46.031779Z",
          "iopub.status.idle": "2022-10-18T08:02:46.174635Z",
          "shell.execute_reply": "2022-10-18T08:02:46.173646Z"
        },
        "papermill": {
          "duration": 0.15276,
          "end_time": "2022-10-18T08:02:46.176592",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.023832",
          "status": "completed"
        },
        "tags": [],
        "id": "cf7df74a",
        "outputId": "54e62cab-08a6-4a1f-ce83-2b167f338ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fold\n",
              "0    978\n",
              "1    977\n",
              "2    978\n",
              "3    978\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def split_cv(conf, df_):\n",
        "    df = df_.copy(deep=True)\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=conf.num_folds, shuffle=True, random_state=conf.seed)\n",
        "    y = df[conf.target_cols]\n",
        "\n",
        "    for n, (train_index, valid_index) in enumerate(mskf.split(df, y)):\n",
        "        df.loc[ｖalid_index, 'fold'] = int(n)\n",
        "    \n",
        "    df['fold'] = df['fold'].astype(int)\n",
        "        \n",
        "    return df\n",
        "        \n",
        "# train_df = train_df.drop(train_df[train_df['text_id'] == 'F69C85F4C3CA'].index).reset_index(drop=True)\n",
        "df = split_cv(BASICCONF, train_df)\n",
        "df.groupby('fold').size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84d6305",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.191599Z",
          "iopub.status.busy": "2022-10-18T08:02:46.190216Z",
          "iopub.status.idle": "2022-10-18T08:02:46.195586Z",
          "shell.execute_reply": "2022-10-18T08:02:46.194729Z"
        },
        "papermill": {
          "duration": 0.014308,
          "end_time": "2022-10-18T08:02:46.197386",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.183078",
          "status": "completed"
        },
        "tags": [],
        "id": "d84d6305"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
        "# all_text = df['full_text']\n",
        "# def ft_array_dist(full_texts, tokenizer):\n",
        "#     lengths = []\n",
        "#     for text in full_texts.fillna('').values:\n",
        "#         length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
        "#         lengths.append(length)\n",
        "#     lengths = np.array(lengths)\n",
        "#     return lengths\n",
        "\n",
        "# all_text_l = ft_array_dist(all_text, tokenizer)\n",
        "# plt.hist(all_text_l, 40)\n",
        "# ft_fold = df[df['fold'] != 1]['full_text']\n",
        "# l_fold = ft_array_dist(ft_fold, tokenizer)\n",
        "# plt.hist(l_fold, 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "688a18b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.212236Z",
          "iopub.status.busy": "2022-10-18T08:02:46.210849Z",
          "iopub.status.idle": "2022-10-18T08:02:46.221284Z",
          "shell.execute_reply": "2022-10-18T08:02:46.220478Z"
        },
        "papermill": {
          "duration": 0.019573,
          "end_time": "2022-10-18T08:02:46.223214",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.203641",
          "status": "completed"
        },
        "tags": [],
        "id": "688a18b3"
      },
      "outputs": [],
      "source": [
        "class FB3Dataset(Dataset):\n",
        "    def __init__(self, conf, df, tokenizer, fold):\n",
        "        self.labels = df[conf.target_cols].reset_index(drop=True)\n",
        "        self.full_texts = df['full_text'].reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        if conf.dynamic_max_len:\n",
        "            self.max_len = self._get_max_len()\n",
        "        else:\n",
        "            self.max_len = conf.static_max_len_list[fold]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.full_texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        token = self._get_token(idx)\n",
        "        label = self._get_label(idx)\n",
        "        \n",
        "        return token, label\n",
        "    \n",
        "    def _get_label(self, idx):\n",
        "        return torch.tensor(self.labels.loc[idx].values, dtype=torch.float)\n",
        "    \n",
        "    def _get_token(self, idx):\n",
        "        tokenized = self.tokenizer(\n",
        "                        self.full_texts.loc[idx],\n",
        "                        add_special_tokens=True,\n",
        "                        max_length=self.max_len,\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        return_tensors=None,\n",
        "                )\n",
        "        return {k: torch.tensor(v, dtype=torch.long) for k, v in tokenized.items()} # stack tensor\n",
        "    \n",
        "    # get longest max_len\n",
        "    # https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train/notebook\n",
        "    def _get_max_len(self):\n",
        "        lengths = []\n",
        "        for text in self.full_texts.fillna('').values:\n",
        "            length = len(self.tokenizer(text, add_special_tokens=False)['input_ids'])\n",
        "            lengths.append(length)\n",
        "        return max(lengths) + 2 # cls & sep"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f364205",
      "metadata": {
        "papermill": {
          "duration": 0.005945,
          "end_time": "2022-10-18T08:02:46.235501",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.229556",
          "status": "completed"
        },
        "tags": [],
        "id": "3f364205"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2addf04e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.249475Z",
          "iopub.status.busy": "2022-10-18T08:02:46.249226Z",
          "iopub.status.idle": "2022-10-18T08:02:46.265960Z",
          "shell.execute_reply": "2022-10-18T08:02:46.265076Z"
        },
        "papermill": {
          "duration": 0.026287,
          "end_time": "2022-10-18T08:02:46.268053",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.241766",
          "status": "completed"
        },
        "tags": [],
        "id": "2addf04e"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/maunish/clrp-pytorch-roberta-finetune/notebook\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, in_features, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.middle_features = hidden_dim\n",
        "        self.W = nn.Linear(in_features, hidden_dim)\n",
        "        self.V = nn.Linear(hidden_dim, 1)\n",
        "        self.out_features = hidden_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        att = torch.tanh(self.W(features))\n",
        "        score = self.V(att)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "# https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/361678\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim),\n",
        "            nn.LayerNorm(in_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(in_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        w = self.attention(x).float()\n",
        "        w[mask==0]=float('-inf')\n",
        "        w = torch.softmax(w,1)\n",
        "        x = torch.sum(w * x, dim=1)\n",
        "        return x    \n",
        "    \n",
        "# https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently\n",
        "class HiddenAttentionPooling(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_size, hiddendim_fc):\n",
        "        super().__init__()\n",
        "        self.num_hidden_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hiddendim_fc = hiddendim_fc\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        q_t = torch.normal(mean=0.0, std=0.02, size=(1, self.hidden_size))\n",
        "        self.q = nn.Parameter(q_t).float()\n",
        "        w_ht = torch.normal(mean=0.0, std=0.02, size=(self.hidden_size, self.hiddendim_fc))\n",
        "        self.w_h = nn.Parameter(w_ht).float()\n",
        "\n",
        "    def forward(self, all_hidden_states):\n",
        "        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n",
        "                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n",
        "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
        "        out = self.attention(hidden_states)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "    def attention(self, h):\n",
        "        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n",
        "        v = F.softmax(v, -1)\n",
        "        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n",
        "        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n",
        "        return v\n",
        "\n",
        "class ConcatPooling(nn.Module):\n",
        "    def __init__(self, pooling_last=4):\n",
        "        super().__init__()\n",
        "        self.pooling_last = pooling_last\n",
        "        \n",
        "    def forward(self, all_hidden_states):\n",
        "        concat_pooling = torch.cat(tuple(all_hidden_states[-l] for l in range(1, self.pooling_last + 1)), -1)\n",
        "#         concat_pooling = concat_pooling.mean(dim=1) # average instead of select only one\n",
        "        concat_pooling = concat_pooling[:, 0] # select the first one\n",
        "        return concat_pooling\n",
        "\n",
        "# https://www.kaggle.com/competitions/google-quest-challenge/discussion/129840\n",
        "class WeightedLayerPooling(nn.Module):\n",
        "    def __init__(self, num_layers=12, init_std=0.02):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        weights_init = torch.zeros(self.num_layers).float()\n",
        "        weights_init.data[:-1] = -3\n",
        "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, all_hidden_states):\n",
        "        all_layer_encoders = torch.stack(\n",
        "            [self.dropout(layer) for layer in all_hidden_states[-self.num_layers:]], dim=0\n",
        "        )\n",
        "        averaged_layers = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * all_layer_encoders).sum(0)\n",
        "        return averaged_layers\n",
        "        \n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "        return mean_embeddings\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, conf, fold_num, config_path=None):\n",
        "        super().__init__()\n",
        "        if not config_path:\n",
        "            self.model_conf = AutoConfig.from_pretrained(conf.model_name, output_hidden_states=True)\n",
        "            self.model_conf = self._set_dropout(self.model_conf)\n",
        "            self.backbone = AutoModel.from_pretrained(conf.model_name, config=self.model_conf)\n",
        "        else:\n",
        "            self.model_conf = torch.load(config_path)\n",
        "            self.backbone = AutoModel.from_config(self.model_conf)\n",
        "        if conf.gradient_checkpointing:\n",
        "            self.backbone.gradient_checkpointing_enable()\n",
        "        \n",
        "        if not config_path:\n",
        "            for layer in self.backbone.encoder.layer[-conf.reinit_last_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    self._init_weights(module)\n",
        "                    \n",
        "        self.pooling_strategy = conf.pooling_strategy_list[fold_num]\n",
        "        if self.pooling_strategy == 'mean_pooling':\n",
        "            self.pooler = MeanPooling()\n",
        "            \n",
        "        elif self.pooling_strategy == 'concat_pooling':\n",
        "            self.pooler = ConcatPooling(conf.concat_pooling_last)\n",
        "            \n",
        "        elif self.pooling_strategy == 'attn_pooling': \n",
        "            self.attn_pooler = AttentionPooling(self.model_conf.hidden_size) \n",
        "            for attn_module in self.attn_pooler.modules():\n",
        "                self._init_weights(attn_module)\n",
        "            \n",
        "        elif self.pooling_strategy == 'wlp_attn_pooling':\n",
        "            self.wlp_pooler = WeightedLayerPooling(self.model_conf.num_hidden_layers, self.model_conf.initializer_range)\n",
        "            self.attn_pooler = AttentionPooling(self.model_conf.hidden_size)\n",
        "            for attn_module in self.attn_pooler.modules():\n",
        "                self._init_weights(attn_module)\n",
        "                \n",
        "        elif self.pooling_strategy == 'concat_h_attn_mean_pooling':\n",
        "            self.hattn_pooler = HiddenAttentionPooling(self.model_conf.num_hidden_layers, self.model_conf.hidden_size, self.model_conf.hidden_size)\n",
        "            self.mean_pooler = MeanPooling()\n",
        "\n",
        "        elif self.pooling_strategy == 'concat_attn_mean_pooling':\n",
        "            self.attn_pooler = AttentionPooling(self.model_conf.hidden_size)\n",
        "            for attn_module in self.attn_pooler.modules():\n",
        "                self._init_weights(attn_module)\n",
        "            self.mean_pooler = MeanPooling()\n",
        "            \n",
        "        else:\n",
        "            raise Exception('Invalid pooling strategy')\n",
        "\n",
        "        if self.pooling_strategy in ['mean_pooling', 'attn_pooling', 'wlp_attn_pooling']:\n",
        "            hidden_size = self.model_conf.hidden_size\n",
        "        elif self.pooling_strategy in ['concat_pooling']:\n",
        "            hidden_size = self.model_conf.hidenn_size * conf.concat_pooling_last\n",
        "        elif self.pooling_strategy in ['concat_h_attn_mean_pooling', 'concat_attn_mean_pooling']:\n",
        "            hidden_size = self.model_conf.hidden_size * 2\n",
        "        else:\n",
        "            raise Exception('Cannot create fc layer.')\n",
        "            \n",
        "        self.multi_dropout = conf.multi_dropout\n",
        "        if self.multi_dropout:\n",
        "            self.dropout1 = nn.Dropout(conf.multi_dropout_p[0])\n",
        "            self.dropout2 = nn.Dropout(conf.multi_dropout_p[1])\n",
        "            self.dropout3 = nn.Dropout(conf.multi_dropout_p[2])\n",
        "            self.dropout4 = nn.Dropout(conf.multi_dropout_p[3])\n",
        "            self.dropout5 = nn.Dropout(conf.multi_dropout_p[4])\n",
        "        else:\n",
        "            self.dropout0 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, conf.num_labels)\n",
        "        if conf.reinit_method is not None:\n",
        "            self._init_weights2_([self.fc], conf.reinit_method)\n",
        "        else:\n",
        "            self._init_weights(self.fc)\n",
        "\n",
        "        self.use_ln = conf.use_ln\n",
        "        if self.use_ln:\n",
        "            self.ln = nn.LayerNorm(hidden_size)\n",
        "            self._init_weights(self.ln)\n",
        "        \n",
        "    def _set_dropout(self, conf, ratio=0.):\n",
        "        conf.attention_probs_dropout_prob = ratio\n",
        "        conf.hidden_dropout = ratio \n",
        "        conf.hidden_dropout_prob = ratio\n",
        "        conf.pooler_dropout = ratio\n",
        "        return conf\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.model_conf.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.model_conf.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "    \n",
        "    def _init_weights2_(self, module_lst, method):\n",
        "        for module in module_lst:\n",
        "            for param in module.parameters():\n",
        "                if param.dim() > 1:\n",
        "                    if method == 'kaiming_normal':\n",
        "                        nn.init.kaiming_normal_(param)\n",
        "                    elif method == 'xavier_normal':\n",
        "                        nn.init.xavier_normal_(param)\n",
        "                    elif method == 'orthoganol':\n",
        "                        nn.init.orthogonal_(param)\n",
        "                    else:\n",
        "                        raise Exception('The method is invalid')\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        backbone_outputs = self.backbone(**inputs)\n",
        "        if self.pooling_strategy == 'mean_pooling':\n",
        "            last_hidden_states = backbone_outputs['last_hidden_state']\n",
        "            pooler_outputs = self.pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            \n",
        "        elif self.pooling_strategy == 'concat_pooling':\n",
        "            all_hidden_states = torch.stack(backbone_outputs['hidden_states'])\n",
        "            pooler_outputs = self.pooler(all_hidden_states)\n",
        "            \n",
        "        elif self.pooling_strategy == 'attn_pooling':\n",
        "            last_hidden_states = backbone_outputs['last_hidden_state']\n",
        "            pooler_outputs = self.attn_pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            \n",
        "        elif self.pooling_strategy == 'wlp_attn_pooling':\n",
        "            all_hidden_states = torch.stack(backbone_outputs['hidden_states'])\n",
        "            wlp_pooler = self.wlp_pooler(all_hidden_states)\n",
        "            pooler_outputs = self.attn_pooler(wlp_pooler, inputs['attention_mask'])\n",
        "            \n",
        "        elif self.pooling_strategy == 'concat_h_attn_mean_pooling':\n",
        "            last_hidden_states = backbone_outputs['last_hidden_state']\n",
        "            all_hidden_states = torch.stack(backbone_outputs['hidden_states'])\n",
        "            hattn_outputs = self.hattn_pooler(all_hidden_states)\n",
        "            mean_outputs = self.mean_pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            pooler_outputs = torch.cat((hattn_outputs, mean_outputs), -1)\n",
        "            \n",
        "        elif self.pooling_strategy == 'concat_attn_mean_pooling':\n",
        "            last_hidden_states = backbone_outputs['last_hidden_state']\n",
        "            attn_outputs = self.attn_pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            mean_outputs = self.mean_pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            pooler_outputs = torch.cat((attn_outputs, mean_outputs), -1)\n",
        "            \n",
        "        if self.use_ln:\n",
        "            pooler_outputs = self.ln(pooler_outputs)\n",
        "            \n",
        "        if self.multi_dropout:\n",
        "            x1 = self.fc(self.dropout1(pooler_outputs))\n",
        "            x2 = self.fc(self.dropout2(pooler_outputs))\n",
        "            x3 = self.fc(self.dropout3(pooler_outputs))\n",
        "            x4 = self.fc(self.dropout4(pooler_outputs))\n",
        "            x5 = self.fc(self.dropout5(pooler_outputs))\n",
        "            \n",
        "            outputs = (x1 + x2 + x3 + x4 + x5) / 5\n",
        "\n",
        "        else:\n",
        "            outputs = self.fc(self.dropout0(pooler_outputs))\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ac2cff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.282536Z",
          "iopub.status.busy": "2022-10-18T08:02:46.282261Z",
          "iopub.status.idle": "2022-10-18T08:02:46.296203Z",
          "shell.execute_reply": "2022-10-18T08:02:46.295271Z"
        },
        "papermill": {
          "duration": 0.023859,
          "end_time": "2022-10-18T08:02:46.298423",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.274564",
          "status": "completed"
        },
        "tags": [],
        "id": "b2ac2cff"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/wht1996/feedback-nn-train/notebook\n",
        "class AWP:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        adv_param=\"weight\",\n",
        "        adv_lr=1,\n",
        "        adv_eps=0.2,\n",
        "        adv_step=1,\n",
        "        scaler=None,\n",
        "        apex=False,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.adv_param = adv_param\n",
        "        self.adv_lr = adv_lr\n",
        "        self.adv_eps = adv_eps\n",
        "        self.adv_step = adv_step\n",
        "        self.backup = {}\n",
        "        self.backup_eps = {}\n",
        "        self.scaler = scaler\n",
        "        self.apex = apex\n",
        "\n",
        "    def attack_backward(self, inputs, labels):\n",
        "        self._save() \n",
        "        for i in range(self.adv_step):\n",
        "            self._attack_step() \n",
        "            with torch.cuda.amp.autocast(enabled=self.apex):\n",
        "                adv_outputs = self.model(inputs)\n",
        "                adv_loss = self.criterion(adv_outputs, labels)\n",
        "                adv_loss = adv_loss.mean()\n",
        "            self.optimizer.zero_grad()\n",
        "            self.scaler.scale(adv_loss).backward()\n",
        "            \n",
        "        self._restore()\n",
        "\n",
        "    def _attack_step(self):\n",
        "        e = 1e-6\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                norm1 = torch.norm(param.grad)\n",
        "                norm2 = torch.norm(param.data.detach())\n",
        "                if norm1 != 0 and not torch.isnan(norm1):\n",
        "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
        "                    param.data.add_(r_at)\n",
        "                    param.data = torch.min(\n",
        "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
        "                    )\n",
        "                # param.data.clamp_(*self.backup_eps[name])\n",
        "\n",
        "    def _save(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                if name not in self.backup:\n",
        "                    self.backup[name] = param.data.clone()\n",
        "                    grad_eps = self.adv_eps * param.abs().detach()\n",
        "                    self.backup_eps[name] = (\n",
        "                        self.backup[name] - grad_eps,\n",
        "                        self.backup[name] + grad_eps,\n",
        "                    )\n",
        "\n",
        "    def _restore(self,):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name in self.backup:\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "        self.backup_eps = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08b989f6",
      "metadata": {
        "papermill": {
          "duration": 0.006893,
          "end_time": "2022-10-18T08:02:46.312328",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.305435",
          "status": "completed"
        },
        "tags": [],
        "id": "08b989f6"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2458d0fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.326921Z",
          "iopub.status.busy": "2022-10-18T08:02:46.326044Z",
          "iopub.status.idle": "2022-10-18T08:02:46.331910Z",
          "shell.execute_reply": "2022-10-18T08:02:46.330988Z"
        },
        "papermill": {
          "duration": 0.015617,
          "end_time": "2022-10-18T08:02:46.334294",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.318677",
          "status": "completed"
        },
        "tags": [],
        "id": "2458d0fb"
      },
      "outputs": [],
      "source": [
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.__name__ = self.__class__.__name__\n",
        "    \n",
        "    def forward(self, y_pred, y_true):\n",
        "        return torch.sqrt(self.mse(y_pred, y_true))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4e6210",
      "metadata": {
        "papermill": {
          "duration": 0.006457,
          "end_time": "2022-10-18T08:02:46.347707",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.341250",
          "status": "completed"
        },
        "tags": [],
        "id": "bf4e6210"
      },
      "source": [
        "# Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0f7b8da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.362706Z",
          "iopub.status.busy": "2022-10-18T08:02:46.362006Z",
          "iopub.status.idle": "2022-10-18T08:02:46.371416Z",
          "shell.execute_reply": "2022-10-18T08:02:46.370358Z"
        },
        "papermill": {
          "duration": 0.0194,
          "end_time": "2022-10-18T08:02:46.373706",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.354306",
          "status": "completed"
        },
        "tags": [],
        "id": "c0f7b8da"
      },
      "outputs": [],
      "source": [
        "# https://realpython.com/python-timer/\n",
        "class TimerError(Exception):\n",
        "    \"\"\"A custom exception used to report errors in use of Timer class\"\"\"\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        self.split_time = []\n",
        "        self._start_time = None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start a new timer\"\"\"\n",
        "        if self._start_time is not None:\n",
        "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
        "\n",
        "        self._start_time = time.perf_counter()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n",
        "        if self._start_time is None:\n",
        "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
        "            \n",
        "        self._start_time = None\n",
        "    \n",
        "    def get_time(self):\n",
        "        if self._start_time is None:\n",
        "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
        "            \n",
        "        return time.perf_counter() - self._start_time\n",
        "    \n",
        "    def split(self):\n",
        "        if self._start_time is None:\n",
        "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
        "            \n",
        "        self.split_time.append(time.perf_counter() - self._start_time)\n",
        "    \n",
        "    def get_split_time(self, idx):\n",
        "        return self.split_time[idx]\n",
        "    \n",
        "    @staticmethod\n",
        "    def formatting(second):\n",
        "        return str(datetime.timedelta(seconds=round(second)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e61c5b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.390327Z",
          "iopub.status.busy": "2022-10-18T08:02:46.389406Z",
          "iopub.status.idle": "2022-10-18T08:02:46.396827Z",
          "shell.execute_reply": "2022-10-18T08:02:46.395904Z"
        },
        "papermill": {
          "duration": 0.017935,
          "end_time": "2022-10-18T08:02:46.399068",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.381133",
          "status": "completed"
        },
        "tags": [],
        "id": "04e61c5b"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train/notebook\n",
        "class Averager:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "    def get_average(self):\n",
        "        return self.avg\n",
        "    \n",
        "    def get_value(self):\n",
        "        return self.val"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWeightAverager:\n",
        "    def __init__(self, conf, fold):\n",
        "        self.conf = conf\n",
        "        self.fold = fold\n",
        "        self.state_dict_list = []\n",
        "    \n",
        "    def add_state_dict(self, state_dict):\n",
        "        self.state_dict_list.append(state_dict)\n",
        "        \n",
        "    def average_state_dict(self):\n",
        "        master_sd = CustomModel(self.conf, self.fold).state_dict()\n",
        "        for key in master_sd:\n",
        "            master_sd[key] = 0\n",
        "            for state_dict in self.state_dict_list:\n",
        "                master_sd[key] += state_dict[key]\n",
        "            master_sd[key] = master_sd[key] / len(self.state_dict_list)\n",
        "        \n",
        "        torch.save({\n",
        "            'model_state_dict': master_sd,\n",
        "        },\n",
        "            Path(self.conf.save_path, f'best-epoch-fold{self.fold}-swa.pt'))\n",
        "        \n",
        "        print(f'SAVED FOLD{self.fold}_M_SWA)')"
      ],
      "metadata": {
        "id": "ikm9GoAkC_U3"
      },
      "id": "ikm9GoAkC_U3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eadbb212",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.415493Z",
          "iopub.status.busy": "2022-10-18T08:02:46.414719Z",
          "iopub.status.idle": "2022-10-18T08:02:46.438527Z",
          "shell.execute_reply": "2022-10-18T08:02:46.437586Z"
        },
        "papermill": {
          "duration": 0.03469,
          "end_time": "2022-10-18T08:02:46.440915",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.406225",
          "status": "completed"
        },
        "tags": [],
        "id": "eadbb212"
      },
      "outputs": [],
      "source": [
        "# optimize padding size\n",
        "# https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train/notebook\n",
        "def collator(inputs):\n",
        "    mask_len = int(inputs['attention_mask'].sum(axis=1).max())\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = inputs[k][:, :mask_len]\n",
        "    return inputs\n",
        "\n",
        "def get_dataloader(conf, df, tokenizer, fold_num):\n",
        "    \n",
        "    train_dataset = FB3Dataset(conf, df[df['fold'] != fold_num], tokenizer, fold_num)\n",
        "    valid_dataset = FB3Dataset(conf, df[df['fold'] == fold_num], tokenizer, fold_num)\n",
        "    \n",
        "    total_train_samples = len(train_dataset)\n",
        "    \n",
        "#     data_collator = DataCollatorWithPadding(tokenizer, padding='longest', return_tensors=None)\n",
        "    \n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=conf.num_batch,\n",
        "#         collate_fn=data_collator,\n",
        "        num_workers=4,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    valid_dataloader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=conf.num_batch,\n",
        "#         collate_fn=data_collator,\n",
        "        num_workers=4,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "    \n",
        "    return train_dataloader, valid_dataloader, total_train_samples\n",
        "\n",
        "def get_model(conf, fold_num):\n",
        "    model = CustomModel(conf, fold_num)\n",
        "    model_config_file = Path(conf.save_path, Path(conf.model_name).name + '_config.pt')\n",
        "    \n",
        "    # save model config file\n",
        "    if not model_config_file.is_file():\n",
        "        torch.save(model.model_conf, model_config_file)\n",
        "        \n",
        "    return model\n",
        "\n",
        "def get_tokenizer(conf):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(conf.model_name)\n",
        "    tokenizer_file = Path(conf.save_path, 'tokenizers/')\n",
        "    \n",
        "    if not tokenizer_file.is_file():\n",
        "        tokenizer.save_pretrained(tokenizer_file) # save tokenizer vocab\n",
        "        \n",
        "    return tokenizer\n",
        "\n",
        "def get_optimizer(conf):\n",
        "    optimizer_dict = {\n",
        "        'adamw': optim.AdamW,\n",
        "    }\n",
        "    return optimizer_dict[conf.optimizer]\n",
        "\n",
        "def get_optimizer_grouped_params(model, bb_lr, ll_lr, weight_decay=0.01, layerwise_learning_rate_decay=0.9):\n",
        "    # turn off weight decay in some layer\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if 'backbone' not in n],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": ll_lr,\n",
        "        },\n",
        "    ]\n",
        "    # layer-wise learning rate decay\n",
        "    layers = [model.backbone.embeddings] + list(model.backbone.encoder.layer)\n",
        "    layers.reverse()\n",
        "    decay_lr = bb_lr\n",
        "    for layer in layers:\n",
        "        decay_lr *= layerwise_learning_rate_decay\n",
        "        optimizer_grouped_parameters += [\n",
        "            {\n",
        "                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"lr\": decay_lr,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "                \"lr\": decay_lr,\n",
        "            },\n",
        "        ]\n",
        "    return optimizer_grouped_parameters\n",
        "\n",
        "def get_scheduler(conf, total_samples):\n",
        "    scheduler_dict = {\n",
        "        'cosine_warmup': {\n",
        "            'scheduler': transformers.get_cosine_schedule_with_warmup,\n",
        "            'params': {\n",
        "                'num_warmup_steps': int((total_samples // conf.num_batch) * conf.warmup_epoch),\n",
        "                'num_training_steps': (total_samples // conf.num_batch) * conf.num_epochs,\n",
        "            }\n",
        "        },\n",
        "        'linear_warmup': {\n",
        "            'scheduler': transformers.get_linear_schedule_with_warmup,\n",
        "            'params': {\n",
        "                'num_warmup_steps': int((total_samples // conf.num_batch) * conf.warmup_epoch),\n",
        "                'num_training_steps': (total_samples // conf.num_batch) * conf.num_epochs,\n",
        "            }\n",
        "        },\n",
        "        'cosine_restart_warmup': {\n",
        "            'scheduler': transformers.get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "            'params': {\n",
        "                'num_warmup_steps':  int((total_samples // conf.num_batch) * conf.warmup_epoch),\n",
        "                'num_training_steps': (total_samples // conf.num_batch) * conf.num_epochs,\n",
        "                'num_cycles': 2,\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "    \n",
        "    return scheduler_dict[conf.scheduler]['scheduler'], scheduler_dict[conf.scheduler]['params']\n",
        "\n",
        "def scores_with_MCRMSE(y_predicteds, y_targets):\n",
        "    cols_score = []\n",
        "    col_wise = y_targets.shape[1]\n",
        "    for col in range(col_wise):\n",
        "        y_predicted = torch.as_tensor(y_predicteds[:, col])\n",
        "        y_target = torch.as_tensor(y_targets[:, col])\n",
        "        rmse = mean_squared_error(y_predicted, y_target, squared=False) # returns RMSE value if set to False\n",
        "        cols_score.append(rmse)\n",
        "    score_mean = torch.mean(torch.as_tensor(cols_score))\n",
        "    return score_mean, cols_score\n",
        "\n",
        "def calculate_fold_mean(conf, df):\n",
        "    bsr_list = []\n",
        "    for i in range(conf.num_folds):\n",
        "        fold = df[df['fold'] == i]\n",
        "        best_score_row = fold.loc[fold['mcrmse'] == fold['mcrmse'].min()]\n",
        "        bsr_list.append(best_score_row)\n",
        "        \n",
        "    cv_df = pd.concat(bsr_list)\n",
        "    cv_mean = cv_df.mean(axis=0)\n",
        "    cv_mean['fold'] = cv_mean['epoch'] = int(99)\n",
        "    cv_mean = cv_mean.to_frame().T\n",
        "    cv_df = pd.concat([cv_df, cv_mean], axis=0)\n",
        "    return cv_df.reset_index(drop=True)\n",
        "\n",
        "def get_oof_df(target_cols, predictions, targets):\n",
        "    oof_df = pd.DataFrame()\n",
        "    oof_df[target_cols] = targets.cpu().numpy()\n",
        "    oof_df[[f'pred_{col}' for col in target_cols]] = predictions.cpu().numpy()\n",
        "    return oof_df\n",
        "\n",
        "def calculate_oof_cv(target_cols, oof_df):\n",
        "    predictions = oof_df[[f'pred_{col}' for col in target_cols]].values\n",
        "    targets = oof_df[target_cols].values\n",
        "    score_mean, cols_score = scores_with_MCRMSE(predictions, targets)\n",
        "    return score_mean, cols_score\n",
        "\n",
        "def export_config(basic, train):\n",
        "    config_dict = {**basic.__dict__, **train.__dict__}\n",
        "    remove_keys_list = ['__module__', '__dict__', '__weakref__', '__doc__']\n",
        "    \n",
        "    for key in remove_keys_list:\n",
        "        config_dict.pop(key)\n",
        "    \n",
        "    with open(Path(train.save_path, 'config.yml'), 'w') as file:\n",
        "        yaml.dump(config_dict, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e476bcd2",
      "metadata": {
        "papermill": {
          "duration": 0.006644,
          "end_time": "2022-10-18T08:02:46.454561",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.447917",
          "status": "completed"
        },
        "tags": [],
        "id": "e476bcd2"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "582161dd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.469984Z",
          "iopub.status.busy": "2022-10-18T08:02:46.469653Z",
          "iopub.status.idle": "2022-10-18T08:02:46.513457Z",
          "shell.execute_reply": "2022-10-18T08:02:46.512517Z"
        },
        "papermill": {
          "duration": 0.054275,
          "end_time": "2022-10-18T08:02:46.515553",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.461278",
          "status": "completed"
        },
        "tags": [],
        "id": "582161dd"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, conf, device, fold, model, optimizer, optimizer_grouped_params, scheduler, scheduler_params):\n",
        "        self.device = device\n",
        "        self.current_fold = fold\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer(optimizer_grouped_params, lr=conf.ll_lr, eps=conf.optim_eps)\n",
        "        self.scheduler = scheduler(self.optimizer, **scheduler_params)\n",
        "        \n",
        "        self.dry_run = conf.dry_run\n",
        "        self.exp = conf.exp\n",
        "\n",
        "        self.model_name = conf.model_name\n",
        "        self.target_cols = conf.target_cols\n",
        "        self.use_apex = conf.use_apex\n",
        "        self.use_awp = conf.use_awp\n",
        "        self.use_swa = conf.use_swa\n",
        "        self.multi_dropout = conf.multi_dropout\n",
        "        self.grad_clip = conf.grad_clip\n",
        "        self.grad_max_norm = conf.grad_max_norm\n",
        "        \n",
        "        self.num_batch = conf.num_batch\n",
        "        self.num_epochs = conf.num_epochs\n",
        "        self.current_epoch = None\n",
        "        self.batch_scheduler = conf.batch_scheduler\n",
        "        self.verbose_step = conf.verbose_step\n",
        "        self.accumulate_step = conf.accumulate_step\n",
        "\n",
        "        self.save_path = conf.save_path\n",
        "        \n",
        "        self.scaler = GradScaler(enabled=self.use_apex)\n",
        "        self.criterion = nn.SmoothL1Loss(beta=conf.loss_beta) # [RMSELoss, SmoothL1Loss]\n",
        "        \n",
        "        self.awp_start_epoch = conf.awp_start_epoch\n",
        "        if self.use_awp:\n",
        "            self.awp = AWP(\n",
        "                model=self.model,\n",
        "                criterion=self.criterion,\n",
        "                optimizer=self.optimizer,\n",
        "                adv_lr=conf.adv_lr,\n",
        "                adv_eps=conf.adv_eps,\n",
        "                scaler=self.scaler,\n",
        "                apex=self.use_apex,\n",
        "            )\n",
        "            \n",
        "        self.swa_start_step_ratio = conf.swa_start_step_ratio\n",
        "        if self.use_swa:\n",
        "            self.swa_model = AveragedModel(self.model)\n",
        "            self.swa_scheduler = SWALR(\n",
        "                self.optimizer,\n",
        "                swa_lr=conf.swa_lr,\n",
        "                anneal_strategy=conf.swa_anneal_strat,\n",
        "            )\n",
        "\n",
        "        self.best_train_loss = torch.tensor(10000) # placeholder\n",
        "        self.best_valid_loss = torch.tensor(10000) # placeholder\n",
        "        self.best_score = torch.tensor(10000) # placeholder\n",
        "        self.best_score_col = [torch.tensor(10000) for i in range(6)] # placeholder\n",
        "        \n",
        "        self.record_cols = [\n",
        "            'fold',\n",
        "            'epoch',\n",
        "            'train_loss',\n",
        "            'valid_loss',\n",
        "            'mcrmse',\n",
        "            'cohesion',\n",
        "            'syntax',\n",
        "            'vocabulary',\n",
        "            'phraseology',\n",
        "            'grammar',\n",
        "            'conventions',\n",
        "        ]\n",
        "        self.record_df = pd.DataFrame(columns=self.record_cols)\n",
        "        \n",
        "        if self.dry_run: \n",
        "            self.log('=============')\n",
        "            self.log('== DRY_RUN ==')\n",
        "            self.log('=============')\n",
        "            \n",
        "\n",
        "        mode = 'EXPERIMENTING_MODE' if self.exp else 'CV_MODE'\n",
        "        current_max_len = 'dynamic_padding' if conf.dynamic_max_len else conf.static_max_len_list[self.current_fold]\n",
        "        current_pooling_strategy = conf.pooling_strategy_list[self.current_fold]\n",
        "        \n",
        "        self.log(f'Date: {datetime.datetime.now(pytz.timezone(\"Asia/Ho_Chi_Minh\") )} (GMT+7)')\n",
        "        self.log(f'Mode: {mode}')\n",
        "        self.log(f'Train_on: {self.device}, (AMP: {self.use_apex}, GradScaler: {self.scaler.is_enabled()})')\n",
        "        self.log(f'Model: {self.model_name}')\n",
        "        self.log(f'Model_config: {self.model.model_conf}')\n",
        "        self.log(f'Pooling_strategy: {current_pooling_strategy}')\n",
        "        self.log(f'Initailzation: {conf.reinit_method}')\n",
        "        self.log(f'AWP: {self.use_awp} (adv_lr: {conf.adv_lr}, adv_eps: {conf.adv_eps}) at epoch {self.awp_start_epoch}')\n",
        "        self.log(f'SWA: {self.use_swa} (swa_lr: {conf.swa_lr}, anneal_strat: {conf.swa_anneal_strat}) at last {conf.swa_start_step_ratio}')\n",
        "        self.log(f'Multi_sample_dropout: {self.multi_dropout} (p: {conf.multi_dropout_p})')\n",
        "        self.log(f'Loss_fn: {str(self.criterion)}')\n",
        "        self.log(f'Optimizer: {optimizer.__name__}')\n",
        "        self.log(f'LR: (Backbone: {conf.bb_lr}, LowerLayer: {conf.ll_lr})')\n",
        "        self.log(f'LR_Scheduler: {scheduler.__name__} {scheduler_params}')\n",
        "        self.log(f'Grad_clip_norm: {self.grad_clip} (max_norm: {self.grad_max_norm})')\n",
        "        self.log(f'Number_of_batches: {self.num_batch} (Gradient_accumulate: {self.accumulate_step})')\n",
        "        self.log(f'max_len: {current_max_len}')\n",
        "        self.log('')\n",
        "        \n",
        "    def fit(self, train_loader, valid_loader):\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.current_epoch = epoch + 1\n",
        "            \n",
        "            timer = Timer()\n",
        "            timer.start()\n",
        "            \n",
        "            self.log('TRAIN_LOOP')\n",
        "            train_loss = self._train_fn(train_loader)\n",
        "            self.log('')\n",
        "            \n",
        "            self.log('VALID_LOOP')\n",
        "            valid_loss, score_mean, cols_score, valid_predictions, valid_targets = self._valid_fn(valid_loader)\n",
        "            self.log('')\n",
        "            \n",
        "            epoch_summary_dict = {\n",
        "                'EPOCH': [f'{self.current_epoch}/{self.num_epochs}'],\n",
        "                'TRAIN_LOSS': [f'{train_loss:.5f}'],\n",
        "                'VALID_LOSS': [f'{valid_loss:.5f}'],\n",
        "                'MCRMSE': [f'{score_mean:.5f}'],\n",
        "                'COLS': [' | '.join([f'{col:.3f}' for col in cols_score])],\n",
        "                'TIME': [Timer.formatting(timer.get_time())],\n",
        "            }\n",
        "            epoch_summary_table = tabulate(epoch_summary_dict, headers='keys', tablefmt=\"github\")\n",
        "            \n",
        "            \n",
        "            self.log('--------------------')\n",
        "            self.log(f'EPOCH: {self.current_epoch}/{self.num_epochs} SUMMARY')\n",
        "            self.log('--------------------')\n",
        "            self.log(epoch_summary_table)\n",
        "            self.log('')\n",
        "            \n",
        "            timer.stop()\n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            self._compare_and_save(score_mean, cols_score, train_loss, valid_loss, valid_predictions, valid_targets)\n",
        "                \n",
        "            self.record([\n",
        "                self.current_fold,\n",
        "                self.current_epoch,\n",
        "                train_loss.detach().cpu().numpy(),\n",
        "                valid_loss.detach().cpu().numpy(),\n",
        "                score_mean.cpu().numpy(),\n",
        "                *[col.cpu().numpy() for col in cols_score],\n",
        "            ])\n",
        "            \n",
        "            if self.dry_run: break\n",
        "            # end of the loop\n",
        "        \n",
        "        if self.use_swa:\n",
        "            torch.optim.swa_utils.update_bn(train_loader, self.swa_model)\n",
        "\n",
        "        fold_summary_dict = {\n",
        "            'MCRMSE': [f'{self.best_score:.5f}'],\n",
        "            'cohesion': [f'{self.best_score_col[0]:.5f}'],\n",
        "            'syntax': [f'{self.best_score_col[1]:.5f}'],\n",
        "            'vocabulary': [f'{self.best_score_col[2]:.5f}'],\n",
        "            'phraseology': [f'{self.best_score_col[3]:.5f}'],\n",
        "            'grammar': [f'{self.best_score_col[4]:.5f}'],\n",
        "            'conventions': [f'{self.best_score_col[5]:.5f}'],\n",
        "        }\n",
        "        \n",
        "        fold_summary_table = tabulate(fold_summary_dict, headers='keys', tablefmt='simple_grid')\n",
        "        \n",
        "        self.log('')\n",
        "        self.log(('-' * 35) + f' FOLD {self.current_fold} RESULT ' + ('-' * 35))\n",
        "        self.log(fold_summary_table)\n",
        "        self.log('')\n",
        "        self.log(('#' * 35) + f' END OF FOlD {self.current_fold} ' + ('#' * 35))\n",
        "        self.log('')\n",
        "        self.log('')\n",
        "        \n",
        "        return self.record_df, self.oof_df\n",
        "        \n",
        "    def _train_fn(self, train_loader):\n",
        "        self.model.train()\n",
        "        total_loss = Averager()\n",
        "        current_lr = self.scheduler.get_lr()[0] # placeholder\n",
        "        timer = Timer()\n",
        "        timer.start()\n",
        "        if self.use_awp and self.awp_start_epoch <= self.current_epoch: self.log('AWP_ACTIVATED')\n",
        "        \n",
        "        for step, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "            inputs = collator(inputs)\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            labels = labels.to(self.device)\n",
        "            batchsize = len(labels)\n",
        "            \n",
        "            with autocast(enabled=self.use_apex):\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "            if self.accumulate_step > 1:\n",
        "                loss = loss / self.accumulate_step\n",
        "            total_loss.update(loss, batchsize)\n",
        "            \n",
        "            self.scaler.scale(loss).backward()\n",
        "            \n",
        "            if (step + 1) % self.accumulate_step == 0:\n",
        "                if self.use_awp and self.awp_start_epoch <= self.current_epoch:\n",
        "                    self.awp.attack_backward(inputs, labels)\n",
        "                \n",
        "                if self.use_swa:\n",
        "                    swa_start_step = int((self.num_epochs * len(train_loader)) * (1 - self.swa_start_step_ratio))\n",
        "                    current_training_step = int(((self.current_epoch - 1) * len(train_loader)) + step)\n",
        "                    if current_training_step == swa_start_step:\n",
        "                        self.log('SWA_ACTIVATED')\n",
        "                    if current_training_step >= swa_start_step:\n",
        "                        self.swa_model.update_parameters(self.model)\n",
        "                        self.swa_scheduler.step()\n",
        "                        current_lr = self.swa_scheduler.get_last_lr()[0]\n",
        "                    else:\n",
        "                        if self.batch_scheduler:\n",
        "                            self.scheduler.step()\n",
        "                            current_lr = self.scheduler.get_lr()[0]\n",
        "                else:\n",
        "                    if self.batch_scheduler:\n",
        "                        self.scheduler.step()\n",
        "                        current_lr = self.scheduler.get_lr()[0]\n",
        "                    \n",
        "                if self.grad_clip:\n",
        "                    self.scaler.unscale_(self.optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                        parameters=self.model.parameters(),\n",
        "                        max_norm=self.grad_max_norm,\n",
        "                    )\n",
        "\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            if step % self.verbose_step == 0 or step == (len(train_loader) - 1):\n",
        "                self.log(\n",
        "                    f'[TRAIN F{self.current_fold}] ' + \\\n",
        "                    f'EPOCH: {self.current_epoch}/{self.num_epochs} | ' + \\\n",
        "                    f'STEP: {str(step).zfill(len(str(len(train_loader))))}/{len(train_loader)} | ' + \\\n",
        "                    f'LOSS: {total_loss.get_value():.5f} ({total_loss.get_average():.5f}) | ' + \\\n",
        "                    f'LR: {current_lr:.8f} | ' + \\\n",
        "                    f'TIME: {Timer.formatting(timer.get_time())} |'\n",
        "                )\n",
        "\n",
        "            if self.dry_run: break\n",
        "            # end of the loop\n",
        "            \n",
        "        timer.stop()\n",
        "\n",
        "        return total_loss.get_average()\n",
        "            \n",
        "    \n",
        "    def _valid_fn(self, valid_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = Averager()\n",
        "        timer = Timer()\n",
        "        timer.start()\n",
        "        outputs_list = [] # for stacking outputs\n",
        "        targets_list = [] # for stacking labels\n",
        "        \n",
        "        for step, (inputs, labels) in enumerate(valid_loader):\n",
        "            \n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            labels = labels.to(self.device)\n",
        "            batchsize = len(labels)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                \n",
        "            total_loss.update(loss, batchsize)\n",
        "            outputs_list.append(outputs)\n",
        "            targets_list.append(labels)\n",
        "            \n",
        "            if step % self.verbose_step == 0 or step == (len(valid_loader) - 1):\n",
        "                self.log(\n",
        "                    f'[VALID F{self.current_fold}] ' + \\\n",
        "                    f'EPOCH: {self.current_epoch}/{self.num_epochs} | ' + \\\n",
        "                    f'STEP: {str(step).zfill(len(str(len(valid_loader))))}/{len(valid_loader)} | ' + \\\n",
        "                    f'LOSS: {total_loss.get_value():.5f} ({total_loss.get_average():.5f}) | ' + \\\n",
        "                    f'TIME: {Timer.formatting(timer.get_time())} |'\n",
        "                )\n",
        "\n",
        "            if self.dry_run: break\n",
        "            # end of the loop\n",
        "\n",
        "        predictions = torch.cat(outputs_list)\n",
        "        targets = torch.cat(targets_list)\n",
        "\n",
        "        score_mean, cols_score = scores_with_MCRMSE(predictions, targets)\n",
        "        \n",
        "        timer.stop()\n",
        "        \n",
        "        return total_loss.get_average(), score_mean, cols_score, predictions, targets\n",
        "    \n",
        "    def _compare_and_save(self, score, col_score, train_loss, valid_loss, valid_predictions, valid_targets):\n",
        "        if score < self.best_score:\n",
        "            self.best_score = score\n",
        "            self.best_score_col = col_score\n",
        "            self.best_train_loss = train_loss\n",
        "            self.best_valid_loss = valid_loss\n",
        "            \n",
        "            file_name = f'best-epoch-fold{self.current_fold}.pt'\n",
        "            self._save(self.save_path, file_name, last_checkpoint=False)\n",
        "            \n",
        "            self.oof_df = get_oof_df(self.target_cols, valid_predictions, valid_targets)\n",
        "        \n",
        "            self.log('')\n",
        "            self.log(f'[SAVED] EPOCH: {self.current_epoch} | MCRMSE: {self.best_score}')\n",
        "            self.log('')\n",
        "    \n",
        "    def _save(self, save_path, file_name, last_checkpoint=False):\n",
        "        self.model.eval()\n",
        "        if last_checkpoint:\n",
        "            torch.save({\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'scaler': self.scaler.state_dict(),\n",
        "                'best_train_loss': self.best_train_loss,\n",
        "                'best_valid_loss': self.best_valid_loss,\n",
        "                'best_score': self.best_score,\n",
        "                'epoch': self.current_epoch,\n",
        "            }, Path(save_path, file_name))\n",
        "        else:\n",
        "            torch.save({\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'best_train_loss': self.best_train_loss,\n",
        "                'best_valid_loss': self.best_valid_loss,\n",
        "                'best_score': self.best_score,\n",
        "                'epoch': self.current_epoch,\n",
        "            }, Path(save_path, file_name))\n",
        "            \n",
        "    def log(self, message):\n",
        "        print(message)\n",
        "        if not self.dry_run:\n",
        "            with open(Path(self.save_path, 'log.txt'), mode='a+', encoding='utf-8') as log:\n",
        "                log.write(f'{message}\\n')\n",
        "\n",
        "    def record(self, record_row):\n",
        "        new_record_dict = {k: [v] for k, v in zip(self.record_cols, record_row)}\n",
        "        new_record = pd.DataFrame.from_dict(new_record_dict)\n",
        "        self.record_df = pd.concat([self.record_df, new_record], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7a1cfc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.529991Z",
          "iopub.status.busy": "2022-10-18T08:02:46.529684Z",
          "iopub.status.idle": "2022-10-18T08:02:46.539727Z",
          "shell.execute_reply": "2022-10-18T08:02:46.538870Z"
        },
        "papermill": {
          "duration": 0.01985,
          "end_time": "2022-10-18T08:02:46.541946",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.522096",
          "status": "completed"
        },
        "tags": [],
        "id": "ad7a1cfc"
      },
      "outputs": [],
      "source": [
        "def run_training(train_df, conf, dry_run=True, exp=True):\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    \n",
        "    conf.dry_run = dry_run # for debugging\n",
        "    conf.exp = exp # for experimenting train only the first fold\n",
        "    \n",
        "    tokenizer = get_tokenizer(conf)\n",
        "    train_result_df = pd.DataFrame()\n",
        "    oof_df = pd.DataFrame()\n",
        "    \n",
        "#     for fold in range(conf.num_folds):\n",
        "    for fold in conf.train_fold_list:\n",
        "        \n",
        "        seed_everything(conf.seed)\n",
        "        model = get_model(conf, fold)\n",
        "        model = model.to(device)\n",
        "        optimizer = get_optimizer(conf)\n",
        "        optimizer_grouped_params = get_optimizer_grouped_params(model, conf.bb_lr, conf.ll_lr, conf.weight_decay, conf.llrd)\n",
        "        train_loader, valid_loader, total_train_samples = get_dataloader(conf, train_df, tokenizer, fold)\n",
        "        scheduler, scheduler_params = get_scheduler(conf, total_train_samples)\n",
        "\n",
        "        trainer = Trainer(conf, device, fold, model, optimizer, optimizer_grouped_params, scheduler, scheduler_params)\n",
        "        fold_result, fold_oof_df = trainer.fit(train_loader, valid_loader)\n",
        "        train_result_df = pd.concat([train_result_df, fold_result], axis=0)\n",
        "        oof_df = pd.concat((oof_df, fold_oof_df), axis=0)\n",
        "        \n",
        "        if conf.dry_run or conf.exp : break\n",
        "    \n",
        "    cv_result_df = calculate_fold_mean(conf, train_result_df)\n",
        "    oof_score_mean, oof_cols_score = calculate_oof_cv(conf.target_cols, oof_df)\n",
        "    cv_df = pd.DataFrame([[oof_score_mean.numpy(), *[col.numpy() for col in oof_cols_score]]], columns=['mcrmse', *conf.target_cols])\n",
        "    \n",
        "    print('Overall Training Result')\n",
        "    display(train_result_df)\n",
        "    print('Best Result')\n",
        "    display(cv_result_df)\n",
        "    print('CV Result')\n",
        "    display(cv_df)\n",
        "    \n",
        "    train_result_df.reset_index(drop=True).to_csv(Path(conf.save_path, 'train_result.csv'), index=False)\n",
        "    cv_result_df.reset_index(drop=True).to_csv(Path(conf.save_path, 'best_result.csv'), index=False)\n",
        "    cv_df.reset_index(drop=True).to_csv(Path(conf.save_path, 'cv_result.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77aaca56",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.556476Z",
          "iopub.status.busy": "2022-10-18T08:02:46.555620Z",
          "iopub.status.idle": "2022-10-18T08:02:46.562979Z",
          "shell.execute_reply": "2022-10-18T08:02:46.562053Z"
        },
        "papermill": {
          "duration": 0.016524,
          "end_time": "2022-10-18T08:02:46.564940",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.548416",
          "status": "completed"
        },
        "tags": [],
        "id": "77aaca56"
      },
      "outputs": [],
      "source": [
        "class TRAINCONF(BASICCONF):\n",
        "    \n",
        "    # general config\n",
        "    exp_num = '0'\n",
        "    save_path = f'/content/drive/MyDrive/Colab Notebooks/kaggle/fb3/results/exp{exp_num}'\n",
        "    \n",
        "     # run config\n",
        "    dry_run = True # for debugging\n",
        "    exp = True # for experimenting\n",
        "    train_fold_list = [0, 1, 2, 3]\n",
        "    \n",
        "    # dataset config\n",
        "    dynamic_max_len = False\n",
        "    static_max_len_list = [768, 768, 768, 768] # if dynaimic_padding is False: then use static_max_len_list instead\n",
        "    # [1428, 1428, 1428, 1428] default max_len for 4 fold\n",
        "    \n",
        "    # model config\n",
        "    model_name = 'microsoft/deberta-v3-base' # [xsmall, small, base, large, xlarge]\n",
        "    reinit_last_layers = 1 # from the bottom (no need to set negative value).\n",
        "    reinit_method = None # [kaiming_normal, xavier_normal, orthoganol, None] # if None: initialize by  normal dist\n",
        "    gradient_checkpointing = True\n",
        "    \n",
        "    use_awp = True\n",
        "    awp_start_epoch = 1\n",
        "    adv_lr = 2e-5\n",
        "    adv_eps = 1e-3\n",
        "    \n",
        "    use_swa = False\n",
        "    swa_start_step_ratio = 0.112 #tart from last n% of the training step # 0.112(1300)\n",
        "    swa_lr = 1e-6\n",
        "    swa_anneal_strat = 'cos' # ['cos', 'linear']\n",
        "    \n",
        "    pooling_strategy_list = [\n",
        "        'concat_attn_mean_pooling',\n",
        "        'concat_attn_mean_pooling',\n",
        "        'concat_attn_mean_pooling',\n",
        "        'concat_attn_mean_pooling',\n",
        "    ]\n",
        "    \n",
        "    # [\n",
        "    #     mean_pooling,\n",
        "    #     concat_pooling,\n",
        "    #     attn_pooling,\n",
        "    #     wlp_attn_pooling,\n",
        "    #     concat_h_attn_mean_pooling,\n",
        "    #     concat_attn_mean_pooling,\n",
        "    # ]\n",
        "    \n",
        "    wlp_pooling_start = 0 # for weighted layer pooling (n from the top)\n",
        "    concat_pooling_last = 4 # for concatenate pooling (n from the bottom)\n",
        "    \n",
        "    use_ln = False\n",
        "\n",
        "    multi_dropout = True\n",
        "    multi_dropout_p = [0.3, 0.3, 0.3, 0.3, 0.3]\n",
        "    \n",
        "    # trainer config\n",
        "    use_apex = torch.cuda.is_available()\n",
        "    verbose_step = 40\n",
        "    num_epochs = 4\n",
        "    num_batch = 8\n",
        "    accumulate_step = 1\n",
        "    \n",
        "    # optimizer config\n",
        "    optimizer = 'adamw'\n",
        "    bb_lr = 2e-5 # backbone lr\n",
        "    ll_lr = 3e-5 # lowerlayer lr\n",
        "    \n",
        "    weight_decay = 0.01\n",
        "    llrd = 0.9\n",
        "    optim_eps = 1e-6\n",
        "    \n",
        "    batch_scheduler = True\n",
        "    scheduler = 'cosine_warmup' # [cosine_warmup, linear_warmup, consine_restart_warmup]\n",
        "    warmup_epoch = 0.25 # first n of the first epoch\n",
        "    \n",
        "    grad_clip = False\n",
        "    grad_max_norm = 10\n",
        "    \n",
        "    # loss_fn config\n",
        "    loss_beta = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_or_create(path):\n",
        "    path = Path(path)\n",
        "    if not path.is_dir():\n",
        "        path.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "eUIYJBlbWMTB"
      },
      "id": "eUIYJBlbWMTB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "51d349f1",
      "metadata": {
        "papermill": {
          "duration": 0.006366,
          "end_time": "2022-10-18T08:02:46.577668",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.571302",
          "status": "completed"
        },
        "tags": [],
        "id": "51d349f1"
      },
      "source": [
        "# RUN TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00aba57",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:02:46.592092Z",
          "iopub.status.busy": "2022-10-18T08:02:46.591281Z",
          "iopub.status.idle": "2022-10-18T08:03:45.990047Z",
          "shell.execute_reply": "2022-10-18T08:03:45.988956Z"
        },
        "papermill": {
          "duration": 59.408732,
          "end_time": "2022-10-18T08:03:45.992697",
          "exception": false,
          "start_time": "2022-10-18T08:02:46.583965",
          "status": "completed"
        },
        "tags": [],
        "id": "c00aba57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "398b3a32683e43c18e34282bec3f326b",
            "f3737d1453a043dc837177b6438f7c62",
            "9408975b3a6b4b88a300621eff7ea10f",
            "635ff14a895e45e3ad3beba168341b96",
            "f8ac8ed8056945bfb0f26ed5d4daab84",
            "d429fd7a693c420784be805580f12a5a",
            "84dc4f6343ff4d13a90ffd9a4424998d",
            "2a9ecd5d0bf24215b1461377591eb384",
            "966e15b8b97d4b4896c54a36db32c339",
            "fd9f8dc8773c42c6b61db78c85700051",
            "b4f50bd40aa149e59fd039f2c961e881",
            "f1bc269c80624ba39de2a3dcacab8ac0",
            "e865889dd9c6427394ea0d2579c50efe",
            "8532dc067b1d4d049c5049178c52b347",
            "3369149214fa4d3f825465c270b56d51",
            "2e5db959679b4a81854a396d4380901a",
            "ff8f4e42cfd94f6f8c962aeeac3980cf",
            "b8ee71325e5b4d1ba7df118d803ba568",
            "1152659ff26b44fca487bc3ae5e2e551",
            "b4a1d7c598c64dd78b27da8f878a62f9",
            "42ca961258e64b02a6782cbe3758af84",
            "104c3ff2018d4e4197bdb64cee5c8737",
            "c24e9f0cd18041f3a12a1352d97a8af7",
            "f2b86a3cb420499f8a047ee0d85562c4",
            "40bd26eb3c3f452c9ee0b6ec6b2bb9b0",
            "2e693f9cd2484de1a78bad4c32dd2164",
            "bd7138400067481bbca12801657f8e39",
            "739c7b6a7f9a4a1eab31c008337a5d90",
            "824f251c2dfd4fbe893943dfada8cc98",
            "857e7baaf28d44ed97c01388d6b2d25c",
            "b9157df502e54522b93bd65afdad998d",
            "1894475de8da43d1aacaea97c7e6336d",
            "c95deab9ca544b21a6f2781bc1257ff5",
            "4ac1ee79bff744768e55d8869a0c110f",
            "61b7515f22d148ebbce24229b0a9e0b5",
            "fd8c0e89a108469b94f96919666b0795",
            "12edfa9d43a749a899c5248ac23ccb52",
            "6be3d416eac04e38b21db65f0c7050af",
            "1d88549d9bf945fe9087cda8d04df22a",
            "af2fd8fe43ca4bfe864e91e8d0c5ed47",
            "07691d154a92456fa93b69ebd53e20f5",
            "7fc14f1270344a02a12e1d02cfaaa839",
            "8ee2c3ea6ac74fe6964708f64350aab2",
            "3cec2e2ee3564907af0d9c020b111408",
            "267c31cffcdd4fd0af3fb7322b3b3f4d",
            "cea08152053b4e4c972e8d57fb5e9180",
            "238cb0537f244094a19af849f447a7ff",
            "f8c1f40a9cec439ebf4778b4af3157ac",
            "b5ee7a294e3742e8a1bcf1c874c7667d",
            "bb2d7b73a6ad4eaaa55253976fcd6851",
            "b83648c4d23441a5b5c730f709d2de61",
            "1daef053b6e045f09d03e8a03e86b147",
            "b347db5192c448e0b91ecd1c6396540c",
            "cd3e80e270f245bc854a11bdadd863f6",
            "61132c6874e44640bcd58b2ce4a16ebe"
          ]
        },
        "outputId": "71f5c681-a98a-431f-9597-30447a881e2e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "398b3a32683e43c18e34282bec3f326b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/474 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1bc269c80624ba39de2a3dcacab8ac0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c24e9f0cd18041f3a12a1352d97a8af7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ac1ee79bff744768e55d8869a0c110f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/533M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "267c31cffcdd4fd0af3fb7322b3b3f4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
            "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2022-11-16 20:17:07.566016+07:00 (GMT+7)\n",
            "Mode: CV_MODE\n",
            "Train_on: cuda, (AMP: True, GradScaler: True)\n",
            "Model: microsoft/deberta-base\n",
            "Model_config: DebertaConfig {\n",
            "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0.0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Pooling_strategy: concat_attn_mean_pooling\n",
            "Initailzation: None\n",
            "AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1\n",
            "SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112\n",
            "Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])\n",
            "Loss_fn: SmoothL1Loss()\n",
            "Optimizer: AdamW\n",
            "LR: (Backbone: 1e-05, LowerLayer: 2e-05)\n",
            "LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 5864}\n",
            "Grad_clip_norm: False (max_norm: 10)\n",
            "Number_of_batches: 2 (Gradient_accumulate: 1)\n",
            "max_len: dynamic_padding\n",
            "\n",
            "TRAIN_LOOP\n",
            "AWP_ACTIVATED\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0000/1466 | LOSS: 2.42342 (2.42342) | LR: 0.00000005 | TIME: 0:00:01 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0040/1466 | LOSS: 3.13166 (2.75603) | LR: 0.00000224 | TIME: 0:00:38 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0080/1466 | LOSS: 1.66495 (2.49346) | LR: 0.00000443 | TIME: 0:01:21 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0120/1466 | LOSS: 0.96857 (1.96387) | LR: 0.00000661 | TIME: 0:02:12 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0160/1466 | LOSS: 0.29088 (1.53343) | LR: 0.00000880 | TIME: 0:02:53 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0200/1466 | LOSS: 0.12758 (1.26927) | LR: 0.00001098 | TIME: 0:04:06 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0240/1466 | LOSS: 0.06657 (1.08980) | LR: 0.00001317 | TIME: 0:04:55 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0280/1466 | LOSS: 0.16681 (0.96609) | LR: 0.00001536 | TIME: 0:06:08 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0320/1466 | LOSS: 0.20748 (0.86751) | LR: 0.00001754 | TIME: 0:06:41 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0360/1466 | LOSS: 0.07386 (0.78920) | LR: 0.00001973 | TIME: 0:07:40 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0400/1466 | LOSS: 0.06869 (0.72722) | LR: 0.00002000 | TIME: 0:09:25 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0440/1466 | LOSS: 0.51585 (0.67637) | LR: 0.00001999 | TIME: 0:10:05 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0480/1466 | LOSS: 0.14964 (0.63301) | LR: 0.00001998 | TIME: 0:11:00 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0520/1466 | LOSS: 0.15420 (0.59684) | LR: 0.00001996 | TIME: 0:11:44 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0560/1466 | LOSS: 0.18138 (0.56463) | LR: 0.00001994 | TIME: 0:12:23 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0600/1466 | LOSS: 0.13869 (0.53676) | LR: 0.00001991 | TIME: 0:13:04 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0640/1466 | LOSS: 0.16670 (0.51281) | LR: 0.00001988 | TIME: 0:14:02 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0680/1466 | LOSS: 0.08610 (0.49072) | LR: 0.00001984 | TIME: 0:14:40 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0720/1466 | LOSS: 0.23653 (0.47247) | LR: 0.00001979 | TIME: 0:15:17 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0760/1466 | LOSS: 0.08247 (0.45655) | LR: 0.00001975 | TIME: 0:16:11 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0800/1466 | LOSS: 0.03743 (0.43988) | LR: 0.00001969 | TIME: 0:16:45 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0840/1466 | LOSS: 0.10575 (0.42598) | LR: 0.00001963 | TIME: 0:17:27 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0880/1466 | LOSS: 0.05585 (0.41313) | LR: 0.00001957 | TIME: 0:18:06 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0920/1466 | LOSS: 0.09375 (0.40209) | LR: 0.00001950 | TIME: 0:18:48 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 0960/1466 | LOSS: 0.23263 (0.39042) | LR: 0.00001943 | TIME: 0:19:29 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 1000/1466 | LOSS: 0.07800 (0.38010) | LR: 0.00001935 | TIME: 0:20:15 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 1040/1466 | LOSS: 0.10186 (0.37083) | LR: 0.00001927 | TIME: 0:21:00 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 1080/1466 | LOSS: 0.13670 (0.36289) | LR: 0.00001918 | TIME: 0:21:40 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 1120/1466 | LOSS: 0.29814 (0.35485) | LR: 0.00001908 | TIME: 0:22:28 |\n",
            "[TRAIN F0] EPOCH: 1/4 | STEP: 1160/1466 | LOSS: 0.39403 (0.34827) | LR: 0.00001899 | TIME: 0:23:30 |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2aec0b98cc89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheck_or_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINCONF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexport_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBASICCONF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAINCONF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINCONF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-c0680d30868c>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(train_df, conf, dry_run, exp)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_grouped_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mfold_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_oof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_result_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_result_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_oof_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-30e844a7d6cb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TRAIN_LOOP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-30e844a7d6cb>\u001b[0m in \u001b[0;36m_train_fn\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_awp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawp_start_epoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_swa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c8c78d3369c9>\u001b[0m in \u001b[0;36mattack_backward\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0madv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    251\u001b[0m                                \"of them.\")\n\u001b[1;32m    252\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;34m\"none of output has requires_grad=True,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \" this checkpoint() is not necessary\")\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_with_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_with_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         grads = tuple(inp.grad if isinstance(inp, torch.Tensor) else None\n\u001b[1;32m    148\u001b[0m                       for inp in detached_inputs)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    251\u001b[0m                                \"of them.\")\n\u001b[1;32m    252\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0minputGrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_backward_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputGrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36msoftmax_backward_data\u001b[0;34m(parent, grad_output, output, dim, self)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_softmax_backward_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_softmax_backward_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 14.76 GiB total capacity; 9.63 GiB already allocated; 387.75 MiB free; 12.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "check_or_create(TRAINCONF.save_path)\n",
        "export_config(basic=BASICCONF, train=TRAINCONF)\n",
        "run_training(df, TRAINCONF, dry_run=False, exp=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b80b8cf",
      "metadata": {
        "papermill": {
          "duration": 0.007401,
          "end_time": "2022-10-18T08:03:46.008033",
          "exception": false,
          "start_time": "2022-10-18T08:03:46.000632",
          "status": "completed"
        },
        "tags": [],
        "id": "4b80b8cf"
      },
      "source": [
        "# Jump to Results 🔼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "487c4f8a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-18T08:03:46.024201Z",
          "iopub.status.busy": "2022-10-18T08:03:46.023874Z",
          "iopub.status.idle": "2022-10-18T08:03:46.029183Z",
          "shell.execute_reply": "2022-10-18T08:03:46.028337Z"
        },
        "papermill": {
          "duration": 0.015704,
          "end_time": "2022-10-18T08:03:46.031026",
          "exception": false,
          "start_time": "2022-10-18T08:03:46.015322",
          "status": "completed"
        },
        "tags": [],
        "id": "487c4f8a"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(TRAINCONF.model_name)\n",
        "# train_loader, valid_loader = get_dataloader(df, tokenizer, TRAINCONF.num_batch, 0)\n",
        "# loader = iter(train_loader)\n",
        "\n",
        "# batch = next(loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r '/content/drive/MyDrive/Colab Notebooks/kaggle/fb3/results/exp2/'"
      ],
      "metadata": {
        "id": "Wj87jVKTWmsH"
      },
      "id": "Wj87jVKTWmsH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.4493041, tl-0.09330081, vl-0.100785285"
      ],
      "metadata": {
        "id": "zRkcC1TMYWxg"
      },
      "id": "zRkcC1TMYWxg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 89.401225,
      "end_time": "2022-10-18T08:03:48.888378",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-10-18T08:02:19.487153",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "398b3a32683e43c18e34282bec3f326b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3737d1453a043dc837177b6438f7c62",
              "IPY_MODEL_9408975b3a6b4b88a300621eff7ea10f",
              "IPY_MODEL_635ff14a895e45e3ad3beba168341b96"
            ],
            "layout": "IPY_MODEL_f8ac8ed8056945bfb0f26ed5d4daab84"
          }
        },
        "f3737d1453a043dc837177b6438f7c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d429fd7a693c420784be805580f12a5a",
            "placeholder": "​",
            "style": "IPY_MODEL_84dc4f6343ff4d13a90ffd9a4424998d",
            "value": "Downloading: 100%"
          }
        },
        "9408975b3a6b4b88a300621eff7ea10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a9ecd5d0bf24215b1461377591eb384",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_966e15b8b97d4b4896c54a36db32c339",
            "value": 52
          }
        },
        "635ff14a895e45e3ad3beba168341b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd9f8dc8773c42c6b61db78c85700051",
            "placeholder": "​",
            "style": "IPY_MODEL_b4f50bd40aa149e59fd039f2c961e881",
            "value": " 52.0/52.0 [00:00&lt;00:00, 1.71kB/s]"
          }
        },
        "f8ac8ed8056945bfb0f26ed5d4daab84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d429fd7a693c420784be805580f12a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84dc4f6343ff4d13a90ffd9a4424998d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a9ecd5d0bf24215b1461377591eb384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "966e15b8b97d4b4896c54a36db32c339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd9f8dc8773c42c6b61db78c85700051": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f50bd40aa149e59fd039f2c961e881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1bc269c80624ba39de2a3dcacab8ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e865889dd9c6427394ea0d2579c50efe",
              "IPY_MODEL_8532dc067b1d4d049c5049178c52b347",
              "IPY_MODEL_3369149214fa4d3f825465c270b56d51"
            ],
            "layout": "IPY_MODEL_2e5db959679b4a81854a396d4380901a"
          }
        },
        "e865889dd9c6427394ea0d2579c50efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff8f4e42cfd94f6f8c962aeeac3980cf",
            "placeholder": "​",
            "style": "IPY_MODEL_b8ee71325e5b4d1ba7df118d803ba568",
            "value": "Downloading: 100%"
          }
        },
        "8532dc067b1d4d049c5049178c52b347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1152659ff26b44fca487bc3ae5e2e551",
            "max": 474,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4a1d7c598c64dd78b27da8f878a62f9",
            "value": 474
          }
        },
        "3369149214fa4d3f825465c270b56d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42ca961258e64b02a6782cbe3758af84",
            "placeholder": "​",
            "style": "IPY_MODEL_104c3ff2018d4e4197bdb64cee5c8737",
            "value": " 474/474 [00:00&lt;00:00, 15.1kB/s]"
          }
        },
        "2e5db959679b4a81854a396d4380901a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff8f4e42cfd94f6f8c962aeeac3980cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8ee71325e5b4d1ba7df118d803ba568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1152659ff26b44fca487bc3ae5e2e551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a1d7c598c64dd78b27da8f878a62f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42ca961258e64b02a6782cbe3758af84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104c3ff2018d4e4197bdb64cee5c8737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c24e9f0cd18041f3a12a1352d97a8af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2b86a3cb420499f8a047ee0d85562c4",
              "IPY_MODEL_40bd26eb3c3f452c9ee0b6ec6b2bb9b0",
              "IPY_MODEL_2e693f9cd2484de1a78bad4c32dd2164"
            ],
            "layout": "IPY_MODEL_bd7138400067481bbca12801657f8e39"
          }
        },
        "f2b86a3cb420499f8a047ee0d85562c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739c7b6a7f9a4a1eab31c008337a5d90",
            "placeholder": "​",
            "style": "IPY_MODEL_824f251c2dfd4fbe893943dfada8cc98",
            "value": "Downloading: 100%"
          }
        },
        "40bd26eb3c3f452c9ee0b6ec6b2bb9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_857e7baaf28d44ed97c01388d6b2d25c",
            "max": 898825,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9157df502e54522b93bd65afdad998d",
            "value": 898825
          }
        },
        "2e693f9cd2484de1a78bad4c32dd2164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1894475de8da43d1aacaea97c7e6336d",
            "placeholder": "​",
            "style": "IPY_MODEL_c95deab9ca544b21a6f2781bc1257ff5",
            "value": " 878k/878k [00:01&lt;00:00, 1.03MB/s]"
          }
        },
        "bd7138400067481bbca12801657f8e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "739c7b6a7f9a4a1eab31c008337a5d90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "824f251c2dfd4fbe893943dfada8cc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "857e7baaf28d44ed97c01388d6b2d25c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9157df502e54522b93bd65afdad998d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1894475de8da43d1aacaea97c7e6336d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c95deab9ca544b21a6f2781bc1257ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ac1ee79bff744768e55d8869a0c110f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61b7515f22d148ebbce24229b0a9e0b5",
              "IPY_MODEL_fd8c0e89a108469b94f96919666b0795",
              "IPY_MODEL_12edfa9d43a749a899c5248ac23ccb52"
            ],
            "layout": "IPY_MODEL_6be3d416eac04e38b21db65f0c7050af"
          }
        },
        "61b7515f22d148ebbce24229b0a9e0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d88549d9bf945fe9087cda8d04df22a",
            "placeholder": "​",
            "style": "IPY_MODEL_af2fd8fe43ca4bfe864e91e8d0c5ed47",
            "value": "Downloading: 100%"
          }
        },
        "fd8c0e89a108469b94f96919666b0795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07691d154a92456fa93b69ebd53e20f5",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fc14f1270344a02a12e1d02cfaaa839",
            "value": 456318
          }
        },
        "12edfa9d43a749a899c5248ac23ccb52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee2c3ea6ac74fe6964708f64350aab2",
            "placeholder": "​",
            "style": "IPY_MODEL_3cec2e2ee3564907af0d9c020b111408",
            "value": " 446k/446k [00:00&lt;00:00, 308kB/s]"
          }
        },
        "6be3d416eac04e38b21db65f0c7050af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d88549d9bf945fe9087cda8d04df22a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af2fd8fe43ca4bfe864e91e8d0c5ed47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07691d154a92456fa93b69ebd53e20f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc14f1270344a02a12e1d02cfaaa839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ee2c3ea6ac74fe6964708f64350aab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cec2e2ee3564907af0d9c020b111408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "267c31cffcdd4fd0af3fb7322b3b3f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cea08152053b4e4c972e8d57fb5e9180",
              "IPY_MODEL_238cb0537f244094a19af849f447a7ff",
              "IPY_MODEL_f8c1f40a9cec439ebf4778b4af3157ac"
            ],
            "layout": "IPY_MODEL_b5ee7a294e3742e8a1bcf1c874c7667d"
          }
        },
        "cea08152053b4e4c972e8d57fb5e9180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2d7b73a6ad4eaaa55253976fcd6851",
            "placeholder": "​",
            "style": "IPY_MODEL_b83648c4d23441a5b5c730f709d2de61",
            "value": "Downloading: 100%"
          }
        },
        "238cb0537f244094a19af849f447a7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1daef053b6e045f09d03e8a03e86b147",
            "max": 558614189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b347db5192c448e0b91ecd1c6396540c",
            "value": 558614189
          }
        },
        "f8c1f40a9cec439ebf4778b4af3157ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd3e80e270f245bc854a11bdadd863f6",
            "placeholder": "​",
            "style": "IPY_MODEL_61132c6874e44640bcd58b2ce4a16ebe",
            "value": " 533M/533M [00:13&lt;00:00, 39.1MB/s]"
          }
        },
        "b5ee7a294e3742e8a1bcf1c874c7667d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb2d7b73a6ad4eaaa55253976fcd6851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b83648c4d23441a5b5c730f709d2de61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1daef053b6e045f09d03e8a03e86b147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b347db5192c448e0b91ecd1c6396540c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd3e80e270f245bc854a11bdadd863f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61132c6874e44640bcd58b2ce4a16ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}