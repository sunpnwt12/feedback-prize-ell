{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7a4b5bdb",
      "metadata": {
        "papermill": {
          "duration": 0.007371,
          "end_time": "2022-11-23T07:55:12.917781",
          "exception": false,
          "start_time": "2022-11-23T07:55:12.910410",
          "status": "completed"
        },
        "tags": [],
        "id": "7a4b5bdb"
      },
      "source": [
        "# Changelog\n",
        "## Changes\n",
        "\n",
        "## Bugs & Issues\n",
        "\n",
        "## Fixed bugs\n",
        "\n",
        "## Holding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199bbe5c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:12.932588Z",
          "iopub.status.busy": "2022-11-23T07:55:12.931758Z",
          "iopub.status.idle": "2022-11-23T07:55:13.935745Z",
          "shell.execute_reply": "2022-11-23T07:55:13.934562Z"
        },
        "papermill": {
          "duration": 1.014448,
          "end_time": "2022-11-23T07:55:13.938632",
          "exception": false,
          "start_time": "2022-11-23T07:55:12.924184",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "199bbe5c",
        "outputId": "3f386f3c-69a3-4d76-ced4-da548cf702b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "# !pip install -q torchmetrics\n",
        "# !pip install -q transformers==4.20.1\n",
        "# !pip install -q sentencepiece\n",
        "!pip install -q kaggle --upgrade"
      ],
      "metadata": {
        "id": "z7LfVCI6AMAj"
      },
      "id": "z7LfVCI6AMAj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data = True\n",
        "import os\n",
        "if os.path.exists('/content/data'):\n",
        "    prepare_data = False"
      ],
      "metadata": {
        "id": "SiRW4hbcAN1Y"
      },
      "id": "SiRW4hbcAN1Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if prepare_data:    \n",
        "    from google.colab import files, drive\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for fn in uploaded.keys():\n",
        "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "            name=fn, length=len(uploaded[fn])))\n",
        "    \n",
        "    # Then move kaggle.json into the folder where the API expects to find it.\n",
        "    !mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    !mkdir data && cd data && kaggle competitions download -c feedback-prize-english-language-learning\n",
        "    !unzip /content/data/feedback-prize-english-language-learning.zip -d /content/data/\n",
        "\n",
        "    # !mkdir data/pretrained && cd data/pretrained && kaggle datasets download -d sunpnwt12/fb3-pretrained-s42\n",
        "    # !unzip /content/data/pretrained/fb3-pretrained-s42.zip -d /content/data/pretrained/\n",
        "\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "O4F6kdR-AO8-",
        "outputId": "2bebe091-1b05-470b-a57b-2e2a998bbed3"
      },
      "id": "O4F6kdR-AO8-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-995e87a1-de8f-4cf0-8c0d-20dda1a0ae9f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-995e87a1-de8f-4cf0-8c0d-20dda1a0ae9f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 65 bytes\n",
            "Downloading feedback-prize-english-language-learning.zip to /content/data\n",
            "100% 2.80M/2.80M [00:00<00:00, 5.30MB/s]\n",
            "100% 2.80M/2.80M [00:00<00:00, 4.51MB/s]\n",
            "Archive:  /content/data/feedback-prize-english-language-learning.zip\n",
            "  inflating: /content/data/sample_submission.csv  \n",
            "  inflating: /content/data/test.csv  \n",
            "  inflating: /content/data/train.csv  \n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6c970f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:13.953201Z",
          "iopub.status.busy": "2022-11-23T07:55:13.952390Z",
          "iopub.status.idle": "2022-11-23T07:55:32.028434Z",
          "shell.execute_reply": "2022-11-23T07:55:32.026858Z"
        },
        "papermill": {
          "duration": 18.086396,
          "end_time": "2022-11-23T07:55:32.031307",
          "exception": false,
          "start_time": "2022-11-23T07:55:13.944911",
          "status": "completed"
        },
        "tags": [],
        "id": "7a6c970f",
        "outputId": "a2629c19-c4e7-47b0-e19b-e0b1ec23aed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n",
            "[GCC 9.4.0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iterstart version: 0.1.6\n",
            "torch version: 1.11.0\n",
            "transfromers version: 4.20.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import glob\n",
        "import pytz\n",
        "import gc; gc.enable()\n",
        "import random\n",
        "import warnings\n",
        "import yaml\n",
        "import shutil\n",
        "import types\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from tabulate import tabulate\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(f'python version: {sys.version}') \n",
        "\n",
        "os.system('pip install -q iterative-stratification==0.1.7')\n",
        "import iterstrat\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "print(f'iterstart version: {iterstrat.__version__}')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "\n",
        "print(f'torch version: {torch.__version__}')\n",
        "\n",
        "from torchmetrics.functional import mean_squared_error\n",
        "\n",
        "# os.system('pip install --root-user-action=ignore --force-reinstall transformers==4.22.1')\n",
        "import transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorWithPadding\n",
        "print(f'transfromers version: {transformers.__version__}')\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "316e9ef5",
      "metadata": {
        "papermill": {
          "duration": 0.006253,
          "end_time": "2022-11-23T07:55:32.044774",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.038521",
          "status": "completed"
        },
        "tags": [],
        "id": "316e9ef5"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b9660c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.059470Z",
          "iopub.status.busy": "2022-11-23T07:55:32.058483Z",
          "iopub.status.idle": "2022-11-23T07:55:32.064657Z",
          "shell.execute_reply": "2022-11-23T07:55:32.063006Z"
        },
        "papermill": {
          "duration": 0.015653,
          "end_time": "2022-11-23T07:55:32.066726",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.051073",
          "status": "completed"
        },
        "tags": [],
        "id": "79b9660c"
      },
      "outputs": [],
      "source": [
        "class BASICCONF:\n",
        "    seed = 42\n",
        "    \n",
        "    data_path = '/kaggle/input/feedback-prize-english-language-learning'\n",
        "    \n",
        "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    num_labels = 6\n",
        "    num_folds = 5\n",
        "    \n",
        "    dropout_ratio = 0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d59edc5",
      "metadata": {
        "papermill": {
          "duration": 0.006057,
          "end_time": "2022-11-23T07:55:32.079155",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.073098",
          "status": "completed"
        },
        "tags": [],
        "id": "1d59edc5"
      },
      "source": [
        "# Seeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7228bf34",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.093264Z",
          "iopub.status.busy": "2022-11-23T07:55:32.092910Z",
          "iopub.status.idle": "2022-11-23T07:55:32.101806Z",
          "shell.execute_reply": "2022-11-23T07:55:32.100894Z"
        },
        "papermill": {
          "duration": 0.018463,
          "end_time": "2022-11-23T07:55:32.103882",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.085419",
          "status": "completed"
        },
        "tags": [],
        "id": "7228bf34"
      },
      "outputs": [],
      "source": [
        "#https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\n",
        "def seed_everything(seed: int):    \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything(BASICCONF.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54b1676",
      "metadata": {
        "papermill": {
          "duration": 0.006953,
          "end_time": "2022-11-23T07:55:32.117092",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.110139",
          "status": "completed"
        },
        "tags": [],
        "id": "f54b1676"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a166ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.131137Z",
          "iopub.status.busy": "2022-11-23T07:55:32.130297Z",
          "iopub.status.idle": "2022-11-23T07:55:32.136817Z",
          "shell.execute_reply": "2022-11-23T07:55:32.135966Z"
        },
        "papermill": {
          "duration": 0.015593,
          "end_time": "2022-11-23T07:55:32.138834",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.123241",
          "status": "completed"
        },
        "tags": [],
        "id": "11a166ff"
      },
      "outputs": [],
      "source": [
        "def get_model_path_dict(source, fold_num_list):\n",
        "    model_path_dict = {\n",
        "        'yaml': f'{source}/config.yml',\n",
        "        'config': [cf for cf in Path(source).glob('*_config.pt')][0].as_posix(),\n",
        "        'tokenizer': f'{source}/tokenizers/',\n",
        "        'models': [[f_n, f'{source}/best-epoch-fold{f_n}.pt'] for f_n in fold_num_list],\n",
        "        'tables': {\n",
        "            'train_result': f'{source}/train_result.csv',\n",
        "            'best_result': f'{source}/best_result.csv',\n",
        "            'cv_result': f'{source}/cv_result.csv',\n",
        "        },\n",
        "        'log': f'{source}/log.txt'\n",
        "    }\n",
        "    return model_path_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3f670e5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.152826Z",
          "iopub.status.busy": "2022-11-23T07:55:32.152106Z",
          "iopub.status.idle": "2022-11-23T07:55:32.348559Z",
          "shell.execute_reply": "2022-11-23T07:55:32.347524Z"
        },
        "papermill": {
          "duration": 0.205835,
          "end_time": "2022-11-23T07:55:32.350900",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.145065",
          "status": "completed"
        },
        "tags": [],
        "id": "d3f670e5"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = f'{BASICCONF.data_path}/train.csv'\n",
        "TEST_PATH = f'{BASICCONF.data_path}/test.csv'\n",
        "SAMP_SUB = f'{BASICCONF.data_path}/sample_submission.csv'\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "samp_sup = pd.read_csv(SAMP_SUB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9e66d7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.365204Z",
          "iopub.status.busy": "2022-11-23T07:55:32.364852Z",
          "iopub.status.idle": "2022-11-23T07:55:32.372350Z",
          "shell.execute_reply": "2022-11-23T07:55:32.371307Z"
        },
        "papermill": {
          "duration": 0.017451,
          "end_time": "2022-11-23T07:55:32.374849",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.357398",
          "status": "completed"
        },
        "tags": [],
        "id": "ad9e66d7"
      },
      "outputs": [],
      "source": [
        "class FB3Dataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "#         self.text_id = df['text_id'].reset_index(drop=True)\n",
        "        self.full_texts = df['full_text'].reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "#         self.max_len = self._get_max_len()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.full_texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        token = self._get_token(idx)\n",
        "        \n",
        "        return token\n",
        "    \n",
        "    def _get_token(self, idx):\n",
        "        tokenized = self.tokenizer(\n",
        "                        self.full_texts.loc[idx],\n",
        "                        add_special_tokens=True,\n",
        "#                         max_length=self.max_len,\n",
        "#                         pad_to_max_length=True,\n",
        "                        truncation=True,\n",
        "                        return_tensors=None\n",
        "                )\n",
        "        return {k: torch.tensor(v, dtype=torch.long) for k, v in tokenized.items()} # stack tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4fb6a0",
      "metadata": {
        "papermill": {
          "duration": 0.006633,
          "end_time": "2022-11-23T07:55:32.388606",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.381973",
          "status": "completed"
        },
        "tags": [],
        "id": "bb4fb6a0"
      },
      "source": [
        "\n",
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c46fd3b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.402645Z",
          "iopub.status.busy": "2022-11-23T07:55:32.402348Z",
          "iopub.status.idle": "2022-11-23T07:55:32.449422Z",
          "shell.execute_reply": "2022-11-23T07:55:32.448408Z"
        },
        "papermill": {
          "duration": 0.056922,
          "end_time": "2022-11-23T07:55:32.451667",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.394745",
          "status": "completed"
        },
        "tags": [],
        "id": "2c46fd3b"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/maunish/clrp-pytorch-roberta-finetune/notebook\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, in_features, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.middle_features = hidden_dim\n",
        "        self.W = nn.Linear(in_features, hidden_dim)\n",
        "        self.V = nn.Linear(hidden_dim, 1)\n",
        "        self.out_features = hidden_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        att = torch.tanh(self.W(features))\n",
        "        score = self.V(att)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "# https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/361678\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim),\n",
        "            nn.LayerNorm(in_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(in_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        w = self.attention(x).float()\n",
        "        w[mask==0]=float('-inf')\n",
        "        w = torch.softmax(w,1)\n",
        "        x = torch.sum(w * x, dim=1)\n",
        "        return x    \n",
        "    \n",
        "# https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently\n",
        "class HiddenAttentionPooling(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_size, hiddendim_fc):\n",
        "        super().__init__()\n",
        "        self.num_hidden_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hiddendim_fc = hiddendim_fc\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        q_t = torch.normal(mean=0.0, std=0.02, size=(1, self.hidden_size))\n",
        "        self.q = nn.Parameter(q_t).float()\n",
        "        w_ht = torch.normal(mean=0.0, std=0.02, size=(self.hidden_size, self.hiddendim_fc))\n",
        "        self.w_h = nn.Parameter(w_ht).float()\n",
        "\n",
        "    def forward(self, all_hidden_states):\n",
        "        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n",
        "                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n",
        "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
        "        out = self.attention(hidden_states)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "    def attention(self, h):\n",
        "        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n",
        "        v = F.softmax(v, -1)\n",
        "        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n",
        "        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n",
        "        return v\n",
        "\n",
        "class ConcatPooling(nn.Module):\n",
        "    def __init__(self, pooling_last=4):\n",
        "        super().__init__()\n",
        "        self.pooling_last = pooling_last\n",
        "        \n",
        "    def forward(self, all_hidden_states):\n",
        "        concat_pooling = torch.cat(tuple(all_hidden_states[-l] for l in range(1, self.pooling_last + 1)), -1)\n",
        "#         concat_pooling = concat_pooling.mean(dim=1) # average instead of select only one\n",
        "        concat_pooling = concat_pooling[:, 0] # select the first one\n",
        "        return concat_pooling\n",
        "\n",
        "# https://www.kaggle.com/competitions/google-quest-challenge/discussion/129840\n",
        "class WeightedLayerPooling(nn.Module):\n",
        "    def __init__(self, num_layers=12, init_std=0.02):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        weights_init = torch.zeros(self.num_layers).float()\n",
        "        weights_init.data[:-1] = -3\n",
        "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, all_hidden_states):\n",
        "        all_layer_encoders = torch.stack(\n",
        "            [self.dropout(layer) for layer in all_hidden_states[-self.num_layers:]], dim=0\n",
        "        )\n",
        "        averaged_layers = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * all_layer_encoders).sum(0)\n",
        "        return averaged_layers\n",
        "        \n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "        return mean_embeddings\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, conf, fold_num, config_path=None):\n",
        "        super().__init__()\n",
        "        if not config_path:\n",
        "            self.model_conf = AutoConfig.from_pretrained(conf.model_name, output_hidden_states=True)\n",
        "            self.model_conf = self._set_dropout(self.model_conf)\n",
        "            self.backbone = AutoModel.from_pretrained(conf.model_name, config=self.model_conf)\n",
        "        else:\n",
        "            self.model_conf = torch.load(config_path)\n",
        "            self.backbone = AutoModel.from_config(self.model_conf)\n",
        "        if conf.gradient_checkpointing:\n",
        "            self.backbone.gradient_checkpointing_enable()\n",
        "        \n",
        "        if not config_path:\n",
        "            for layer in self.backbone.encoder.layer[-conf.reinit_last_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    self._init_weights(module)\n",
        "                    \n",
        "        self.pooling_strategy = conf.pooling_strategy_list[fold_num]\n",
        "        if self.pooling_strategy == 'mean_pooling':\n",
        "            self.pooler = MeanPooling()\n",
        "            \n",
        "        elif self.pooling_strategy == 'concat_pooling':\n",
        "            self.pooler = ConcatPooling(conf.concat_pooling_last)\n",
        "            \n",
        "        elif self.pooling_strategy == 'attn_pooling': \n",
        "            self.attn_pooler = AttentionPooling(self.model_conf.hidden_size) \n",
        "            for attn_module in self.attn_pooler.modules():\n",
        "                self._init_weights(attn_module)\n",
        "            \n",
        "        elif self.pooling_strategy == 'wlp_attn_pooling':\n",
        "            self.wlp_pooler = WeightedLayerPooling(self.model_conf.num_hidden_layers, self.model_conf.initializer_range)\n",
        "            self.attn_pooler = AttentionPooling(self.model_conf.hidden_size)\n",
        "            for attn_module in self.attn_pooler.modules():\n",
        "                self._init_weights(attn_module)\n",
        "                \n",
        "        elif self.pooling_strategy == 'concat_h_attn_mean_pooling':\n",
        "            self.hattn_pooler = HiddenAttentionPooling(self.model_conf.num_hidden_layers, self.model_conf.hidden_size, self.model_conf.hidden_size)\n",
        "            self.mean_pooler = MeanPooling()\n",
        "\n",
        "        elif self.pooling_strategy == 'concat_attn_mean_pooling':\n",
        "            self.attn_pooler = AttentionPooling(self.model_conf.hidden_size)\n",
        "            for attn_module in self.attn_pooler.modules():\n",
        "                self._init_weights(attn_module)\n",
        "            self.mean_pooler = MeanPooling()\n",
        "            \n",
        "        else:\n",
        "            raise Exception('Invalid pooling strategy')\n",
        "\n",
        "        if self.pooling_strategy in ['mean_pooling', 'attn_pooling', 'wlp_attn_pooling']:\n",
        "            hidden_size = self.model_conf.hidden_size\n",
        "        elif self.pooling_strategy in ['concat_pooling']:\n",
        "            hidden_size = self.model_conf.hidenn_size * conf.concat_pooling_last\n",
        "        elif self.pooling_strategy in ['concat_h_attn_mean_pooling', 'concat_attn_mean_pooling']:\n",
        "            hidden_size = self.model_conf.hidden_size * 2\n",
        "        else:\n",
        "            raise Exception('Cannot create fc layer.')\n",
        "            \n",
        "        self.multi_dropout = conf.multi_dropout\n",
        "        if self.multi_dropout:\n",
        "            self.dropout1 = nn.Dropout(conf.multi_dropout_p[0])\n",
        "            self.dropout2 = nn.Dropout(conf.multi_dropout_p[1])\n",
        "            self.dropout3 = nn.Dropout(conf.multi_dropout_p[2])\n",
        "            self.dropout4 = nn.Dropout(conf.multi_dropout_p[3])\n",
        "            self.dropout5 = nn.Dropout(conf.multi_dropout_p[4])\n",
        "        else:\n",
        "            self.dropout0 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, conf.num_labels)\n",
        "        if conf.reinit_method is not None:\n",
        "            self._init_weights2_([self.fc], conf.reinit_method)\n",
        "        else:\n",
        "            self._init_weights(self.fc)\n",
        "\n",
        "        self.use_ln = conf.use_ln\n",
        "        if self.use_ln:\n",
        "            self.ln = nn.LayerNorm(hidden_size)\n",
        "            self._init_weights(self.ln)\n",
        "        \n",
        "    def _set_dropout(self, conf, ratio=0.):\n",
        "        conf.attention_probs_dropout_prob = ratio\n",
        "        conf.hidden_dropout = ratio \n",
        "        conf.hidden_dropout_prob = ratio\n",
        "        conf.pooler_dropout = ratio\n",
        "        return conf\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.model_conf.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.model_conf.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "    \n",
        "    def _init_weights2_(self, module_lst, method):\n",
        "        for module in module_lst:\n",
        "            for param in module.parameters():\n",
        "                if param.dim() > 1:\n",
        "                    if method == 'kaiming_normal':\n",
        "                        nn.init.kaiming_normal_(param)\n",
        "                    elif method == 'xavier_normal':\n",
        "                        nn.init.xavier_normal_(param)\n",
        "                    elif method == 'orthoganol':\n",
        "                        nn.init.orthogonal_(param)\n",
        "                    else:\n",
        "                        raise Exception('The method is invalid')\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        backbone_outputs = self.backbone(**inputs)\n",
        "        if self.pooling_strategy == 'mean_pooling':\n",
        "            last_hidden_states = backbone_outputs['last_hidden_state']\n",
        "            pooler_outputs = self.pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            \n",
        "        elif self.pooling_strategy == 'concat_pooling':\n",
        "            all_hidden_states = torch.stack(backbone_outputs['hidden_states'])\n",
        "            pooler_outputs = self.pooler(all_hidden_states)\n",
        "            \n",
        "        elif self.pooling_strategy == 'attn_pooling':\n",
        "            last_hidden_states = backbone_outputs['last_hidden_state']\n",
        "            pooler_outputs = self.attn_pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            \n",
        "        elif self.pooling_strategy == 'wlp_attn_pooling':\n",
        "            all_hidden_states = torch.stack(backbone_outputs['hidden_states'])\n",
        "            wlp_pooler = self.wlp_pooler(all_hidden_states)\n",
        "            pooler_outputs = self.attn_pooler(wlp_pooler, inputs['attention_mask'])\n",
        "            \n",
        "        elif self.pooling_strategy == 'concat_h_attn_mean_pooling':\n",
        "            last_hidden_states = backbone_outputs['last_hidden_state']\n",
        "            all_hidden_states = torch.stack(backbone_outputs['hidden_states'])\n",
        "            hattn_outputs = self.hattn_pooler(all_hidden_states)\n",
        "            mean_outputs = self.mean_pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            pooler_outputs = torch.cat((hattn_outputs, mean_outputs), -1)\n",
        "            \n",
        "        elif self.pooling_strategy == 'concat_attn_mean_pooling':\n",
        "            last_hidden_states = backbone_outputs['last_hidden_state']\n",
        "            attn_outputs = self.attn_pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            mean_outputs = self.mean_pooler(last_hidden_states, inputs['attention_mask'])\n",
        "            pooler_outputs = torch.cat((attn_outputs, mean_outputs), -1)\n",
        "            \n",
        "        if self.use_ln:\n",
        "            pooler_outputs = self.ln(pooler_outputs)\n",
        "            \n",
        "        if self.multi_dropout:\n",
        "            x1 = self.fc(self.dropout1(pooler_outputs))\n",
        "            x2 = self.fc(self.dropout2(pooler_outputs))\n",
        "            x3 = self.fc(self.dropout3(pooler_outputs))\n",
        "            x4 = self.fc(self.dropout4(pooler_outputs))\n",
        "            x5 = self.fc(self.dropout5(pooler_outputs))\n",
        "            \n",
        "            outputs = (x1 + x2 + x3 + x4 + x5) / 5\n",
        "\n",
        "        else:\n",
        "            outputs = self.fc(self.dropout0(pooler_outputs))\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e45244",
      "metadata": {
        "papermill": {
          "duration": 0.005996,
          "end_time": "2022-11-23T07:55:32.464795",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.458799",
          "status": "completed"
        },
        "tags": [],
        "id": "67e45244"
      },
      "source": [
        "# Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6798bd93",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.478402Z",
          "iopub.status.busy": "2022-11-23T07:55:32.478064Z",
          "iopub.status.idle": "2022-11-23T07:55:32.485964Z",
          "shell.execute_reply": "2022-11-23T07:55:32.485075Z"
        },
        "papermill": {
          "duration": 0.017108,
          "end_time": "2022-11-23T07:55:32.487994",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.470886",
          "status": "completed"
        },
        "tags": [],
        "id": "6798bd93"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train/notebook\n",
        "class Averager:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "    def get_average(self):\n",
        "        return self.avg\n",
        "    \n",
        "    def get_value(self):\n",
        "        return self.val"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca93aa72",
      "metadata": {
        "papermill": {
          "duration": 0.006071,
          "end_time": "2022-11-23T07:55:32.500434",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.494363",
          "status": "completed"
        },
        "tags": [],
        "id": "ca93aa72"
      },
      "source": [
        "# Helper for Pseudo Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee91fc71",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.514254Z",
          "iopub.status.busy": "2022-11-23T07:55:32.513814Z",
          "iopub.status.idle": "2022-11-23T07:55:32.523594Z",
          "shell.execute_reply": "2022-11-23T07:55:32.522588Z"
        },
        "papermill": {
          "duration": 0.019083,
          "end_time": "2022-11-23T07:55:32.525656",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.506573",
          "status": "completed"
        },
        "tags": [],
        "id": "ee91fc71"
      },
      "outputs": [],
      "source": [
        "def inference_fn_pl(model, device, dataloader, tokenizer):\n",
        "    predictions = []\n",
        "    \n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    for inputs in tqdm(dataloader):\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "        predictions.append(outputs)\n",
        "        \n",
        "    predictions_stack = torch.stack(predictions)\n",
        "    \n",
        "    del model, dataloader\n",
        "    \n",
        "    return predictions_stack\n",
        "\n",
        "def inference_cv_pl(conf, model_path_dict, df):\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    \n",
        "    fold_predictions_list = []\n",
        "    text_id_df = df['text_id']\n",
        "    \n",
        "    model_config_path = model_path_dict['config']\n",
        "    tokenizer = get_tokenizer_pl(model_path_dict['tokenizer'])\n",
        "    \n",
        "    print(f'Using \"{model_config_path}\"')\n",
        "    \n",
        "    for fold_num, model_path in model_path_dict['models']:\n",
        "        model = load_model_pl(conf, device, fold_num, model_config_path, model_path)\n",
        "        test_dataloader = get_dataloader_pl(df, tokenizer, 1)\n",
        "        \n",
        "        fold_predictions = inference_fn_pl(model, device, test_dataloader, tokenizer)\n",
        "        fold_predictions_list.append(fold_predictions)\n",
        "    \n",
        "    cv_mean = torch.mean(torch.stack(fold_predictions_list), dim=0).squeeze().cpu().numpy()\n",
        "    cv_mean_df = pd.DataFrame(cv_mean, columns=conf.target_cols)\n",
        "\n",
        "    cv_df = pd.concat([text_id_df, cv_mean_df], axis=1)\n",
        "    \n",
        "    del model, test_dataloader\n",
        "    \n",
        "    return cv_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950cd943",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.539750Z",
          "iopub.status.busy": "2022-11-23T07:55:32.539460Z",
          "iopub.status.idle": "2022-11-23T07:55:32.555135Z",
          "shell.execute_reply": "2022-11-23T07:55:32.554279Z"
        },
        "papermill": {
          "duration": 0.024944,
          "end_time": "2022-11-23T07:55:32.557105",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.532161",
          "status": "completed"
        },
        "tags": [],
        "id": "950cd943"
      },
      "outputs": [],
      "source": [
        "def get_dataloader_pl(df, tokenizer, batch_num):\n",
        "    \n",
        "    test_dataset = FB3Dataset(df, tokenizer)\n",
        "    \n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_num,\n",
        "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, padding='longest'),\n",
        "        num_workers=4,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    \n",
        "    return test_dataloader\n",
        "\n",
        "def get_tokenizer_pl(tokenizer_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "def load_model_pl(conf, device, fold_num, config_path, pretrained_model_path):\n",
        "    model = CustomModel(conf, fold_num, config_path=config_path)\n",
        "    state_dict = torch.load(pretrained_model_path, map_location=device)['model_state_dict']\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(f'Loaded \"{pretrained_model_path}\"')\n",
        "    return model\n",
        "\n",
        "def pick_sample(num_data):\n",
        "    pl_data_path = '/kaggle/input/feedback-prize-2021/train/'\n",
        "    file_list = glob.glob(pl_data_path + '*.txt')\n",
        "    if isinstance(num_data, int):\n",
        "        selected_files = random.sample(file_list, num_data)\n",
        "    elif num_data == 'all':\n",
        "        selected_files = file_list\n",
        "    else:\n",
        "        raise Exception('Invalid num_data input')\n",
        "    \n",
        "    return selected_files\n",
        "\n",
        "def make_pl_df(model_path_dict, selected_files):\n",
        "    with open(model_path_dict['yaml']) as file:\n",
        "        conf = types.SimpleNamespace(**yaml.safe_load(file))\n",
        "    \n",
        "    ids_text_list = []\n",
        "\n",
        "    for file in selected_files:\n",
        "        ids = Path(file).stem\n",
        "        with open(file, mode='r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            full_text = ''.join(lines)\n",
        "            ids_text_list.append([ids, full_text])\n",
        "            \n",
        "    fb_df = pd.DataFrame(ids_text_list, columns=['text_id', 'full_text'])\n",
        "    pl_df = inference_cv_pl(conf, model_path_dict, fb_df)\n",
        "    pl_df = fb_df.merge(pl_df, how='inner', on='text_id')\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return pl_df\n",
        "\n",
        "def concat_pl(train_df, pl_df):\n",
        "    concated_df = pd.concat([train_df, pl_df], axis=0)\n",
        "    return concated_df.drop_duplicates(subset=['text_id']).reset_index(drop=True)\n",
        "\n",
        "def combine_result(df_list, method=None):\n",
        "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    text_id_df = df_list[0]['text_id']\n",
        "    np_pred_list = [df[target_cols].values for df in df_list]\n",
        "    \n",
        "    if method == 'mean':\n",
        "        pred_mean = np.mean(np_pred_list, axis=0)\n",
        "        pred_mean_df = pd.DataFrame(pred_mean, columns=target_cols)\n",
        "        result = pd.concat([text_id_df, pred_mean_df], axis=1)\n",
        "    \n",
        "    if isinstance(method, list):\n",
        "        if len(df_list) != len(method):\n",
        "            raise Exception('Weight len or df_list is not equivalent')\n",
        "        else:\n",
        "            pred_weighted_mean = np.average(np_pred_list, axis=0, weights=method)\n",
        "            pred_weighted_mean_df = pd.DataFrame(pred_weighted_mean, columns=target_cols)\n",
        "            result = pd.concat([text_id_df, pred_weighted_mean_df], axis=1)\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35621902",
      "metadata": {
        "papermill": {
          "duration": 0.006004,
          "end_time": "2022-11-23T07:55:32.569384",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.563380",
          "status": "completed"
        },
        "tags": [],
        "id": "35621902"
      },
      "source": [
        "# Making Pseudo Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd57596d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.584319Z",
          "iopub.status.busy": "2022-11-23T07:55:32.582803Z",
          "iopub.status.idle": "2022-11-23T07:55:32.588330Z",
          "shell.execute_reply": "2022-11-23T07:55:32.587442Z"
        },
        "papermill": {
          "duration": 0.014679,
          "end_time": "2022-11-23T07:55:32.590275",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.575596",
          "status": "completed"
        },
        "tags": [],
        "id": "cd57596d"
      },
      "outputs": [],
      "source": [
        "model_path_list_s42 = [\n",
        "    '/kaggle/input/fb3-deberta-v3-base/exp43s42',\n",
        "    '/kaggle/input/fb3-deberta-v3-large/exp50s42',\n",
        "    '/kaggle/input/fb3-bigbird-roberta-base/exp49s42',\n",
        "    '/kaggle/input/fb3-roberta-large/exp54s42',\n",
        "    '/kaggle/input/fb3-longformer-large/exp57s42',\n",
        "]\n",
        "\n",
        "model_path_list_s12 = [\n",
        "    '/kaggle/input/fb3-deberta-v3-base/exp43s12',\n",
        "    '/kaggle/input/fb3-deberta-v3-large/exp50s12',\n",
        "    '/kaggle/input/fb3-roberta-large/exp54s12',\n",
        "    '/kaggle/input/fb3-longformer-large/exp57s12',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d23178",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.604328Z",
          "iopub.status.busy": "2022-11-23T07:55:32.603481Z",
          "iopub.status.idle": "2022-11-23T07:55:32.612008Z",
          "shell.execute_reply": "2022-11-23T07:55:32.611189Z"
        },
        "papermill": {
          "duration": 0.017604,
          "end_time": "2022-11-23T07:55:32.613993",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.596389",
          "status": "completed"
        },
        "tags": [],
        "id": "d4d23178"
      },
      "outputs": [],
      "source": [
        "def make_pl_fold(model_path_list, seed, sample_num, weight_list):\n",
        "    if len(model_path_list) != len(weight_list):\n",
        "        raise Exception('model_path_list and weight_list len is not equivalent')\n",
        "        \n",
        "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    seed_everything(seed)\n",
        "    selected_samples = pick_sample(sample_num)\n",
        "    \n",
        "    for f in [0, 1, 2, 3]:\n",
        "        print(f'Start creating pl fold {f}')\n",
        "        pl_f_targets_list = []\n",
        "        for model_path in model_path_list:\n",
        "            model_path_dict_path= get_model_path_dict(model_path, [f])\n",
        "            pl_f_df = make_pl_df(model_path_dict_path, selected_samples)\n",
        "            pl_f_targets_list.append(pl_f_df[target_cols].values)\n",
        "            \n",
        "#         mean_pl_f = np.mean(pl_f_targets_list, axis=0)\n",
        "        mean_pl_f = np.average(pl_f_targets_list, axis=0, weights=weight_list)\n",
        "        mean_pl_f_df = pd.DataFrame(mean_pl_f, columns=target_cols)\n",
        "        full_pl_f_df = pd.concat([pl_f_df[['text_id', 'full_text']], mean_pl_f_df], axis=1)\n",
        "        full_pl_f_df.to_csv(Path('./', f'pl_s{seed}_f{f}.csv', index=False))\n",
        "        print('==========================')\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2afe9bd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.628238Z",
          "iopub.status.busy": "2022-11-23T07:55:32.627435Z",
          "iopub.status.idle": "2022-11-23T07:55:32.632095Z",
          "shell.execute_reply": "2022-11-23T07:55:32.631269Z"
        },
        "papermill": {
          "duration": 0.013673,
          "end_time": "2022-11-23T07:55:32.634066",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.620393",
          "status": "completed"
        },
        "tags": [],
        "id": "c2afe9bd"
      },
      "outputs": [],
      "source": [
        "# 42\n",
        "# [0.5251053345458068, 0.22003221662481717, 0.8542359345530548, 0.42515310855913024, 0.42911461149981833]\n",
        "# 0.44652450528900384\n",
        "\n",
        "# [0.6273137771770656, 0.8787619240919684, 0.45825589785978954, 0.48178205912577515]\n",
        "# 0.44665768571664594\n",
        "\n",
        "# 12\n",
        "# [0.7031174591345418, 0.17526885209382478, 0.8931524975803248, 0.5275256661576303, 0.6753998852501416]\n",
        "# 0.44691928761081545\n",
        "\n",
        "# [0.8074348842019702, 0.9534285472400551, 0.579247898557129, 0.7839275842173122]\n",
        "# 0.4469713746715202"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4bed0aa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.648583Z",
          "iopub.status.busy": "2022-11-23T07:55:32.647001Z",
          "iopub.status.idle": "2022-11-23T07:55:32.651808Z",
          "shell.execute_reply": "2022-11-23T07:55:32.650972Z"
        },
        "papermill": {
          "duration": 0.013612,
          "end_time": "2022-11-23T07:55:32.653743",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.640131",
          "status": "completed"
        },
        "tags": [],
        "id": "b4bed0aa"
      },
      "outputs": [],
      "source": [
        "sample_num = 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea481ebf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-23T07:55:32.667273Z",
          "iopub.status.busy": "2022-11-23T07:55:32.666981Z",
          "iopub.status.idle": "2022-11-23T09:41:14.309110Z",
          "shell.execute_reply": "2022-11-23T09:41:14.307435Z"
        },
        "papermill": {
          "duration": 6341.653212,
          "end_time": "2022-11-23T09:41:14.313159",
          "exception": false,
          "start_time": "2022-11-23T07:55:32.659947",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "a9325983a7ae4bbbb2ed153728a70f7a",
            "b4a4d535cd5c48748feebfe657851cec",
            "9de0dd7a752a43cdbca9c5c675f5fb12",
            "1fd6180106584493ac0a964caf6d97b9",
            "048b686aa2af4db4bdb6ea891d37927a",
            "d12dc32ea0fc484980ed3a2e5cabc71a",
            "eb458c706d0b4c05966900b4490986da",
            "f1529c757abf407ea8e1062cdb3d9529",
            "7ed284aa2af3437db1533f1f071c87cf",
            "63172d03e0484e94be718cdaeb373887",
            "4a8a85b03ab04c76a3030e24eb10f2e6",
            "514e9e72f30e40d99d1e6158478cec72",
            "ae5c606fa90a4a8b9f0eede3657fa861",
            "18bd0cac622f4bc3baf9daaea597d69c",
            "5607de045ba04b6482276414171cf074",
            "7373efc9350f4b7da552e39e091039f5",
            "478d0bd1143d41079a672581db58d05e",
            "b3992ca950c64edbbde97d9f14c1aeac",
            "eacc55ca246c48619853a526868c37d7",
            "16a6f540670a43b08d5f862d92960798"
          ]
        },
        "id": "ea481ebf",
        "outputId": "8077cf42-6815-47c4-ce54-cf0388de4620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start creating pl fold 0\n",
            "Using \"/kaggle/input/fb3-deberta-v3-base/exp43s42/deberta-v3-base_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-deberta-v3-base/exp43s42/best-epoch-fold0.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9325983a7ae4bbbb2ed153728a70f7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-deberta-v3-large/exp50s42/deberta-v3-large_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-deberta-v3-large/exp50s42/best-epoch-fold0.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4a4d535cd5c48748feebfe657851cec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-bigbird-roberta-base/exp49s42/bigbird-roberta-base_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-bigbird-roberta-base/exp49s42/best-epoch-fold0.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9de0dd7a752a43cdbca9c5c675f5fb12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 420 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-roberta-large/exp54s42/roberta-large_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-roberta-large/exp54s42/best-epoch-fold0.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fd6180106584493ac0a964caf6d97b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-longformer-large/exp57s42/longformer-large-4096_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-longformer-large/exp57s42/best-epoch-fold0.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "048b686aa2af4db4bdb6ea891d37927a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================\n",
            "\n",
            "Start creating pl fold 1\n",
            "Using \"/kaggle/input/fb3-deberta-v3-base/exp43s42/deberta-v3-base_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-deberta-v3-base/exp43s42/best-epoch-fold1.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d12dc32ea0fc484980ed3a2e5cabc71a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-deberta-v3-large/exp50s42/deberta-v3-large_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-deberta-v3-large/exp50s42/best-epoch-fold1.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb458c706d0b4c05966900b4490986da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-bigbird-roberta-base/exp49s42/bigbird-roberta-base_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-bigbird-roberta-base/exp49s42/best-epoch-fold1.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1529c757abf407ea8e1062cdb3d9529",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 420 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-roberta-large/exp54s42/roberta-large_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-roberta-large/exp54s42/best-epoch-fold1.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ed284aa2af3437db1533f1f071c87cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-longformer-large/exp57s42/longformer-large-4096_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-longformer-large/exp57s42/best-epoch-fold1.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63172d03e0484e94be718cdaeb373887",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================\n",
            "\n",
            "Start creating pl fold 2\n",
            "Using \"/kaggle/input/fb3-deberta-v3-base/exp43s42/deberta-v3-base_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-deberta-v3-base/exp43s42/best-epoch-fold2.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a8a85b03ab04c76a3030e24eb10f2e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-deberta-v3-large/exp50s42/deberta-v3-large_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-deberta-v3-large/exp50s42/best-epoch-fold2.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "514e9e72f30e40d99d1e6158478cec72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-bigbird-roberta-base/exp49s42/bigbird-roberta-base_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-bigbird-roberta-base/exp49s42/best-epoch-fold2.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae5c606fa90a4a8b9f0eede3657fa861",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 420 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-roberta-large/exp54s42/roberta-large_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-roberta-large/exp54s42/best-epoch-fold2.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18bd0cac622f4bc3baf9daaea597d69c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-longformer-large/exp57s42/longformer-large-4096_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-longformer-large/exp57s42/best-epoch-fold2.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5607de045ba04b6482276414171cf074",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================\n",
            "\n",
            "Start creating pl fold 3\n",
            "Using \"/kaggle/input/fb3-deberta-v3-base/exp43s42/deberta-v3-base_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-deberta-v3-base/exp43s42/best-epoch-fold3.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7373efc9350f4b7da552e39e091039f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-deberta-v3-large/exp50s42/deberta-v3-large_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-deberta-v3-large/exp50s42/best-epoch-fold3.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "478d0bd1143d41079a672581db58d05e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-bigbird-roberta-base/exp49s42/bigbird-roberta-base_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-bigbird-roberta-base/exp49s42/best-epoch-fold3.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3992ca950c64edbbde97d9f14c1aeac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 420 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-roberta-large/exp54s42/roberta-large_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-roberta-large/exp54s42/best-epoch-fold3.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eacc55ca246c48619853a526868c37d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using \"/kaggle/input/fb3-longformer-large/exp57s42/longformer-large-4096_config.pt\"\n",
            "Loaded \"/kaggle/input/fb3-longformer-large/exp57s42/best-epoch-fold3.pt\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16a6f540670a43b08d5f862d92960798",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# make_pl_fold(model_path_list_s42, 42, sample_num, [0.5251053345458068, 0.22003221662481717, 0.8542359345530548, 0.42515310855913024, 0.42911461149981833])\n",
        "make_pl_fold(model_path_list_s12, 12, sample_num, [0.8074348842019702, 0.9534285472400551, 0.579247898557129, 0.7839275842173122])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94dcdd6e",
      "metadata": {
        "papermill": {
          "duration": 0.017998,
          "end_time": "2022-11-23T09:41:14.605918",
          "exception": false,
          "start_time": "2022-11-23T09:41:14.587920",
          "status": "completed"
        },
        "tags": [],
        "id": "94dcdd6e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6372.491697,
      "end_time": "2022-11-23T09:41:17.610830",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-11-23T07:55:05.119133",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}