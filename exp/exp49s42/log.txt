Date: 2022-11-14 19:45:55.837122+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: google/bigbird-roberta-base
Model_config: BigBirdConfig {
  "_name_or_path": "google/bigbird-roberta-base",
  "architectures": [
    "BigBirdForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.0,
  "attention_type": "block_sparse",
  "block_size": 64,
  "bos_token_id": 1,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu_new",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4096,
  "model_type": "big_bird",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_random_blocks": 3,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "rescale_embeddings": false,
  "sep_token_id": 66,
  "transformers_version": "4.20.1",
  "type_vocab_size": 2,
  "use_bias": true,
  "use_cache": true,
  "vocab_size": 50358
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 3e-05, LowerLayer: 5e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: dynamic_padding

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.38028 (2.38028) | LR: 0.00000055 | TIME: 0:00:05 |
[TRAIN F0] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.38242 (1.23880) | LR: 0.00002253 | TIME: 0:01:46 |
[TRAIN F0] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.20486 (0.72394) | LR: 0.00004451 | TIME: 0:03:22 |
[TRAIN F0] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.11969 (0.53699) | LR: 0.00004994 | TIME: 0:05:09 |
[TRAIN F0] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.13070 (0.43891) | LR: 0.00004968 | TIME: 0:06:46 |
[TRAIN F0] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.15650 (0.37882) | LR: 0.00004921 | TIME: 0:08:21 |
[TRAIN F0] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.14990 (0.33814) | LR: 0.00004854 | TIME: 0:10:02 |
[TRAIN F0] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.16305 (0.30895) | LR: 0.00004767 | TIME: 0:11:40 |
[TRAIN F0] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.12890 (0.28680) | LR: 0.00004662 | TIME: 0:13:12 |
[TRAIN F0] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.09391 (0.26938) | LR: 0.00004538 | TIME: 0:14:51 |
[TRAIN F0] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.13020 (0.26775) | LR: 0.00004521 | TIME: 0:15:00 |

VALID_LOOP
[VALID F0] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.06280 (0.06280) | TIME: 0:00:01 |
[VALID F0] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.08886 (0.10697) | TIME: 0:00:25 |
[VALID F0] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.09916 (0.10840) | TIME: 0:00:49 |
[VALID F0] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.10083 (0.11007) | TIME: 0:01:12 |
[VALID F0] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.08671 (0.10973) | TIME: 0:01:13 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.26775 |      0.10973 |  0.46942 | 0.500 | 0.476 | 0.436 | 0.463 | 0.489 | 0.452 | 0:16:13 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4694223701953888

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.16389 (0.16389) | LR: 0.00004518 | TIME: 0:00:02 |
[TRAIN F0] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.09826 (0.11357) | LR: 0.00004374 | TIME: 0:01:39 |
[TRAIN F0] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.08236 (0.11353) | LR: 0.00004215 | TIME: 0:03:25 |
[TRAIN F0] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.08691 (0.11630) | LR: 0.00004042 | TIME: 0:05:00 |
[TRAIN F0] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.12720 (0.11726) | LR: 0.00003856 | TIME: 0:06:38 |
[TRAIN F0] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.11057 (0.11666) | LR: 0.00003658 | TIME: 0:08:18 |
[TRAIN F0] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.12366 (0.11624) | LR: 0.00003451 | TIME: 0:09:57 |
[TRAIN F0] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.12270 (0.11709) | LR: 0.00003235 | TIME: 0:11:43 |
[TRAIN F0] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.14332 (0.11771) | LR: 0.00003014 | TIME: 0:13:22 |
[TRAIN F0] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.10883 (0.11774) | LR: 0.00002788 | TIME: 0:14:59 |
[TRAIN F0] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.07597 (0.11744) | LR: 0.00002760 | TIME: 0:15:09 |

VALID_LOOP
[VALID F0] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.05174 (0.05174) | TIME: 0:00:01 |
[VALID F0] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.09090 (0.10266) | TIME: 0:00:25 |
[VALID F0] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.11579 (0.10556) | TIME: 0:00:48 |
[VALID F0] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.10749 (0.10682) | TIME: 0:01:12 |
[VALID F0] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.09876 (0.10661) | TIME: 0:01:13 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11744 |      0.10661 |  0.46234 | 0.488 | 0.460 | 0.419 | 0.466 | 0.490 | 0.451 | 0:16:23 |


[SAVED] EPOCH: 2 | MCRMSE: 0.46234288811683655

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.08120 (0.08120) | LR: 0.00002754 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.12533 (0.10232) | LR: 0.00002526 | TIME: 0:01:35 |
[TRAIN F0] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.10792 (0.10149) | LR: 0.00002297 | TIME: 0:03:16 |
[TRAIN F0] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.07227 (0.09981) | LR: 0.00002070 | TIME: 0:04:49 |
[TRAIN F0] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.09172 (0.09969) | LR: 0.00001847 | TIME: 0:06:27 |
[TRAIN F0] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.10709 (0.10127) | LR: 0.00001629 | TIME: 0:08:09 |
[TRAIN F0] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.09256 (0.10187) | LR: 0.00001419 | TIME: 0:09:48 |
[TRAIN F0] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.11170 (0.10087) | LR: 0.00001217 | TIME: 0:11:27 |
[TRAIN F0] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.08866 (0.09957) | LR: 0.00001026 | TIME: 0:13:04 |
[TRAIN F0] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.11888 (0.09886) | LR: 0.00000848 | TIME: 0:14:43 |
[TRAIN F0] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.05629 (0.09875) | LR: 0.00000827 | TIME: 0:14:56 |

VALID_LOOP
[VALID F0] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.05506 (0.05506) | TIME: 0:00:01 |
[VALID F0] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.09568 (0.10183) | TIME: 0:00:25 |
[VALID F0] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.13377 (0.10429) | TIME: 0:00:48 |
[VALID F0] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.10625 (0.10472) | TIME: 0:01:12 |
[VALID F0] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.08867 (0.10451) | TIME: 0:01:13 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.09875 |      0.10451 |  0.45754 | 0.501 | 0.448 | 0.413 | 0.460 | 0.479 | 0.445 | 0:16:09 |


[SAVED] EPOCH: 3 | MCRMSE: 0.45754364132881165

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.08700 (0.08700) | LR: 0.00000822 | TIME: 0:00:02 |
[TRAIN F0] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.07151 (0.08980) | LR: 0.00000660 | TIME: 0:01:38 |
[TRAIN F0] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.11879 (0.09044) | LR: 0.00000513 | TIME: 0:03:21 |
[TRAIN F0] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.07009 (0.08842) | LR: 0.00000383 | TIME: 0:05:00 |
[TRAIN F0] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.05064 (0.08944) | LR: 0.00000270 | TIME: 0:06:31 |
[TRAIN F0] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.06499 (0.08875) | LR: 0.00000176 | TIME: 0:08:11 |
[TRAIN F0] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.10907 (0.08815) | LR: 0.00000102 | TIME: 0:09:54 |
[TRAIN F0] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.11836 (0.08848) | LR: 0.00000047 | TIME: 0:11:37 |
[TRAIN F0] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.09361 (0.08827) | LR: 0.00000013 | TIME: 0:13:17 |
[TRAIN F0] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.07055 (0.08814) | LR: 0.00000000 | TIME: 0:14:51 |
[TRAIN F0] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.10512 (0.08806) | LR: 0.00000000 | TIME: 0:15:02 |

VALID_LOOP
[VALID F0] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.05775 (0.05775) | TIME: 0:00:01 |
[VALID F0] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.09699 (0.10105) | TIME: 0:00:25 |
[VALID F0] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.12411 (0.10268) | TIME: 0:00:48 |
[VALID F0] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.10558 (0.10296) | TIME: 0:01:12 |
[VALID F0] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.07250 (0.10267) | TIME: 0:01:13 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.08806 |      0.10267 |  0.45362 | 0.485 | 0.446 | 0.413 | 0.458 | 0.473 | 0.446 | 0:16:15 |


[SAVED] EPOCH: 4 | MCRMSE: 0.45362189412117004


----------------------------------- FOLD 0 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45362     0.48521   0.44642       0.41329        0.45821    0.47307        0.44554

################################### END OF FOlD 0 ###################################


Date: 2022-11-14 20:51:11.695928+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: google/bigbird-roberta-base
Model_config: BigBirdConfig {
  "_name_or_path": "google/bigbird-roberta-base",
  "architectures": [
    "BigBirdForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.0,
  "attention_type": "block_sparse",
  "block_size": 64,
  "bos_token_id": 1,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu_new",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4096,
  "model_type": "big_bird",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_random_blocks": 3,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "rescale_embeddings": false,
  "sep_token_id": 66,
  "transformers_version": "4.20.1",
  "type_vocab_size": 2,
  "use_bias": true,
  "use_cache": true,
  "vocab_size": 50358
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 3e-05, LowerLayer: 5e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: dynamic_padding

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.08123 (2.08123) | LR: 0.00000055 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.33997 (1.22447) | LR: 0.00002253 | TIME: 0:01:47 |
[TRAIN F1] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.18792 (0.71108) | LR: 0.00004451 | TIME: 0:03:21 |
[TRAIN F1] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.13702 (0.52481) | LR: 0.00004994 | TIME: 0:05:02 |
[TRAIN F1] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.13386 (0.42886) | LR: 0.00004968 | TIME: 0:06:52 |
[TRAIN F1] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.11925 (0.37415) | LR: 0.00004921 | TIME: 0:08:32 |
[TRAIN F1] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.08810 (0.33669) | LR: 0.00004854 | TIME: 0:10:13 |
[TRAIN F1] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.13865 (0.30782) | LR: 0.00004767 | TIME: 0:11:44 |
[TRAIN F1] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.12096 (0.28563) | LR: 0.00004662 | TIME: 0:13:23 |
[TRAIN F1] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.16482 (0.26835) | LR: 0.00004538 | TIME: 0:14:56 |
[TRAIN F1] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.14601 (0.26656) | LR: 0.00004521 | TIME: 0:15:10 |

VALID_LOOP
[VALID F1] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.06781 (0.06781) | TIME: 0:00:01 |
[VALID F1] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.11248 (0.12368) | TIME: 0:00:25 |
[VALID F1] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.12202 (0.12224) | TIME: 0:00:49 |
[VALID F1] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.09590 (0.11916) | TIME: 0:01:13 |
[VALID F1] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.07313 (0.11915) | TIME: 0:01:13 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.26656 |      0.11915 |   0.4896 | 0.511 | 0.477 | 0.435 | 0.474 | 0.511 | 0.529 | 0:16:23 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4895976781845093

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.10034 (0.10034) | LR: 0.00004518 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.12460 (0.12558) | LR: 0.00004374 | TIME: 0:01:44 |
[TRAIN F1] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.12752 (0.11971) | LR: 0.00004215 | TIME: 0:03:21 |
[TRAIN F1] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.11694 (0.11683) | LR: 0.00004042 | TIME: 0:05:03 |
[TRAIN F1] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.09532 (0.11642) | LR: 0.00003856 | TIME: 0:06:42 |
[TRAIN F1] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.11370 (0.11783) | LR: 0.00003658 | TIME: 0:08:28 |
[TRAIN F1] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.06981 (0.11768) | LR: 0.00003451 | TIME: 0:10:00 |
[TRAIN F1] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.07620 (0.11668) | LR: 0.00003235 | TIME: 0:11:37 |
[TRAIN F1] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.12333 (0.11639) | LR: 0.00003014 | TIME: 0:13:19 |
[TRAIN F1] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.12497 (0.11535) | LR: 0.00002788 | TIME: 0:15:00 |
[TRAIN F1] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.10673 (0.11541) | LR: 0.00002760 | TIME: 0:15:12 |

VALID_LOOP
[VALID F1] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.09664 (0.09664) | TIME: 0:00:01 |
[VALID F1] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.08429 (0.11572) | TIME: 0:00:25 |
[VALID F1] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.09997 (0.11650) | TIME: 0:00:49 |
[VALID F1] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.08525 (0.11685) | TIME: 0:01:13 |
[VALID F1] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.03250 (0.11698) | TIME: 0:01:13 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11541 |      0.11698 |   0.4852 | 0.496 | 0.453 | 0.477 | 0.465 | 0.494 | 0.527 | 0:16:26 |


[SAVED] EPOCH: 2 | MCRMSE: 0.48520365357398987

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.11034 (0.11034) | LR: 0.00002754 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.07214 (0.10753) | LR: 0.00002526 | TIME: 0:01:37 |
[TRAIN F1] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.12383 (0.10473) | LR: 0.00002297 | TIME: 0:03:16 |
[TRAIN F1] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.10659 (0.10431) | LR: 0.00002070 | TIME: 0:04:50 |
[TRAIN F1] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.18977 (0.10455) | LR: 0.00001847 | TIME: 0:06:34 |
[TRAIN F1] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.12583 (0.10292) | LR: 0.00001629 | TIME: 0:08:06 |
[TRAIN F1] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.09888 (0.10181) | LR: 0.00001419 | TIME: 0:09:39 |
[TRAIN F1] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.07261 (0.10167) | LR: 0.00001217 | TIME: 0:11:25 |
[TRAIN F1] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.09891 (0.10056) | LR: 0.00001026 | TIME: 0:13:07 |
[TRAIN F1] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.09313 (0.09951) | LR: 0.00000848 | TIME: 0:14:52 |
[TRAIN F1] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.08421 (0.09943) | LR: 0.00000827 | TIME: 0:15:08 |

VALID_LOOP
[VALID F1] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.07995 (0.07995) | TIME: 0:00:01 |
[VALID F1] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.06762 (0.11038) | TIME: 0:00:25 |
[VALID F1] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.09635 (0.10928) | TIME: 0:00:49 |
[VALID F1] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.08810 (0.10888) | TIME: 0:01:13 |
[VALID F1] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.03985 (0.10909) | TIME: 0:01:13 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.09943 |      0.10909 |  0.46816 | 0.496 | 0.457 | 0.428 | 0.466 | 0.497 | 0.466 | 0:16:21 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4681595265865326

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.08943 (0.08943) | LR: 0.00000822 | TIME: 0:00:02 |
[TRAIN F1] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.07110 (0.08341) | LR: 0.00000660 | TIME: 0:01:41 |
[TRAIN F1] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.06956 (0.08719) | LR: 0.00000513 | TIME: 0:03:29 |
[TRAIN F1] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.06579 (0.08679) | LR: 0.00000383 | TIME: 0:05:14 |
[TRAIN F1] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.05727 (0.08694) | LR: 0.00000270 | TIME: 0:06:51 |
[TRAIN F1] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.08302 (0.08747) | LR: 0.00000176 | TIME: 0:08:27 |
[TRAIN F1] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.07378 (0.08764) | LR: 0.00000102 | TIME: 0:10:11 |
[TRAIN F1] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.10310 (0.08784) | LR: 0.00000047 | TIME: 0:11:42 |
[TRAIN F1] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.11279 (0.08744) | LR: 0.00000013 | TIME: 0:13:19 |
[TRAIN F1] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.06756 (0.08698) | LR: 0.00000000 | TIME: 0:14:53 |
[TRAIN F1] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.11475 (0.08704) | LR: 0.00000000 | TIME: 0:15:05 |

VALID_LOOP
[VALID F1] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.08270 (0.08270) | TIME: 0:00:01 |
[VALID F1] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.07462 (0.11022) | TIME: 0:00:25 |
[VALID F1] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.09623 (0.10825) | TIME: 0:00:49 |
[VALID F1] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.08809 (0.10767) | TIME: 0:01:13 |
[VALID F1] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.03850 (0.10784) | TIME: 0:01:13 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.08704 |      0.10784 |  0.46551 | 0.490 | 0.454 | 0.424 | 0.468 | 0.495 | 0.463 | 0:16:19 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4655059576034546


----------------------------------- FOLD 1 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.46551     0.48986   0.45352       0.42386        0.46789    0.49452         0.4634

################################### END OF FOlD 1 ###################################


Date: 2022-11-14 21:56:55.715318+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: google/bigbird-roberta-base
Model_config: BigBirdConfig {
  "_name_or_path": "google/bigbird-roberta-base",
  "architectures": [
    "BigBirdForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.0,
  "attention_type": "block_sparse",
  "block_size": 64,
  "bos_token_id": 1,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu_new",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4096,
  "model_type": "big_bird",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_random_blocks": 3,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "rescale_embeddings": false,
  "sep_token_id": 66,
  "transformers_version": "4.20.1",
  "type_vocab_size": 2,
  "use_bias": true,
  "use_cache": true,
  "vocab_size": 50358
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 3e-05, LowerLayer: 5e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: dynamic_padding

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.26316 (2.26316) | LR: 0.00000055 | TIME: 0:00:05 |
[TRAIN F2] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.27069 (1.23798) | LR: 0.00002253 | TIME: 0:01:59 |
[TRAIN F2] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.19506 (0.72409) | LR: 0.00004451 | TIME: 0:03:40 |
[TRAIN F2] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.10594 (0.53548) | LR: 0.00004994 | TIME: 0:05:22 |
[TRAIN F2] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.09127 (0.43557) | LR: 0.00004968 | TIME: 0:07:04 |
[TRAIN F2] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.20492 (0.38069) | LR: 0.00004921 | TIME: 0:08:45 |
[TRAIN F2] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.13468 (0.34199) | LR: 0.00004854 | TIME: 0:10:22 |
[TRAIN F2] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.10883 (0.31269) | LR: 0.00004767 | TIME: 0:12:11 |
[TRAIN F2] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.08983 (0.29105) | LR: 0.00004662 | TIME: 0:14:01 |
[TRAIN F2] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.12709 (0.27340) | LR: 0.00004538 | TIME: 0:15:46 |
[TRAIN F2] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.10460 (0.27123) | LR: 0.00004521 | TIME: 0:15:56 |

VALID_LOOP
[VALID F2] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.18332 (0.18332) | TIME: 0:00:01 |
[VALID F2] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.18287 (0.15169) | TIME: 0:00:24 |
[VALID F2] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.15400 (0.15430) | TIME: 0:00:47 |
[VALID F2] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.12233 (0.15322) | TIME: 0:01:09 |
[VALID F2] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.36028 (0.15334) | TIME: 0:01:10 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.27123 |      0.15334 |  0.55623 | 0.588 | 0.485 | 0.527 | 0.604 | 0.632 | 0.501 | 0:17:07 |


[SAVED] EPOCH: 1 | MCRMSE: 0.556230902671814

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.15077 (0.15077) | LR: 0.00004518 | TIME: 0:00:05 |
[TRAIN F2] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.12151 (0.12352) | LR: 0.00004374 | TIME: 0:01:43 |
[TRAIN F2] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.12161 (0.11999) | LR: 0.00004215 | TIME: 0:03:22 |
[TRAIN F2] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.09350 (0.11599) | LR: 0.00004042 | TIME: 0:05:08 |
[TRAIN F2] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.12138 (0.11518) | LR: 0.00003856 | TIME: 0:06:47 |
[TRAIN F2] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.12470 (0.11555) | LR: 0.00003658 | TIME: 0:08:30 |
[TRAIN F2] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.06548 (0.11513) | LR: 0.00003451 | TIME: 0:10:12 |
[TRAIN F2] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.09346 (0.11516) | LR: 0.00003235 | TIME: 0:11:59 |
[TRAIN F2] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.09996 (0.11658) | LR: 0.00003014 | TIME: 0:13:38 |
[TRAIN F2] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.10101 (0.11551) | LR: 0.00002788 | TIME: 0:15:20 |
[TRAIN F2] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.10292 (0.11550) | LR: 0.00002760 | TIME: 0:15:33 |

VALID_LOOP
[VALID F2] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.14309 (0.14309) | TIME: 0:00:01 |
[VALID F2] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.09050 (0.11637) | TIME: 0:00:24 |
[VALID F2] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.10542 (0.11392) | TIME: 0:00:47 |
[VALID F2] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.11500 (0.11378) | TIME: 0:01:09 |
[VALID F2] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.15982 (0.11353) | TIME: 0:01:10 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |       0.1155 |      0.11353 |   0.4778 | 0.504 | 0.488 | 0.439 | 0.478 | 0.495 | 0.464 | 0:16:43 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4778021275997162

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.13656 (0.13656) | LR: 0.00002754 | TIME: 0:00:05 |
[TRAIN F2] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.08062 (0.09379) | LR: 0.00002526 | TIME: 0:01:57 |
[TRAIN F2] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.10355 (0.09736) | LR: 0.00002297 | TIME: 0:03:44 |
[TRAIN F2] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.13025 (0.09890) | LR: 0.00002070 | TIME: 0:05:32 |
[TRAIN F2] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.07139 (0.09745) | LR: 0.00001847 | TIME: 0:07:02 |
[TRAIN F2] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.13260 (0.09762) | LR: 0.00001629 | TIME: 0:08:46 |
[TRAIN F2] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.10479 (0.09661) | LR: 0.00001419 | TIME: 0:10:22 |
[TRAIN F2] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.07730 (0.09594) | LR: 0.00001217 | TIME: 0:12:03 |
[TRAIN F2] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.09823 (0.09552) | LR: 0.00001026 | TIME: 0:13:48 |
[TRAIN F2] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.09774 (0.09581) | LR: 0.00000848 | TIME: 0:15:29 |
[TRAIN F2] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.06869 (0.09570) | LR: 0.00000827 | TIME: 0:15:42 |

VALID_LOOP
[VALID F2] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.15360 (0.15360) | TIME: 0:00:01 |
[VALID F2] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.09738 (0.11058) | TIME: 0:00:24 |
[VALID F2] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.10216 (0.10904) | TIME: 0:00:47 |
[VALID F2] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.12468 (0.10873) | TIME: 0:01:09 |
[VALID F2] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.16609 (0.10856) | TIME: 0:01:10 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |       0.0957 |      0.10856 |  0.46715 | 0.489 | 0.461 | 0.426 | 0.473 | 0.489 | 0.465 | 0:16:52 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4671489894390106

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.05875 (0.05875) | LR: 0.00000822 | TIME: 0:00:02 |
[TRAIN F2] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.10383 (0.08695) | LR: 0.00000660 | TIME: 0:01:40 |
[TRAIN F2] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.05641 (0.08383) | LR: 0.00000513 | TIME: 0:03:27 |
[TRAIN F2] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.11196 (0.08414) | LR: 0.00000383 | TIME: 0:05:13 |
[TRAIN F2] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.08723 (0.08509) | LR: 0.00000270 | TIME: 0:06:54 |
[TRAIN F2] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.09877 (0.08455) | LR: 0.00000176 | TIME: 0:08:35 |
[TRAIN F2] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.07294 (0.08432) | LR: 0.00000102 | TIME: 0:10:22 |
[TRAIN F2] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.07014 (0.08423) | LR: 0.00000047 | TIME: 0:12:07 |
[TRAIN F2] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.07663 (0.08519) | LR: 0.00000013 | TIME: 0:13:44 |
[TRAIN F2] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.10939 (0.08500) | LR: 0.00000000 | TIME: 0:15:27 |
[TRAIN F2] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.07700 (0.08476) | LR: 0.00000000 | TIME: 0:15:42 |

VALID_LOOP
[VALID F2] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.14861 (0.14861) | TIME: 0:00:01 |
[VALID F2] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.09514 (0.10959) | TIME: 0:00:24 |
[VALID F2] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.10135 (0.10790) | TIME: 0:00:47 |
[VALID F2] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.13008 (0.10813) | TIME: 0:01:09 |
[VALID F2] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.13255 (0.10789) | TIME: 0:01:10 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.08476 |      0.10789 |  0.46559 | 0.490 | 0.450 | 0.427 | 0.475 | 0.487 | 0.465 | 0:16:52 |


[SAVED] EPOCH: 4 | MCRMSE: 0.465591698884964


----------------------------------- FOLD 2 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.46559     0.48998    0.4497        0.4274        0.47459    0.48706        0.46483

################################### END OF FOlD 2 ###################################


Date: 2022-11-14 23:04:44.421260+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: google/bigbird-roberta-base
Model_config: BigBirdConfig {
  "_name_or_path": "google/bigbird-roberta-base",
  "architectures": [
    "BigBirdForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.0,
  "attention_type": "block_sparse",
  "block_size": 64,
  "bos_token_id": 1,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu_new",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 4096,
  "model_type": "big_bird",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_random_blocks": 3,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "rescale_embeddings": false,
  "sep_token_id": 66,
  "transformers_version": "4.20.1",
  "type_vocab_size": 2,
  "use_bias": true,
  "use_cache": true,
  "vocab_size": 50358
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 3e-05, LowerLayer: 5e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: dynamic_padding

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.40339 (2.40339) | LR: 0.00000055 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.27791 (1.27238) | LR: 0.00002253 | TIME: 0:01:43 |
[TRAIN F3] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.14286 (0.74573) | LR: 0.00004451 | TIME: 0:03:26 |
[TRAIN F3] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.12634 (0.55162) | LR: 0.00004994 | TIME: 0:05:11 |
[TRAIN F3] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.10531 (0.44796) | LR: 0.00004968 | TIME: 0:06:46 |
[TRAIN F3] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.12691 (0.38405) | LR: 0.00004921 | TIME: 0:08:33 |
[TRAIN F3] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.22130 (0.34326) | LR: 0.00004854 | TIME: 0:10:15 |
[TRAIN F3] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.11916 (0.31214) | LR: 0.00004767 | TIME: 0:11:53 |
[TRAIN F3] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.11064 (0.29244) | LR: 0.00004662 | TIME: 0:13:34 |
[TRAIN F3] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.11683 (0.27471) | LR: 0.00004538 | TIME: 0:15:16 |
[TRAIN F3] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.12526 (0.27263) | LR: 0.00004521 | TIME: 0:15:27 |

VALID_LOOP
[VALID F3] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.13545 (0.13545) | TIME: 0:00:01 |
[VALID F3] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.11284 (0.12264) | TIME: 0:00:24 |
[VALID F3] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.14652 (0.12803) | TIME: 0:00:46 |
[VALID F3] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.11526 (0.12878) | TIME: 0:01:09 |
[VALID F3] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.07875 (0.12842) | TIME: 0:01:09 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.27263 |      0.12842 |  0.50791 | 0.557 | 0.483 | 0.455 | 0.494 | 0.580 | 0.479 | 0:16:37 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5079077482223511

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.15819 (0.15819) | LR: 0.00004518 | TIME: 0:00:02 |
[TRAIN F3] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.13125 (0.12398) | LR: 0.00004374 | TIME: 0:01:40 |
[TRAIN F3] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.11105 (0.12681) | LR: 0.00004215 | TIME: 0:03:26 |
[TRAIN F3] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.09855 (0.12330) | LR: 0.00004042 | TIME: 0:05:06 |
[TRAIN F3] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.14723 (0.12179) | LR: 0.00003856 | TIME: 0:06:50 |
[TRAIN F3] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.11543 (0.12033) | LR: 0.00003658 | TIME: 0:08:31 |
[TRAIN F3] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.17694 (0.12005) | LR: 0.00003451 | TIME: 0:10:13 |
[TRAIN F3] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.07692 (0.11880) | LR: 0.00003235 | TIME: 0:11:46 |
[TRAIN F3] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.16015 (0.11825) | LR: 0.00003014 | TIME: 0:13:29 |
[TRAIN F3] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.12825 (0.11740) | LR: 0.00002788 | TIME: 0:15:14 |
[TRAIN F3] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.14969 (0.11710) | LR: 0.00002760 | TIME: 0:15:28 |

VALID_LOOP
[VALID F3] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.12204 (0.12204) | TIME: 0:00:01 |
[VALID F3] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.06529 (0.10579) | TIME: 0:00:23 |
[VALID F3] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.12434 (0.10900) | TIME: 0:00:46 |
[VALID F3] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.09426 (0.10957) | TIME: 0:01:08 |
[VALID F3] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.03513 (0.10922) | TIME: 0:01:09 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |       0.1171 |      0.10922 |  0.46909 | 0.507 | 0.489 | 0.427 | 0.450 | 0.490 | 0.452 | 0:16:37 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4690851867198944

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.05891 (0.05891) | LR: 0.00002754 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.12091 (0.10248) | LR: 0.00002526 | TIME: 0:01:43 |
[TRAIN F3] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.07529 (0.10279) | LR: 0.00002297 | TIME: 0:03:20 |
[TRAIN F3] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.06902 (0.10502) | LR: 0.00002070 | TIME: 0:05:01 |
[TRAIN F3] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.12007 (0.10283) | LR: 0.00001847 | TIME: 0:06:56 |
[TRAIN F3] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.09880 (0.10127) | LR: 0.00001629 | TIME: 0:08:38 |
[TRAIN F3] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.06554 (0.10078) | LR: 0.00001419 | TIME: 0:10:17 |
[TRAIN F3] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.06826 (0.09997) | LR: 0.00001217 | TIME: 0:12:07 |
[TRAIN F3] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.08088 (0.09890) | LR: 0.00001026 | TIME: 0:13:44 |
[TRAIN F3] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.14656 (0.09799) | LR: 0.00000848 | TIME: 0:15:13 |
[TRAIN F3] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.09600 (0.09804) | LR: 0.00000827 | TIME: 0:15:27 |

VALID_LOOP
[VALID F3] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.11618 (0.11618) | TIME: 0:00:01 |
[VALID F3] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.06386 (0.10268) | TIME: 0:00:23 |
[VALID F3] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.12141 (0.10466) | TIME: 0:00:46 |
[VALID F3] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.09367 (0.10554) | TIME: 0:01:08 |
[VALID F3] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.03293 (0.10520) | TIME: 0:01:09 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.09804 |       0.1052 |   0.4601 | 0.496 | 0.458 | 0.422 | 0.451 | 0.485 | 0.448 | 0:16:36 |


[SAVED] EPOCH: 3 | MCRMSE: 0.46009561419487

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.07323 (0.07323) | LR: 0.00000822 | TIME: 0:00:02 |
[TRAIN F3] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.07277 (0.08752) | LR: 0.00000660 | TIME: 0:01:39 |
[TRAIN F3] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.08489 (0.08989) | LR: 0.00000513 | TIME: 0:03:21 |
[TRAIN F3] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.06622 (0.08870) | LR: 0.00000383 | TIME: 0:05:09 |
[TRAIN F3] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.12599 (0.08816) | LR: 0.00000270 | TIME: 0:06:51 |
[TRAIN F3] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.07874 (0.08767) | LR: 0.00000176 | TIME: 0:08:41 |
[TRAIN F3] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.06738 (0.08719) | LR: 0.00000102 | TIME: 0:10:19 |
[TRAIN F3] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.08554 (0.08716) | LR: 0.00000047 | TIME: 0:11:59 |
[TRAIN F3] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.08033 (0.08703) | LR: 0.00000013 | TIME: 0:13:33 |
[TRAIN F3] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.08787 (0.08658) | LR: 0.00000000 | TIME: 0:15:18 |
[TRAIN F3] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.09975 (0.08636) | LR: 0.00000000 | TIME: 0:15:31 |

VALID_LOOP
[VALID F3] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.11703 (0.11703) | TIME: 0:00:01 |
[VALID F3] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.06240 (0.10210) | TIME: 0:00:23 |
[VALID F3] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.11677 (0.10397) | TIME: 0:00:46 |
[VALID F3] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.09812 (0.10489) | TIME: 0:01:08 |
[VALID F3] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.03279 (0.10458) | TIME: 0:01:09 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.08636 |      0.10458 |  0.45862 | 0.493 | 0.457 | 0.420 | 0.450 | 0.485 | 0.447 | 0:16:41 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4586161673069


----------------------------------- FOLD 3 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45862     0.49339   0.45652       0.41992        0.44956    0.48529        0.44702

################################### END OF FOlD 3 ###################################


