Date: 2022-11-19 16:27:56.987316+07:00 (GMT+7)
Mode: EXPERIMENTING_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: roberta-large
Model_config: RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 512

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 1/4 | STEP: 000/366 | LOSS: 1.96238 (1.96238) | LR: 0.00000033 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.39879 (1.16570) | LR: 0.00001352 | TIME: 0:01:38 |
[TRAIN F0] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.16628 (0.69827) | LR: 0.00002670 | TIME: 0:03:14 |
[TRAIN F0] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.15190 (0.51798) | LR: 0.00002996 | TIME: 0:04:51 |
[TRAIN F0] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.16120 (0.42816) | LR: 0.00002981 | TIME: 0:06:27 |
[TRAIN F0] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.22407 (0.37304) | LR: 0.00002953 | TIME: 0:08:04 |
[TRAIN F0] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.17660 (0.33402) | LR: 0.00002913 | TIME: 0:09:40 |
[TRAIN F0] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.12401 (0.30706) | LR: 0.00002860 | TIME: 0:11:17 |
[TRAIN F0] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.19088 (0.28824) | LR: 0.00002797 | TIME: 0:12:54 |
[TRAIN F0] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.16868 (0.27324) | LR: 0.00002723 | TIME: 0:14:30 |
[TRAIN F0] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.27385 (0.27181) | LR: 0.00002713 | TIME: 0:14:42 |

VALID_LOOP
[VALID F0] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.11469 (0.11469) | TIME: 0:00:02 |
[VALID F0] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.11568 (0.13765) | TIME: 0:00:34 |
[VALID F0] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.12160 (0.13720) | TIME: 0:01:06 |
[VALID F0] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.16431 (0.14039) | TIME: 0:01:37 |
[VALID F0] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.17255 (0.14063) | TIME: 0:01:38 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.27181 |      0.14063 |  0.53237 | 0.522 | 0.581 | 0.503 | 0.547 | 0.517 | 0.524 | 0:16:21 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5323690176010132

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.16348 (0.16348) | LR: 0.00002711 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.10617 (0.13883) | LR: 0.00002625 | TIME: 0:01:40 |
[TRAIN F0] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.09768 (0.12599) | LR: 0.00002529 | TIME: 0:03:16 |
[TRAIN F0] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.10405 (0.12660) | LR: 0.00002425 | TIME: 0:04:52 |
[TRAIN F0] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.08700 (0.12430) | LR: 0.00002313 | TIME: 0:06:28 |
[TRAIN F0] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.09503 (0.12360) | LR: 0.00002195 | TIME: 0:08:05 |
[TRAIN F0] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.10433 (0.12170) | LR: 0.00002070 | TIME: 0:09:41 |
[TRAIN F0] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.12970 (0.12253) | LR: 0.00001941 | TIME: 0:11:17 |
[TRAIN F0] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.09604 (0.12203) | LR: 0.00001808 | TIME: 0:12:53 |
[TRAIN F0] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.18247 (0.12212) | LR: 0.00001673 | TIME: 0:14:30 |
[TRAIN F0] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.10572 (0.12191) | LR: 0.00001656 | TIME: 0:14:42 |

VALID_LOOP
[VALID F0] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.07374 (0.07374) | TIME: 0:00:02 |
[VALID F0] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.11447 (0.12306) | TIME: 0:00:34 |
[VALID F0] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.12652 (0.12342) | TIME: 0:01:06 |
[VALID F0] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.11910 (0.12653) | TIME: 0:01:37 |
[VALID F0] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.23313 (0.12678) | TIME: 0:01:38 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.12191 |      0.12678 |  0.50434 | 0.561 | 0.489 | 0.475 | 0.470 | 0.478 | 0.553 | 0:16:21 |


[SAVED] EPOCH: 2 | MCRMSE: 0.5043355822563171

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.08669 (0.08669) | LR: 0.00001652 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.10070 (0.10182) | LR: 0.00001515 | TIME: 0:01:39 |
[TRAIN F0] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.10285 (0.09841) | LR: 0.00001378 | TIME: 0:03:16 |
[TRAIN F0] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.08927 (0.09850) | LR: 0.00001242 | TIME: 0:04:53 |
[TRAIN F0] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.07387 (0.09753) | LR: 0.00001108 | TIME: 0:06:31 |
[TRAIN F0] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.07919 (0.09716) | LR: 0.00000977 | TIME: 0:08:07 |
[TRAIN F0] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.04474 (0.09666) | LR: 0.00000851 | TIME: 0:09:43 |
[TRAIN F0] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.09119 (0.09662) | LR: 0.00000730 | TIME: 0:11:20 |
[TRAIN F0] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.08995 (0.09619) | LR: 0.00000616 | TIME: 0:12:58 |
[TRAIN F0] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.06863 (0.09569) | LR: 0.00000509 | TIME: 0:14:34 |
[TRAIN F0] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.09524 (0.09559) | LR: 0.00000496 | TIME: 0:14:46 |

VALID_LOOP
[VALID F0] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.05951 (0.05951) | TIME: 0:00:01 |
[VALID F0] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.09394 (0.10801) | TIME: 0:00:33 |
[VALID F0] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.08979 (0.10652) | TIME: 0:01:05 |
[VALID F0] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.12047 (0.10754) | TIME: 0:01:37 |
[VALID F0] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.09643 (0.10725) | TIME: 0:01:38 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.09559 |      0.10725 |  0.46352 | 0.500 | 0.469 | 0.413 | 0.472 | 0.471 | 0.457 | 0:16:25 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4635179340839386

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.10456 (0.10456) | LR: 0.00000493 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.09097 (0.07868) | LR: 0.00000396 | TIME: 0:01:40 |
[TRAIN F0] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.09622 (0.08028) | LR: 0.00000308 | TIME: 0:03:16 |
[TRAIN F0] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.07785 (0.08112) | LR: 0.00000230 | TIME: 0:04:52 |
[TRAIN F0] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.09788 (0.08138) | LR: 0.00000162 | TIME: 0:06:28 |
[TRAIN F0] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.09586 (0.08030) | LR: 0.00000106 | TIME: 0:08:05 |
[TRAIN F0] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.08751 (0.08009) | LR: 0.00000061 | TIME: 0:09:41 |
[TRAIN F0] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.09066 (0.08010) | LR: 0.00000028 | TIME: 0:11:17 |
[TRAIN F0] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.07356 (0.07942) | LR: 0.00000008 | TIME: 0:12:53 |
[TRAIN F0] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.07132 (0.07946) | LR: 0.00000000 | TIME: 0:14:31 |
[TRAIN F0] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.06899 (0.07951) | LR: 0.00000000 | TIME: 0:14:43 |

VALID_LOOP
[VALID F0] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.05613 (0.05613) | TIME: 0:00:01 |
[VALID F0] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.09196 (0.10126) | TIME: 0:00:33 |
[VALID F0] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.09509 (0.10059) | TIME: 0:01:05 |
[VALID F0] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.11186 (0.10215) | TIME: 0:01:37 |
[VALID F0] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.12339 (0.10201) | TIME: 0:01:38 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.07951 |      0.10201 |    0.452 | 0.480 | 0.445 | 0.412 | 0.460 | 0.470 | 0.445 | 0:16:21 |


[SAVED] EPOCH: 4 | MCRMSE: 0.45199814438819885


----------------------------------- FOLD 0 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
   0.452     0.48038   0.44472       0.41222        0.46023    0.46971        0.44473

################################### END OF FOlD 0 ###################################


Date: 2022-11-19 17:48:27.063737+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: roberta-large
Model_config: RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 512

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.73681 (2.73681) | LR: 0.00000033 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.28729 (1.21592) | LR: 0.00001352 | TIME: 0:01:35 |
[TRAIN F1] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.19888 (0.73172) | LR: 0.00002670 | TIME: 0:03:11 |
[TRAIN F1] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.11675 (0.55218) | LR: 0.00002996 | TIME: 0:04:48 |
[TRAIN F1] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.18268 (0.45281) | LR: 0.00002981 | TIME: 0:06:26 |
[TRAIN F1] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.14980 (0.39196) | LR: 0.00002953 | TIME: 0:08:03 |
[TRAIN F1] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.13001 (0.34996) | LR: 0.00002913 | TIME: 0:09:41 |
[TRAIN F1] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.12965 (0.31943) | LR: 0.00002860 | TIME: 0:11:19 |
[TRAIN F1] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.14839 (0.29844) | LR: 0.00002797 | TIME: 0:12:56 |
[TRAIN F1] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.14711 (0.28303) | LR: 0.00002723 | TIME: 0:14:33 |
[TRAIN F1] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.11802 (0.28093) | LR: 0.00002713 | TIME: 0:14:46 |

VALID_LOOP
[VALID F1] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.10826 (0.10826) | TIME: 0:00:02 |
[VALID F1] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.08171 (0.12898) | TIME: 0:00:34 |
[VALID F1] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.13578 (0.12761) | TIME: 0:01:07 |
[VALID F1] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.13647 (0.12815) | TIME: 0:01:39 |
[VALID F1] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.11327 (0.12832) | TIME: 0:01:40 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.28093 |      0.12832 |   0.5081 | 0.501 | 0.481 | 0.453 | 0.483 | 0.589 | 0.542 | 0:16:26 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5080950260162354

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.10875 (0.10875) | LR: 0.00002711 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.16279 (0.12097) | LR: 0.00002625 | TIME: 0:01:41 |
[TRAIN F1] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.06149 (0.12114) | LR: 0.00002529 | TIME: 0:03:18 |
[TRAIN F1] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.13433 (0.12089) | LR: 0.00002425 | TIME: 0:04:57 |
[TRAIN F1] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.11370 (0.12067) | LR: 0.00002313 | TIME: 0:06:35 |
[TRAIN F1] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.14501 (0.12167) | LR: 0.00002195 | TIME: 0:08:12 |
[TRAIN F1] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.13605 (0.12129) | LR: 0.00002070 | TIME: 0:09:50 |
[TRAIN F1] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.13918 (0.12052) | LR: 0.00001941 | TIME: 0:11:27 |
[TRAIN F1] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.09761 (0.11959) | LR: 0.00001808 | TIME: 0:13:04 |
[TRAIN F1] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.13337 (0.11876) | LR: 0.00001673 | TIME: 0:14:42 |
[TRAIN F1] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.12044 (0.11874) | LR: 0.00001656 | TIME: 0:14:54 |

VALID_LOOP
[VALID F1] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.09964 (0.09964) | TIME: 0:00:02 |
[VALID F1] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.07652 (0.11360) | TIME: 0:00:34 |
[VALID F1] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.09552 (0.11154) | TIME: 0:01:07 |
[VALID F1] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.09268 (0.11011) | TIME: 0:01:39 |
[VALID F1] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.03354 (0.11017) | TIME: 0:01:40 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11874 |      0.11017 |  0.47103 | 0.497 | 0.473 | 0.448 | 0.458 | 0.482 | 0.467 | 0:16:34 |


[SAVED] EPOCH: 2 | MCRMSE: 0.47103336453437805

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.07263 (0.07263) | LR: 0.00001652 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.06816 (0.09875) | LR: 0.00001515 | TIME: 0:01:41 |
[TRAIN F1] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.13415 (0.09883) | LR: 0.00001378 | TIME: 0:03:18 |
[TRAIN F1] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.13111 (0.09883) | LR: 0.00001242 | TIME: 0:04:57 |
[TRAIN F1] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.09307 (0.09594) | LR: 0.00001108 | TIME: 0:06:34 |
[TRAIN F1] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.05482 (0.09371) | LR: 0.00000977 | TIME: 0:08:11 |
[TRAIN F1] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.06436 (0.09403) | LR: 0.00000851 | TIME: 0:09:48 |
[TRAIN F1] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.09315 (0.09383) | LR: 0.00000730 | TIME: 0:11:27 |
[TRAIN F1] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.07758 (0.09355) | LR: 0.00000616 | TIME: 0:13:04 |
[TRAIN F1] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.07114 (0.09367) | LR: 0.00000509 | TIME: 0:14:40 |
[TRAIN F1] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.07470 (0.09368) | LR: 0.00000496 | TIME: 0:14:53 |

VALID_LOOP
[VALID F1] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.09003 (0.09003) | TIME: 0:00:01 |
[VALID F1] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.07448 (0.10559) | TIME: 0:00:34 |
[VALID F1] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.08713 (0.10580) | TIME: 0:01:06 |
[VALID F1] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.08887 (0.10487) | TIME: 0:01:39 |
[VALID F1] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.05138 (0.10477) | TIME: 0:01:40 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.09368 |      0.10477 |   0.4587 | 0.488 | 0.448 | 0.421 | 0.455 | 0.484 | 0.456 | 0:16:33 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4587007462978363

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.05291 (0.05291) | LR: 0.00000493 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.09550 (0.07653) | LR: 0.00000396 | TIME: 0:01:41 |
[TRAIN F1] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.05787 (0.07676) | LR: 0.00000308 | TIME: 0:03:18 |
[TRAIN F1] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.05432 (0.07688) | LR: 0.00000230 | TIME: 0:04:55 |
[TRAIN F1] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.14651 (0.07619) | LR: 0.00000162 | TIME: 0:06:33 |
[TRAIN F1] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.07293 (0.07643) | LR: 0.00000106 | TIME: 0:08:11 |
[TRAIN F1] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.06863 (0.07631) | LR: 0.00000061 | TIME: 0:09:48 |
[TRAIN F1] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.10940 (0.07691) | LR: 0.00000028 | TIME: 0:11:26 |
[TRAIN F1] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.08176 (0.07606) | LR: 0.00000008 | TIME: 0:13:03 |
[TRAIN F1] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.06075 (0.07576) | LR: 0.00000000 | TIME: 0:14:40 |
[TRAIN F1] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.06767 (0.07571) | LR: 0.00000000 | TIME: 0:14:52 |

VALID_LOOP
[VALID F1] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.09302 (0.09302) | TIME: 0:00:01 |
[VALID F1] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.07289 (0.10537) | TIME: 0:00:34 |
[VALID F1] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.08453 (0.10514) | TIME: 0:01:06 |
[VALID F1] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.08891 (0.10452) | TIME: 0:01:39 |
[VALID F1] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.05644 (0.10445) | TIME: 0:01:39 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.07571 |      0.10445 |  0.45808 | 0.486 | 0.446 | 0.421 | 0.455 | 0.482 | 0.458 | 0:16:32 |


[SAVED] EPOCH: 4 | MCRMSE: 0.458076149225235


----------------------------------- FOLD 1 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45808     0.48587   0.44648       0.42124        0.45545    0.48185        0.45757

################################### END OF FOlD 1 ###################################


Date: 2022-11-19 18:55:15.181687+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: roberta-large
Model_config: RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 512

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.21264 (2.21264) | LR: 0.00000033 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.54812 (1.14258) | LR: 0.00001352 | TIME: 0:01:41 |
[TRAIN F2] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.19298 (0.69725) | LR: 0.00002670 | TIME: 0:03:18 |
[TRAIN F2] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.17984 (0.52221) | LR: 0.00002996 | TIME: 0:04:56 |
[TRAIN F2] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.17807 (0.42783) | LR: 0.00002981 | TIME: 0:06:33 |
[TRAIN F2] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.14270 (0.37394) | LR: 0.00002953 | TIME: 0:08:10 |
[TRAIN F2] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.13127 (0.33483) | LR: 0.00002913 | TIME: 0:09:48 |
[TRAIN F2] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.14483 (0.30629) | LR: 0.00002860 | TIME: 0:11:25 |
[TRAIN F2] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.09869 (0.28504) | LR: 0.00002797 | TIME: 0:13:02 |
[TRAIN F2] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.15007 (0.26967) | LR: 0.00002723 | TIME: 0:14:40 |
[TRAIN F2] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.12563 (0.26791) | LR: 0.00002713 | TIME: 0:14:52 |

VALID_LOOP
[VALID F2] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.16910 (0.16910) | TIME: 0:00:01 |
[VALID F2] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.11696 (0.12517) | TIME: 0:00:34 |
[VALID F2] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.12035 (0.12443) | TIME: 0:01:06 |
[VALID F2] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.10582 (0.12484) | TIME: 0:01:38 |
[VALID F2] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.22836 (0.12465) | TIME: 0:01:40 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.26791 |      0.12465 |  0.49928 | 0.509 | 0.463 | 0.452 | 0.493 | 0.604 | 0.474 | 0:16:32 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4992814064025879

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.12815 (0.12815) | LR: 0.00002711 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.08859 (0.12697) | LR: 0.00002625 | TIME: 0:01:41 |
[TRAIN F2] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.15777 (0.11727) | LR: 0.00002529 | TIME: 0:03:18 |
[TRAIN F2] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.08654 (0.11799) | LR: 0.00002425 | TIME: 0:04:56 |
[TRAIN F2] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.09872 (0.11904) | LR: 0.00002313 | TIME: 0:06:32 |
[TRAIN F2] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.10311 (0.12005) | LR: 0.00002195 | TIME: 0:08:10 |
[TRAIN F2] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.10508 (0.11927) | LR: 0.00002070 | TIME: 0:09:49 |
[TRAIN F2] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.08685 (0.11736) | LR: 0.00001941 | TIME: 0:11:27 |
[TRAIN F2] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.11034 (0.11753) | LR: 0.00001808 | TIME: 0:13:05 |
[TRAIN F2] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.10760 (0.11745) | LR: 0.00001673 | TIME: 0:14:42 |
[TRAIN F2] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.14418 (0.11766) | LR: 0.00001656 | TIME: 0:14:54 |

VALID_LOOP
[VALID F2] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.14832 (0.14832) | TIME: 0:00:01 |
[VALID F2] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.07732 (0.11316) | TIME: 0:00:34 |
[VALID F2] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.11469 (0.11158) | TIME: 0:01:06 |
[VALID F2] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.11239 (0.11277) | TIME: 0:01:38 |
[VALID F2] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.11693 (0.11250) | TIME: 0:01:40 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11766 |       0.1125 |   0.4756 | 0.537 | 0.457 | 0.422 | 0.478 | 0.500 | 0.460 | 0:16:34 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4756045341491699

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.09115 (0.09115) | LR: 0.00001652 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.06780 (0.09592) | LR: 0.00001515 | TIME: 0:01:42 |
[TRAIN F2] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.06256 (0.09367) | LR: 0.00001378 | TIME: 0:03:19 |
[TRAIN F2] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.08848 (0.09509) | LR: 0.00001242 | TIME: 0:04:56 |
[TRAIN F2] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.08461 (0.09624) | LR: 0.00001108 | TIME: 0:06:34 |
[TRAIN F2] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.08256 (0.09534) | LR: 0.00000977 | TIME: 0:08:12 |
[TRAIN F2] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.06060 (0.09408) | LR: 0.00000851 | TIME: 0:09:50 |
[TRAIN F2] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.06829 (0.09470) | LR: 0.00000730 | TIME: 0:11:28 |
[TRAIN F2] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.08775 (0.09401) | LR: 0.00000616 | TIME: 0:13:05 |
[TRAIN F2] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.08221 (0.09395) | LR: 0.00000509 | TIME: 0:14:42 |
[TRAIN F2] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.06950 (0.09378) | LR: 0.00000496 | TIME: 0:14:54 |

VALID_LOOP
[VALID F2] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.15999 (0.15999) | TIME: 0:00:01 |
[VALID F2] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.09198 (0.10870) | TIME: 0:00:34 |
[VALID F2] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.09537 (0.10795) | TIME: 0:01:06 |
[VALID F2] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.10008 (0.10897) | TIME: 0:01:38 |
[VALID F2] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.16998 (0.10882) | TIME: 0:01:40 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.09378 |      0.10882 |   0.4677 | 0.485 | 0.454 | 0.447 | 0.475 | 0.489 | 0.456 | 0:16:34 |


[SAVED] EPOCH: 3 | MCRMSE: 0.46769845485687256

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.09694 (0.09694) | LR: 0.00000493 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.08138 (0.07177) | LR: 0.00000396 | TIME: 0:01:41 |
[TRAIN F2] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.07830 (0.07318) | LR: 0.00000308 | TIME: 0:03:19 |
[TRAIN F2] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.10310 (0.07517) | LR: 0.00000230 | TIME: 0:04:56 |
[TRAIN F2] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.03747 (0.07474) | LR: 0.00000162 | TIME: 0:06:33 |
[TRAIN F2] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.06942 (0.07484) | LR: 0.00000106 | TIME: 0:08:11 |
[TRAIN F2] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.05146 (0.07411) | LR: 0.00000061 | TIME: 0:09:49 |
[TRAIN F2] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.07040 (0.07407) | LR: 0.00000028 | TIME: 0:11:27 |
[TRAIN F2] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.05720 (0.07365) | LR: 0.00000008 | TIME: 0:13:04 |
[TRAIN F2] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.07072 (0.07334) | LR: 0.00000000 | TIME: 0:14:42 |
[TRAIN F2] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.06667 (0.07353) | LR: 0.00000000 | TIME: 0:14:54 |

VALID_LOOP
[VALID F2] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.14626 (0.14626) | TIME: 0:00:01 |
[VALID F2] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.08177 (0.10668) | TIME: 0:00:34 |
[VALID F2] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.08711 (0.10469) | TIME: 0:01:06 |
[VALID F2] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.10491 (0.10620) | TIME: 0:01:38 |
[VALID F2] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.12876 (0.10599) | TIME: 0:01:39 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.07353 |      0.10599 |  0.46153 | 0.483 | 0.451 | 0.426 | 0.470 | 0.483 | 0.457 | 0:16:34 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4615310728549957


----------------------------------- FOLD 2 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.46153     0.48322   0.45106        0.4257        0.46975    0.48262        0.45684

################################### END OF FOlD 2 ###################################


Date: 2022-11-19 20:02:07.459947+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: roberta-large
Model_config: RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 512

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.33011 (2.33011) | LR: 0.00000033 | TIME: 0:00:05 |
[TRAIN F3] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.37434 (1.20486) | LR: 0.00001352 | TIME: 0:01:42 |
[TRAIN F3] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.14377 (0.72077) | LR: 0.00002670 | TIME: 0:03:19 |
[TRAIN F3] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.15829 (0.53894) | LR: 0.00002996 | TIME: 0:04:57 |
[TRAIN F3] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.21081 (0.44699) | LR: 0.00002981 | TIME: 0:06:35 |
[TRAIN F3] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.10125 (0.39035) | LR: 0.00002953 | TIME: 0:08:14 |
[TRAIN F3] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.16744 (0.35000) | LR: 0.00002913 | TIME: 0:09:51 |
[TRAIN F3] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.10901 (0.31952) | LR: 0.00002860 | TIME: 0:11:29 |
[TRAIN F3] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.13785 (0.29796) | LR: 0.00002797 | TIME: 0:13:06 |
[TRAIN F3] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.18298 (0.28061) | LR: 0.00002723 | TIME: 0:14:44 |
[TRAIN F3] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.15929 (0.27859) | LR: 0.00002713 | TIME: 0:14:56 |

VALID_LOOP
[VALID F3] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.12347 (0.12347) | TIME: 0:00:01 |
[VALID F3] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.07225 (0.11052) | TIME: 0:00:34 |
[VALID F3] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.12167 (0.11235) | TIME: 0:01:06 |
[VALID F3] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.09587 (0.11367) | TIME: 0:01:39 |
[VALID F3] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.03377 (0.11336) | TIME: 0:01:40 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.27859 |      0.11336 |  0.47675 | 0.500 | 0.459 | 0.429 | 0.467 | 0.544 | 0.461 | 0:16:36 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4767548143863678

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.09109 (0.09109) | LR: 0.00002711 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.12719 (0.12140) | LR: 0.00002625 | TIME: 0:01:41 |
[TRAIN F3] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.11727 (0.12541) | LR: 0.00002529 | TIME: 0:03:18 |
[TRAIN F3] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.08362 (0.12440) | LR: 0.00002425 | TIME: 0:04:56 |
[TRAIN F3] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.13211 (0.12341) | LR: 0.00002313 | TIME: 0:06:33 |
[TRAIN F3] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.09442 (0.12051) | LR: 0.00002195 | TIME: 0:08:10 |
[TRAIN F3] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.08336 (0.12035) | LR: 0.00002070 | TIME: 0:09:48 |
[TRAIN F3] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.08779 (0.11997) | LR: 0.00001941 | TIME: 0:11:25 |
[TRAIN F3] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.08574 (0.11956) | LR: 0.00001808 | TIME: 0:13:03 |
[TRAIN F3] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.05476 (0.11929) | LR: 0.00001673 | TIME: 0:14:41 |
[TRAIN F3] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.09976 (0.11905) | LR: 0.00001656 | TIME: 0:14:53 |

VALID_LOOP
[VALID F3] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.10079 (0.10079) | TIME: 0:00:01 |
[VALID F3] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.08186 (0.10456) | TIME: 0:00:34 |
[VALID F3] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.16797 (0.10867) | TIME: 0:01:06 |
[VALID F3] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.08507 (0.10984) | TIME: 0:01:38 |
[VALID F3] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.05470 (0.10969) | TIME: 0:01:39 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11905 |      0.10969 |  0.46946 | 0.538 | 0.465 | 0.428 | 0.456 | 0.481 | 0.448 | 0:16:33 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4694649279117584

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.07089 (0.07089) | LR: 0.00001652 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.08259 (0.09989) | LR: 0.00001515 | TIME: 0:01:42 |
[TRAIN F3] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.13201 (0.09974) | LR: 0.00001378 | TIME: 0:03:20 |
[TRAIN F3] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.08409 (0.09802) | LR: 0.00001242 | TIME: 0:04:57 |
[TRAIN F3] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.07473 (0.09737) | LR: 0.00001108 | TIME: 0:06:35 |
[TRAIN F3] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.08273 (0.09554) | LR: 0.00000977 | TIME: 0:08:12 |
[TRAIN F3] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.11506 (0.09537) | LR: 0.00000851 | TIME: 0:09:49 |
[TRAIN F3] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.09483 (0.09545) | LR: 0.00000730 | TIME: 0:11:29 |
[TRAIN F3] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.09778 (0.09529) | LR: 0.00000616 | TIME: 0:13:08 |
[TRAIN F3] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.08172 (0.09365) | LR: 0.00000509 | TIME: 0:14:46 |
[TRAIN F3] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.09681 (0.09360) | LR: 0.00000496 | TIME: 0:14:58 |

VALID_LOOP
[VALID F3] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.11518 (0.11518) | TIME: 0:00:01 |
[VALID F3] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.08058 (0.10550) | TIME: 0:00:34 |
[VALID F3] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.16090 (0.10810) | TIME: 0:01:06 |
[VALID F3] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.09349 (0.10953) | TIME: 0:01:39 |
[VALID F3] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.06563 (0.10946) | TIME: 0:01:40 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |       0.0936 |      0.10946 |  0.46891 | 0.493 | 0.463 | 0.430 | 0.455 | 0.512 | 0.462 | 0:16:38 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4689067304134369

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.04383 (0.04383) | LR: 0.00000493 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.06495 (0.07898) | LR: 0.00000396 | TIME: 0:01:41 |
[TRAIN F3] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.05187 (0.07948) | LR: 0.00000308 | TIME: 0:03:18 |
[TRAIN F3] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.08018 (0.07891) | LR: 0.00000230 | TIME: 0:04:57 |
[TRAIN F3] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.07733 (0.07839) | LR: 0.00000162 | TIME: 0:06:34 |
[TRAIN F3] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.04070 (0.07718) | LR: 0.00000106 | TIME: 0:08:12 |
[TRAIN F3] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.05981 (0.07695) | LR: 0.00000061 | TIME: 0:09:50 |
[TRAIN F3] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.08471 (0.07676) | LR: 0.00000028 | TIME: 0:11:29 |
[TRAIN F3] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.08255 (0.07611) | LR: 0.00000008 | TIME: 0:13:06 |
[TRAIN F3] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.10247 (0.07620) | LR: 0.00000000 | TIME: 0:14:44 |
[TRAIN F3] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.07676 (0.07615) | LR: 0.00000000 | TIME: 0:14:56 |

VALID_LOOP
[VALID F3] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.11486 (0.11486) | TIME: 0:00:01 |
[VALID F3] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.07287 (0.10128) | TIME: 0:00:34 |
[VALID F3] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.13482 (0.10284) | TIME: 0:01:06 |
[VALID F3] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.09507 (0.10394) | TIME: 0:01:39 |
[VALID F3] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.05846 (0.10380) | TIME: 0:01:40 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.07615 |       0.1038 |   0.4567 | 0.490 | 0.456 | 0.422 | 0.449 | 0.475 | 0.447 | 0:16:36 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4566972553730011


----------------------------------- FOLD 3 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
  0.4567     0.49049   0.45631       0.42212        0.44948    0.47486        0.44692

################################### END OF FOlD 3 ###################################


