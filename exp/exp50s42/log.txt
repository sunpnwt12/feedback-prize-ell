Date: 2022-11-15 21:21:52.490510+07:00 (GMT+7)
Mode: EXPERIMENTING_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 5864}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 0)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
Date: 2022-11-15 21:24:37.838656+07:00 (GMT+7)
Mode: EXPERIMENTING_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 5864}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 1/4 | STEP: 0000/1466 | LOSS: 2.93493 (2.93493) | LR: 0.00000005 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0040/1466 | LOSS: 2.54623 (2.46328) | LR: 0.00000224 | TIME: 0:01:35 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0080/1466 | LOSS: 0.23020 (1.78178) | LR: 0.00000443 | TIME: 0:03:01 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0120/1466 | LOSS: 0.09212 (1.28730) | LR: 0.00000661 | TIME: 0:04:32 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0160/1466 | LOSS: 0.08535 (1.01317) | LR: 0.00000880 | TIME: 0:06:09 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0200/1466 | LOSS: 0.19160 (0.85831) | LR: 0.00001098 | TIME: 0:07:39 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0240/1466 | LOSS: 0.34262 (0.74127) | LR: 0.00001317 | TIME: 0:09:08 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0280/1466 | LOSS: 0.13099 (0.65823) | LR: 0.00001536 | TIME: 0:10:36 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0320/1466 | LOSS: 0.15835 (0.59427) | LR: 0.00001754 | TIME: 0:12:05 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0360/1466 | LOSS: 0.21558 (0.54423) | LR: 0.00001973 | TIME: 0:13:41 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0400/1466 | LOSS: 0.15465 (0.50526) | LR: 0.00002000 | TIME: 0:15:19 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0440/1466 | LOSS: 0.12489 (0.47155) | LR: 0.00001999 | TIME: 0:16:47 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0480/1466 | LOSS: 0.30865 (0.44481) | LR: 0.00001998 | TIME: 0:18:05 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0520/1466 | LOSS: 0.25652 (0.42021) | LR: 0.00001996 | TIME: 0:19:34 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0560/1466 | LOSS: 0.14287 (0.40015) | LR: 0.00001994 | TIME: 0:20:56 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0600/1466 | LOSS: 0.13946 (0.38557) | LR: 0.00001991 | TIME: 0:22:25 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0640/1466 | LOSS: 0.05723 (0.37071) | LR: 0.00001988 | TIME: 0:24:05 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0680/1466 | LOSS: 0.06727 (0.35724) | LR: 0.00001984 | TIME: 0:25:21 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0720/1466 | LOSS: 0.07855 (0.34417) | LR: 0.00001979 | TIME: 0:26:51 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0760/1466 | LOSS: 0.08574 (0.33321) | LR: 0.00001975 | TIME: 0:28:23 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0800/1466 | LOSS: 0.08922 (0.32402) | LR: 0.00001969 | TIME: 0:30:02 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0840/1466 | LOSS: 0.16676 (0.31547) | LR: 0.00001963 | TIME: 0:31:31 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0880/1466 | LOSS: 0.15889 (0.30803) | LR: 0.00001957 | TIME: 0:33:05 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0920/1466 | LOSS: 0.04883 (0.30112) | LR: 0.00001950 | TIME: 0:34:42 |
[TRAIN F0] EPOCH: 1/4 | STEP: 0960/1466 | LOSS: 0.11367 (0.29468) | LR: 0.00001943 | TIME: 0:36:13 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1000/1466 | LOSS: 0.10626 (0.28789) | LR: 0.00001935 | TIME: 0:37:47 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1040/1466 | LOSS: 0.10210 (0.28165) | LR: 0.00001927 | TIME: 0:39:21 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1080/1466 | LOSS: 0.15997 (0.27616) | LR: 0.00001918 | TIME: 0:40:53 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1120/1466 | LOSS: 0.11063 (0.27097) | LR: 0.00001908 | TIME: 0:42:18 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1160/1466 | LOSS: 0.16007 (0.26645) | LR: 0.00001899 | TIME: 0:43:56 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1200/1466 | LOSS: 0.26175 (0.26222) | LR: 0.00001888 | TIME: 0:45:30 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1240/1466 | LOSS: 0.11613 (0.25810) | LR: 0.00001878 | TIME: 0:46:56 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1280/1466 | LOSS: 0.18345 (0.25445) | LR: 0.00001866 | TIME: 0:48:28 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1320/1466 | LOSS: 0.15697 (0.25056) | LR: 0.00001855 | TIME: 0:49:59 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1360/1466 | LOSS: 0.28459 (0.24757) | LR: 0.00001843 | TIME: 0:51:27 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1400/1466 | LOSS: 0.06203 (0.24420) | LR: 0.00001830 | TIME: 0:52:48 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1440/1466 | LOSS: 0.08093 (0.24089) | LR: 0.00001817 | TIME: 0:54:16 |
[TRAIN F0] EPOCH: 1/4 | STEP: 1465/1466 | LOSS: 0.28588 (0.23910) | LR: 0.00001809 | TIME: 0:55:10 |

VALID_LOOP
[VALID F0] EPOCH: 1/4 | STEP: 000/489 | LOSS: 0.06648 (0.06648) | TIME: 0:00:01 |
[VALID F0] EPOCH: 1/4 | STEP: 040/489 | LOSS: 0.08931 (0.12756) | TIME: 0:00:39 |
[VALID F0] EPOCH: 1/4 | STEP: 080/489 | LOSS: 0.12311 (0.11714) | TIME: 0:01:16 |
[VALID F0] EPOCH: 1/4 | STEP: 120/489 | LOSS: 0.08432 (0.11561) | TIME: 0:01:53 |
[VALID F0] EPOCH: 1/4 | STEP: 160/489 | LOSS: 0.13887 (0.11643) | TIME: 0:02:30 |
[VALID F0] EPOCH: 1/4 | STEP: 200/489 | LOSS: 0.09987 (0.11480) | TIME: 0:03:07 |
[VALID F0] EPOCH: 1/4 | STEP: 240/489 | LOSS: 0.08071 (0.11416) | TIME: 0:03:45 |
[VALID F0] EPOCH: 1/4 | STEP: 280/489 | LOSS: 0.11034 (0.11630) | TIME: 0:04:22 |
[VALID F0] EPOCH: 1/4 | STEP: 320/489 | LOSS: 0.12353 (0.11592) | TIME: 0:04:59 |
[VALID F0] EPOCH: 1/4 | STEP: 360/489 | LOSS: 0.06972 (0.11574) | TIME: 0:05:36 |
[VALID F0] EPOCH: 1/4 | STEP: 400/489 | LOSS: 0.15688 (0.11618) | TIME: 0:06:13 |
[VALID F0] EPOCH: 1/4 | STEP: 440/489 | LOSS: 0.08877 (0.11764) | TIME: 0:06:51 |
[VALID F0] EPOCH: 1/4 | STEP: 480/489 | LOSS: 0.12508 (0.11713) | TIME: 0:07:28 |
[VALID F0] EPOCH: 1/4 | STEP: 488/489 | LOSS: 0.14327 (0.11705) | TIME: 0:07:35 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |       0.2391 |      0.11705 |  0.48447 | 0.497 | 0.459 | 0.477 | 0.542 | 0.484 | 0.447 | 1:02:46 |


[SAVED] EPOCH: 1 | MCRMSE: 0.48446908593177795

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 2/4 | STEP: 0000/1466 | LOSS: 0.03814 (0.03814) | LR: 0.00001809 | TIME: 0:00:02 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0040/1466 | LOSS: 0.03286 (0.09699) | LR: 0.00001795 | TIME: 0:01:30 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0080/1466 | LOSS: 0.13839 (0.10844) | LR: 0.00001781 | TIME: 0:03:00 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0120/1466 | LOSS: 0.13537 (0.11381) | LR: 0.00001766 | TIME: 0:04:33 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0160/1466 | LOSS: 0.21397 (0.11032) | LR: 0.00001751 | TIME: 0:06:11 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0200/1466 | LOSS: 0.05925 (0.11460) | LR: 0.00001736 | TIME: 0:07:40 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0240/1466 | LOSS: 0.12241 (0.11437) | LR: 0.00001721 | TIME: 0:08:57 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0280/1466 | LOSS: 0.16187 (0.11612) | LR: 0.00001704 | TIME: 0:10:24 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0320/1466 | LOSS: 0.10869 (0.11847) | LR: 0.00001688 | TIME: 0:12:06 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0360/1466 | LOSS: 0.05954 (0.11812) | LR: 0.00001671 | TIME: 0:13:34 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0400/1466 | LOSS: 0.03171 (0.11921) | LR: 0.00001654 | TIME: 0:15:14 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0440/1466 | LOSS: 0.10012 (0.11947) | LR: 0.00001637 | TIME: 0:16:45 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0480/1466 | LOSS: 0.18149 (0.11922) | LR: 0.00001619 | TIME: 0:18:05 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0520/1466 | LOSS: 0.15557 (0.11958) | LR: 0.00001601 | TIME: 0:19:35 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0560/1466 | LOSS: 0.14698 (0.11857) | LR: 0.00001582 | TIME: 0:21:05 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0600/1466 | LOSS: 0.06625 (0.11803) | LR: 0.00001564 | TIME: 0:22:30 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0640/1466 | LOSS: 0.27866 (0.11795) | LR: 0.00001545 | TIME: 0:23:58 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0680/1466 | LOSS: 0.06190 (0.11726) | LR: 0.00001525 | TIME: 0:25:34 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0720/1466 | LOSS: 0.09491 (0.11634) | LR: 0.00001506 | TIME: 0:26:55 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0760/1466 | LOSS: 0.16145 (0.11585) | LR: 0.00001486 | TIME: 0:28:14 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0800/1466 | LOSS: 0.26181 (0.11549) | LR: 0.00001466 | TIME: 0:29:46 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0840/1466 | LOSS: 0.03977 (0.11565) | LR: 0.00001445 | TIME: 0:31:20 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0880/1466 | LOSS: 0.21042 (0.11563) | LR: 0.00001425 | TIME: 0:32:46 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0920/1466 | LOSS: 0.10475 (0.11524) | LR: 0.00001404 | TIME: 0:34:21 |
[TRAIN F0] EPOCH: 2/4 | STEP: 0960/1466 | LOSS: 0.12151 (0.11521) | LR: 0.00001383 | TIME: 0:35:49 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1000/1466 | LOSS: 0.07104 (0.11528) | LR: 0.00001362 | TIME: 0:37:22 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1040/1466 | LOSS: 0.15333 (0.11516) | LR: 0.00001340 | TIME: 0:38:56 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1080/1466 | LOSS: 0.12407 (0.11495) | LR: 0.00001319 | TIME: 0:40:23 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1120/1466 | LOSS: 0.04724 (0.11544) | LR: 0.00001297 | TIME: 0:41:57 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1160/1466 | LOSS: 0.06344 (0.11488) | LR: 0.00001275 | TIME: 0:43:21 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1200/1466 | LOSS: 0.06599 (0.11457) | LR: 0.00001253 | TIME: 0:44:53 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1240/1466 | LOSS: 0.10386 (0.11417) | LR: 0.00001231 | TIME: 0:46:23 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1280/1466 | LOSS: 0.11521 (0.11387) | LR: 0.00001209 | TIME: 0:47:54 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1320/1466 | LOSS: 0.17025 (0.11387) | LR: 0.00001186 | TIME: 0:49:21 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1360/1466 | LOSS: 0.08141 (0.11382) | LR: 0.00001164 | TIME: 0:50:55 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1400/1466 | LOSS: 0.08487 (0.11323) | LR: 0.00001141 | TIME: 0:52:17 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1440/1466 | LOSS: 0.02688 (0.11301) | LR: 0.00001119 | TIME: 0:53:45 |
[TRAIN F0] EPOCH: 2/4 | STEP: 1465/1466 | LOSS: 0.10023 (0.11284) | LR: 0.00001104 | TIME: 0:54:49 |

VALID_LOOP
[VALID F0] EPOCH: 2/4 | STEP: 000/489 | LOSS: 0.09567 (0.09567) | TIME: 0:00:01 |
[VALID F0] EPOCH: 2/4 | STEP: 040/489 | LOSS: 0.06160 (0.11926) | TIME: 0:00:38 |
[VALID F0] EPOCH: 2/4 | STEP: 080/489 | LOSS: 0.14110 (0.10936) | TIME: 0:01:15 |
[VALID F0] EPOCH: 2/4 | STEP: 120/489 | LOSS: 0.04846 (0.10681) | TIME: 0:01:51 |
[VALID F0] EPOCH: 2/4 | STEP: 160/489 | LOSS: 0.16303 (0.10734) | TIME: 0:02:28 |
[VALID F0] EPOCH: 2/4 | STEP: 200/489 | LOSS: 0.08398 (0.10582) | TIME: 0:03:04 |
[VALID F0] EPOCH: 2/4 | STEP: 240/489 | LOSS: 0.08812 (0.10572) | TIME: 0:03:41 |
[VALID F0] EPOCH: 2/4 | STEP: 280/489 | LOSS: 0.06870 (0.10842) | TIME: 0:04:18 |
[VALID F0] EPOCH: 2/4 | STEP: 320/489 | LOSS: 0.09227 (0.10813) | TIME: 0:04:54 |
[VALID F0] EPOCH: 2/4 | STEP: 360/489 | LOSS: 0.10343 (0.10817) | TIME: 0:05:31 |
[VALID F0] EPOCH: 2/4 | STEP: 400/489 | LOSS: 0.12592 (0.10900) | TIME: 0:06:08 |
[VALID F0] EPOCH: 2/4 | STEP: 440/489 | LOSS: 0.09305 (0.11074) | TIME: 0:06:44 |
[VALID F0] EPOCH: 2/4 | STEP: 480/489 | LOSS: 0.12923 (0.11035) | TIME: 0:07:21 |
[VALID F0] EPOCH: 2/4 | STEP: 488/489 | LOSS: 0.16077 (0.11030) | TIME: 0:07:28 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11284 |       0.1103 |  0.46928 | 0.545 | 0.440 | 0.421 | 0.451 | 0.488 | 0.470 | 1:02:17 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4692787230014801

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 3/4 | STEP: 0000/1466 | LOSS: 0.12528 (0.12528) | LR: 0.00001104 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0040/1466 | LOSS: 0.07327 (0.10011) | LR: 0.00001081 | TIME: 0:01:37 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0080/1466 | LOSS: 0.09315 (0.09641) | LR: 0.00001058 | TIME: 0:02:57 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0120/1466 | LOSS: 0.11153 (0.09615) | LR: 0.00001035 | TIME: 0:04:34 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0160/1466 | LOSS: 0.06751 (0.09389) | LR: 0.00001013 | TIME: 0:06:05 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0200/1466 | LOSS: 0.07457 (0.09130) | LR: 0.00000990 | TIME: 0:07:22 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0240/1466 | LOSS: 0.06627 (0.09003) | LR: 0.00000967 | TIME: 0:08:48 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0280/1466 | LOSS: 0.05485 (0.09179) | LR: 0.00000944 | TIME: 0:10:30 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0320/1466 | LOSS: 0.04667 (0.09321) | LR: 0.00000921 | TIME: 0:11:56 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0360/1466 | LOSS: 0.12098 (0.09314) | LR: 0.00000898 | TIME: 0:13:21 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0400/1466 | LOSS: 0.07203 (0.09283) | LR: 0.00000876 | TIME: 0:15:01 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0440/1466 | LOSS: 0.05980 (0.09232) | LR: 0.00000853 | TIME: 0:16:38 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0480/1466 | LOSS: 0.10109 (0.09234) | LR: 0.00000831 | TIME: 0:18:06 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0520/1466 | LOSS: 0.05056 (0.09249) | LR: 0.00000808 | TIME: 0:19:45 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0560/1466 | LOSS: 0.06115 (0.09318) | LR: 0.00000786 | TIME: 0:21:20 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0600/1466 | LOSS: 0.08702 (0.09289) | LR: 0.00000763 | TIME: 0:22:51 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0640/1466 | LOSS: 0.18462 (0.09287) | LR: 0.00000741 | TIME: 0:24:32 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0680/1466 | LOSS: 0.07077 (0.09261) | LR: 0.00000719 | TIME: 0:25:59 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0720/1466 | LOSS: 0.12535 (0.09235) | LR: 0.00000697 | TIME: 0:27:30 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0760/1466 | LOSS: 0.06522 (0.09131) | LR: 0.00000676 | TIME: 0:29:07 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0800/1466 | LOSS: 0.08700 (0.09213) | LR: 0.00000654 | TIME: 0:30:28 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0840/1466 | LOSS: 0.10949 (0.09184) | LR: 0.00000633 | TIME: 0:31:57 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0880/1466 | LOSS: 0.13500 (0.09180) | LR: 0.00000612 | TIME: 0:33:25 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0920/1466 | LOSS: 0.03779 (0.09175) | LR: 0.00000591 | TIME: 0:35:10 |
[TRAIN F0] EPOCH: 3/4 | STEP: 0960/1466 | LOSS: 0.09268 (0.09151) | LR: 0.00000570 | TIME: 0:36:44 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1000/1466 | LOSS: 0.08035 (0.09111) | LR: 0.00000549 | TIME: 0:38:10 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1040/1466 | LOSS: 0.06390 (0.09138) | LR: 0.00000529 | TIME: 0:39:31 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1080/1466 | LOSS: 0.14314 (0.09083) | LR: 0.00000509 | TIME: 0:40:57 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1120/1466 | LOSS: 0.17888 (0.09061) | LR: 0.00000489 | TIME: 0:42:23 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1160/1466 | LOSS: 0.05948 (0.09000) | LR: 0.00000470 | TIME: 0:43:46 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1200/1466 | LOSS: 0.03349 (0.08994) | LR: 0.00000451 | TIME: 0:45:01 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1240/1466 | LOSS: 0.12284 (0.08988) | LR: 0.00000432 | TIME: 0:46:15 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1280/1466 | LOSS: 0.02621 (0.08952) | LR: 0.00000413 | TIME: 0:47:42 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1320/1466 | LOSS: 0.10493 (0.08933) | LR: 0.00000395 | TIME: 0:49:23 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1360/1466 | LOSS: 0.07455 (0.08949) | LR: 0.00000377 | TIME: 0:50:52 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1400/1466 | LOSS: 0.01451 (0.08950) | LR: 0.00000359 | TIME: 0:52:19 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1440/1466 | LOSS: 0.07583 (0.08987) | LR: 0.00000341 | TIME: 0:53:47 |
[TRAIN F0] EPOCH: 3/4 | STEP: 1465/1466 | LOSS: 0.08773 (0.08971) | LR: 0.00000331 | TIME: 0:54:40 |

VALID_LOOP
[VALID F0] EPOCH: 3/4 | STEP: 000/489 | LOSS: 0.07311 (0.07311) | TIME: 0:00:01 |
[VALID F0] EPOCH: 3/4 | STEP: 040/489 | LOSS: 0.06177 (0.10333) | TIME: 0:00:38 |
[VALID F0] EPOCH: 3/4 | STEP: 080/489 | LOSS: 0.13272 (0.09619) | TIME: 0:01:15 |
[VALID F0] EPOCH: 3/4 | STEP: 120/489 | LOSS: 0.04894 (0.09611) | TIME: 0:01:51 |
[VALID F0] EPOCH: 3/4 | STEP: 160/489 | LOSS: 0.13851 (0.09855) | TIME: 0:02:28 |
[VALID F0] EPOCH: 3/4 | STEP: 200/489 | LOSS: 0.09969 (0.09655) | TIME: 0:03:05 |
[VALID F0] EPOCH: 3/4 | STEP: 240/489 | LOSS: 0.08546 (0.09648) | TIME: 0:03:41 |
[VALID F0] EPOCH: 3/4 | STEP: 280/489 | LOSS: 0.07949 (0.09909) | TIME: 0:04:18 |
[VALID F0] EPOCH: 3/4 | STEP: 320/489 | LOSS: 0.08146 (0.09800) | TIME: 0:04:55 |
[VALID F0] EPOCH: 3/4 | STEP: 360/489 | LOSS: 0.06935 (0.09791) | TIME: 0:05:31 |
[VALID F0] EPOCH: 3/4 | STEP: 400/489 | LOSS: 0.13165 (0.09884) | TIME: 0:06:08 |
[VALID F0] EPOCH: 3/4 | STEP: 440/489 | LOSS: 0.09512 (0.09998) | TIME: 0:06:45 |
[VALID F0] EPOCH: 3/4 | STEP: 480/489 | LOSS: 0.09453 (0.09970) | TIME: 0:07:21 |
[VALID F0] EPOCH: 3/4 | STEP: 488/489 | LOSS: 0.12427 (0.09958) | TIME: 0:07:29 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.08971 |      0.09958 |  0.44656 | 0.477 | 0.438 | 0.409 | 0.452 | 0.465 | 0.438 | 1:02:09 |


[SAVED] EPOCH: 3 | MCRMSE: 0.44656094908714294

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 4/4 | STEP: 0000/1466 | LOSS: 0.03415 (0.03415) | LR: 0.00000330 | TIME: 0:00:02 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0040/1466 | LOSS: 0.09988 (0.05967) | LR: 0.00000314 | TIME: 0:01:23 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0080/1466 | LOSS: 0.05484 (0.06986) | LR: 0.00000297 | TIME: 0:03:00 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0120/1466 | LOSS: 0.09199 (0.07193) | LR: 0.00000281 | TIME: 0:04:36 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0160/1466 | LOSS: 0.07457 (0.07352) | LR: 0.00000265 | TIME: 0:06:10 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0200/1466 | LOSS: 0.14222 (0.07358) | LR: 0.00000250 | TIME: 0:07:46 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0240/1466 | LOSS: 0.14344 (0.07409) | LR: 0.00000235 | TIME: 0:09:21 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0280/1466 | LOSS: 0.04013 (0.07483) | LR: 0.00000221 | TIME: 0:10:47 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0320/1466 | LOSS: 0.06989 (0.07344) | LR: 0.00000207 | TIME: 0:12:12 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0360/1466 | LOSS: 0.12456 (0.07338) | LR: 0.00000193 | TIME: 0:13:37 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0400/1466 | LOSS: 0.17865 (0.07320) | LR: 0.00000180 | TIME: 0:14:59 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0440/1466 | LOSS: 0.14407 (0.07354) | LR: 0.00000167 | TIME: 0:16:28 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0480/1466 | LOSS: 0.02504 (0.07360) | LR: 0.00000154 | TIME: 0:17:52 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0520/1466 | LOSS: 0.05219 (0.07336) | LR: 0.00000142 | TIME: 0:19:12 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0560/1466 | LOSS: 0.11926 (0.07407) | LR: 0.00000131 | TIME: 0:20:45 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0600/1466 | LOSS: 0.03352 (0.07425) | LR: 0.00000120 | TIME: 0:22:12 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0640/1466 | LOSS: 0.11546 (0.07443) | LR: 0.00000109 | TIME: 0:23:46 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0680/1466 | LOSS: 0.12267 (0.07426) | LR: 0.00000099 | TIME: 0:25:14 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0720/1466 | LOSS: 0.04148 (0.07382) | LR: 0.00000089 | TIME: 0:26:44 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0760/1466 | LOSS: 0.08304 (0.07339) | LR: 0.00000080 | TIME: 0:28:07 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0800/1466 | LOSS: 0.05659 (0.07362) | LR: 0.00000071 | TIME: 0:29:36 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0840/1466 | LOSS: 0.13285 (0.07386) | LR: 0.00000063 | TIME: 0:31:11 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0880/1466 | LOSS: 0.16366 (0.07440) | LR: 0.00000055 | TIME: 0:32:53 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0920/1466 | LOSS: 0.05772 (0.07407) | LR: 0.00000048 | TIME: 0:34:24 |
[TRAIN F0] EPOCH: 4/4 | STEP: 0960/1466 | LOSS: 0.15353 (0.07421) | LR: 0.00000041 | TIME: 0:35:55 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1000/1466 | LOSS: 0.05738 (0.07411) | LR: 0.00000035 | TIME: 0:37:19 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1040/1466 | LOSS: 0.09150 (0.07396) | LR: 0.00000029 | TIME: 0:38:51 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1080/1466 | LOSS: 0.12070 (0.07413) | LR: 0.00000024 | TIME: 0:40:15 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1120/1466 | LOSS: 0.05936 (0.07388) | LR: 0.00000019 | TIME: 0:41:48 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1160/1466 | LOSS: 0.08473 (0.07377) | LR: 0.00000015 | TIME: 0:43:11 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1200/1466 | LOSS: 0.03733 (0.07382) | LR: 0.00000011 | TIME: 0:44:36 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1240/1466 | LOSS: 0.06806 (0.07380) | LR: 0.00000008 | TIME: 0:46:08 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1280/1466 | LOSS: 0.05288 (0.07388) | LR: 0.00000006 | TIME: 0:47:54 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1320/1466 | LOSS: 0.01828 (0.07384) | LR: 0.00000003 | TIME: 0:49:20 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1360/1466 | LOSS: 0.09413 (0.07365) | LR: 0.00000002 | TIME: 0:50:44 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1400/1466 | LOSS: 0.05279 (0.07372) | LR: 0.00000001 | TIME: 0:52:22 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1440/1466 | LOSS: 0.06811 (0.07344) | LR: 0.00000000 | TIME: 0:53:57 |
[TRAIN F0] EPOCH: 4/4 | STEP: 1465/1466 | LOSS: 0.04991 (0.07353) | LR: 0.00000000 | TIME: 0:55:03 |

VALID_LOOP
[VALID F0] EPOCH: 4/4 | STEP: 000/489 | LOSS: 0.06996 (0.06996) | TIME: 0:00:01 |
[VALID F0] EPOCH: 4/4 | STEP: 040/489 | LOSS: 0.06616 (0.10278) | TIME: 0:00:38 |
[VALID F0] EPOCH: 4/4 | STEP: 080/489 | LOSS: 0.14067 (0.09558) | TIME: 0:01:14 |
[VALID F0] EPOCH: 4/4 | STEP: 120/489 | LOSS: 0.04984 (0.09592) | TIME: 0:01:51 |
[VALID F0] EPOCH: 4/4 | STEP: 160/489 | LOSS: 0.14428 (0.09836) | TIME: 0:02:28 |
[VALID F0] EPOCH: 4/4 | STEP: 200/489 | LOSS: 0.09624 (0.09629) | TIME: 0:03:04 |
[VALID F0] EPOCH: 4/4 | STEP: 240/489 | LOSS: 0.08651 (0.09657) | TIME: 0:03:41 |
[VALID F0] EPOCH: 4/4 | STEP: 280/489 | LOSS: 0.08910 (0.09923) | TIME: 0:04:17 |
[VALID F0] EPOCH: 4/4 | STEP: 320/489 | LOSS: 0.07128 (0.09807) | TIME: 0:04:54 |
[VALID F0] EPOCH: 4/4 | STEP: 360/489 | LOSS: 0.07504 (0.09812) | TIME: 0:05:30 |
[VALID F0] EPOCH: 4/4 | STEP: 400/489 | LOSS: 0.12581 (0.09905) | TIME: 0:06:07 |
[VALID F0] EPOCH: 4/4 | STEP: 440/489 | LOSS: 0.10017 (0.10021) | TIME: 0:06:44 |
[VALID F0] EPOCH: 4/4 | STEP: 480/489 | LOSS: 0.09384 (0.09974) | TIME: 0:07:20 |
[VALID F0] EPOCH: 4/4 | STEP: 488/489 | LOSS: 0.12925 (0.09963) | TIME: 0:07:28 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.07353 |      0.09963 |  0.44672 | 0.476 | 0.439 | 0.410 | 0.453 | 0.466 | 0.437 | 1:02:31 |


----------------------------------- FOLD 0 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.44656     0.47749   0.43771       0.40941        0.45185    0.46504        0.43786

################################### END OF FOlD 0 ###################################


Date: 2022-11-16 22:44:15.494310+07:00 (GMT+7)
Mode: EXPERIMENTING_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 5868}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 1/4 | STEP: 0000/1467 | LOSS: 2.40750 (2.40750) | LR: 0.00000005 | TIME: 0:00:04 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0040/1467 | LOSS: 1.56377 (2.35163) | LR: 0.00000224 | TIME: 0:01:18 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0080/1467 | LOSS: 0.49520 (1.79823) | LR: 0.00000443 | TIME: 0:02:55 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0120/1467 | LOSS: 0.20167 (1.28653) | LR: 0.00000661 | TIME: 0:04:20 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0160/1467 | LOSS: 0.17525 (1.01472) | LR: 0.00000880 | TIME: 0:05:55 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0200/1467 | LOSS: 0.08575 (0.84638) | LR: 0.00001098 | TIME: 0:07:24 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0240/1467 | LOSS: 0.12000 (0.73390) | LR: 0.00001317 | TIME: 0:08:53 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0280/1467 | LOSS: 0.12068 (0.65232) | LR: 0.00001536 | TIME: 0:10:17 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0320/1467 | LOSS: 0.10160 (0.59158) | LR: 0.00001754 | TIME: 0:11:52 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0360/1467 | LOSS: 0.16117 (0.54221) | LR: 0.00001973 | TIME: 0:13:15 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0400/1467 | LOSS: 0.05533 (0.50004) | LR: 0.00002000 | TIME: 0:14:53 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0440/1467 | LOSS: 0.10452 (0.46712) | LR: 0.00001999 | TIME: 0:16:27 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0480/1467 | LOSS: 0.06097 (0.43965) | LR: 0.00001998 | TIME: 0:18:01 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0520/1467 | LOSS: 0.12419 (0.41521) | LR: 0.00001996 | TIME: 0:19:28 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0560/1467 | LOSS: 0.15299 (0.39563) | LR: 0.00001994 | TIME: 0:21:07 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0600/1467 | LOSS: 0.27593 (0.37842) | LR: 0.00001991 | TIME: 0:22:36 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0640/1467 | LOSS: 0.14514 (0.36328) | LR: 0.00001988 | TIME: 0:24:09 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0680/1467 | LOSS: 0.08074 (0.35054) | LR: 0.00001984 | TIME: 0:25:34 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0720/1467 | LOSS: 0.08664 (0.33982) | LR: 0.00001980 | TIME: 0:27:06 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0760/1467 | LOSS: 0.12934 (0.32904) | LR: 0.00001975 | TIME: 0:28:44 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0800/1467 | LOSS: 0.08000 (0.32054) | LR: 0.00001969 | TIME: 0:30:15 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0840/1467 | LOSS: 0.19471 (0.31274) | LR: 0.00001963 | TIME: 0:31:35 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0880/1467 | LOSS: 0.05868 (0.30463) | LR: 0.00001957 | TIME: 0:33:07 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0920/1467 | LOSS: 0.17177 (0.29809) | LR: 0.00001950 | TIME: 0:34:27 |
[TRAIN F1] EPOCH: 1/4 | STEP: 0960/1467 | LOSS: 0.07843 (0.29120) | LR: 0.00001943 | TIME: 0:36:02 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1000/1467 | LOSS: 0.11110 (0.28485) | LR: 0.00001935 | TIME: 0:37:19 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1040/1467 | LOSS: 0.15789 (0.27880) | LR: 0.00001927 | TIME: 0:38:46 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1080/1467 | LOSS: 0.12834 (0.27353) | LR: 0.00001918 | TIME: 0:40:16 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1120/1467 | LOSS: 0.08213 (0.26816) | LR: 0.00001909 | TIME: 0:41:44 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1160/1467 | LOSS: 0.11367 (0.26298) | LR: 0.00001899 | TIME: 0:43:16 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1200/1467 | LOSS: 0.09038 (0.25831) | LR: 0.00001888 | TIME: 0:44:44 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1240/1467 | LOSS: 0.20247 (0.25448) | LR: 0.00001878 | TIME: 0:46:02 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1280/1467 | LOSS: 0.07228 (0.25123) | LR: 0.00001867 | TIME: 0:47:35 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1320/1467 | LOSS: 0.17963 (0.24711) | LR: 0.00001855 | TIME: 0:49:05 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1360/1467 | LOSS: 0.31631 (0.24470) | LR: 0.00001843 | TIME: 0:50:30 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1400/1467 | LOSS: 0.17906 (0.24159) | LR: 0.00001830 | TIME: 0:52:02 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1440/1467 | LOSS: 0.10414 (0.23861) | LR: 0.00001817 | TIME: 0:53:17 |
[TRAIN F1] EPOCH: 1/4 | STEP: 1466/1467 | LOSS: 0.10358 (0.23642) | LR: 0.00001809 | TIME: 0:54:16 |

VALID_LOOP
[VALID F1] EPOCH: 1/4 | STEP: 000/489 | LOSS: 0.04462 (0.04462) | TIME: 0:00:02 |
[VALID F1] EPOCH: 1/4 | STEP: 040/489 | LOSS: 0.10033 (0.12286) | TIME: 0:00:39 |
[VALID F1] EPOCH: 1/4 | STEP: 080/489 | LOSS: 0.08308 (0.12299) | TIME: 0:01:16 |
[VALID F1] EPOCH: 1/4 | STEP: 120/489 | LOSS: 0.10584 (0.12499) | TIME: 0:01:53 |
[VALID F1] EPOCH: 1/4 | STEP: 160/489 | LOSS: 0.13837 (0.12716) | TIME: 0:02:30 |
[VALID F1] EPOCH: 1/4 | STEP: 200/489 | LOSS: 0.07948 (0.12314) | TIME: 0:03:07 |
[VALID F1] EPOCH: 1/4 | STEP: 240/489 | LOSS: 0.13858 (0.12350) | TIME: 0:03:44 |
[VALID F1] EPOCH: 1/4 | STEP: 280/489 | LOSS: 0.11093 (0.12270) | TIME: 0:04:21 |
[VALID F1] EPOCH: 1/4 | STEP: 320/489 | LOSS: 0.08017 (0.12364) | TIME: 0:04:58 |
[VALID F1] EPOCH: 1/4 | STEP: 360/489 | LOSS: 0.11943 (0.12275) | TIME: 0:05:35 |
[VALID F1] EPOCH: 1/4 | STEP: 400/489 | LOSS: 0.18156 (0.12148) | TIME: 0:06:11 |
[VALID F1] EPOCH: 1/4 | STEP: 440/489 | LOSS: 0.10193 (0.12230) | TIME: 0:06:49 |
[VALID F1] EPOCH: 1/4 | STEP: 480/489 | LOSS: 0.10605 (0.12185) | TIME: 0:07:26 |
[VALID F1] EPOCH: 1/4 | STEP: 488/489 | LOSS: 0.08623 (0.12145) | TIME: 0:07:33 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.23642 |      0.12145 |  0.49479 | 0.502 | 0.505 | 0.466 | 0.489 | 0.526 | 0.481 | 1:01:50 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4947948753833771

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 2/4 | STEP: 0000/1467 | LOSS: 0.06363 (0.06363) | LR: 0.00001808 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0040/1467 | LOSS: 0.12342 (0.09998) | LR: 0.00001795 | TIME: 0:01:21 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0080/1467 | LOSS: 0.22920 (0.10537) | LR: 0.00001781 | TIME: 0:02:54 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0120/1467 | LOSS: 0.08756 (0.11094) | LR: 0.00001766 | TIME: 0:04:22 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0160/1467 | LOSS: 0.11205 (0.11141) | LR: 0.00001751 | TIME: 0:05:48 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0200/1467 | LOSS: 0.04781 (0.11320) | LR: 0.00001736 | TIME: 0:07:15 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0240/1467 | LOSS: 0.04873 (0.11714) | LR: 0.00001721 | TIME: 0:08:40 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0280/1467 | LOSS: 0.04911 (0.11694) | LR: 0.00001704 | TIME: 0:10:19 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0320/1467 | LOSS: 0.04730 (0.11733) | LR: 0.00001688 | TIME: 0:11:47 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0360/1467 | LOSS: 0.12942 (0.11651) | LR: 0.00001671 | TIME: 0:13:21 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0400/1467 | LOSS: 0.08811 (0.11729) | LR: 0.00001654 | TIME: 0:15:03 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0440/1467 | LOSS: 0.05172 (0.11688) | LR: 0.00001637 | TIME: 0:16:28 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0480/1467 | LOSS: 0.14845 (0.11670) | LR: 0.00001619 | TIME: 0:17:59 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0520/1467 | LOSS: 0.06798 (0.11621) | LR: 0.00001601 | TIME: 0:19:18 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0560/1467 | LOSS: 0.08494 (0.11629) | LR: 0.00001583 | TIME: 0:20:52 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0600/1467 | LOSS: 0.11495 (0.11669) | LR: 0.00001564 | TIME: 0:22:18 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0640/1467 | LOSS: 0.21122 (0.11747) | LR: 0.00001545 | TIME: 0:23:48 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0680/1467 | LOSS: 0.13916 (0.11674) | LR: 0.00001525 | TIME: 0:25:16 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0720/1467 | LOSS: 0.18575 (0.11665) | LR: 0.00001506 | TIME: 0:26:34 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0760/1467 | LOSS: 0.09141 (0.11675) | LR: 0.00001486 | TIME: 0:28:04 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0800/1467 | LOSS: 0.12192 (0.11668) | LR: 0.00001466 | TIME: 0:29:30 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0840/1467 | LOSS: 0.08073 (0.11664) | LR: 0.00001446 | TIME: 0:31:12 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0880/1467 | LOSS: 0.11662 (0.11575) | LR: 0.00001425 | TIME: 0:32:44 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0920/1467 | LOSS: 0.15137 (0.11566) | LR: 0.00001404 | TIME: 0:34:10 |
[TRAIN F1] EPOCH: 2/4 | STEP: 0960/1467 | LOSS: 0.17866 (0.11554) | LR: 0.00001383 | TIME: 0:35:33 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1000/1467 | LOSS: 0.05545 (0.11456) | LR: 0.00001362 | TIME: 0:37:02 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1040/1467 | LOSS: 0.05080 (0.11421) | LR: 0.00001341 | TIME: 0:38:26 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1080/1467 | LOSS: 0.10741 (0.11408) | LR: 0.00001319 | TIME: 0:40:00 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1120/1467 | LOSS: 0.08981 (0.11345) | LR: 0.00001297 | TIME: 0:41:32 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1160/1467 | LOSS: 0.10535 (0.11298) | LR: 0.00001276 | TIME: 0:43:00 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1200/1467 | LOSS: 0.05371 (0.11314) | LR: 0.00001254 | TIME: 0:44:21 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1240/1467 | LOSS: 0.10879 (0.11334) | LR: 0.00001231 | TIME: 0:45:51 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1280/1467 | LOSS: 0.06954 (0.11290) | LR: 0.00001209 | TIME: 0:47:17 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1320/1467 | LOSS: 0.06017 (0.11273) | LR: 0.00001187 | TIME: 0:48:42 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1360/1467 | LOSS: 0.11672 (0.11296) | LR: 0.00001164 | TIME: 0:50:03 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1400/1467 | LOSS: 0.06490 (0.11294) | LR: 0.00001142 | TIME: 0:51:32 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1440/1467 | LOSS: 0.10610 (0.11274) | LR: 0.00001119 | TIME: 0:52:54 |
[TRAIN F1] EPOCH: 2/4 | STEP: 1466/1467 | LOSS: 0.07134 (0.11272) | LR: 0.00001104 | TIME: 0:53:51 |

VALID_LOOP
[VALID F1] EPOCH: 2/4 | STEP: 000/489 | LOSS: 0.06669 (0.06669) | TIME: 0:00:01 |
[VALID F1] EPOCH: 2/4 | STEP: 040/489 | LOSS: 0.07537 (0.10075) | TIME: 0:00:38 |
[VALID F1] EPOCH: 2/4 | STEP: 080/489 | LOSS: 0.04530 (0.10187) | TIME: 0:01:15 |
[VALID F1] EPOCH: 2/4 | STEP: 120/489 | LOSS: 0.09385 (0.10368) | TIME: 0:01:51 |
[VALID F1] EPOCH: 2/4 | STEP: 160/489 | LOSS: 0.08582 (0.10791) | TIME: 0:02:28 |
[VALID F1] EPOCH: 2/4 | STEP: 200/489 | LOSS: 0.07583 (0.10476) | TIME: 0:03:04 |
[VALID F1] EPOCH: 2/4 | STEP: 240/489 | LOSS: 0.13164 (0.10619) | TIME: 0:03:41 |
[VALID F1] EPOCH: 2/4 | STEP: 280/489 | LOSS: 0.08336 (0.10517) | TIME: 0:04:17 |
[VALID F1] EPOCH: 2/4 | STEP: 320/489 | LOSS: 0.07857 (0.10628) | TIME: 0:04:54 |
[VALID F1] EPOCH: 2/4 | STEP: 360/489 | LOSS: 0.05530 (0.10479) | TIME: 0:05:30 |
[VALID F1] EPOCH: 2/4 | STEP: 400/489 | LOSS: 0.09722 (0.10366) | TIME: 0:06:07 |
[VALID F1] EPOCH: 2/4 | STEP: 440/489 | LOSS: 0.13892 (0.10463) | TIME: 0:06:43 |
[VALID F1] EPOCH: 2/4 | STEP: 480/489 | LOSS: 0.10210 (0.10489) | TIME: 0:07:20 |
[VALID F1] EPOCH: 2/4 | STEP: 488/489 | LOSS: 0.03875 (0.10453) | TIME: 0:07:27 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11272 |      0.10453 |  0.45821 | 0.484 | 0.445 | 0.431 | 0.459 | 0.473 | 0.458 | 1:01:18 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4582069218158722

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 3/4 | STEP: 0000/1467 | LOSS: 0.03707 (0.03707) | LR: 0.00001104 | TIME: 0:00:02 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0040/1467 | LOSS: 0.03563 (0.10152) | LR: 0.00001081 | TIME: 0:01:28 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0080/1467 | LOSS: 0.09530 (0.09612) | LR: 0.00001058 | TIME: 0:02:55 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0120/1467 | LOSS: 0.11313 (0.09315) | LR: 0.00001035 | TIME: 0:04:19 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0160/1467 | LOSS: 0.09044 (0.09363) | LR: 0.00001013 | TIME: 0:05:45 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0200/1467 | LOSS: 0.21608 (0.09215) | LR: 0.00000990 | TIME: 0:07:11 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0240/1467 | LOSS: 0.08525 (0.09202) | LR: 0.00000967 | TIME: 0:08:51 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0280/1467 | LOSS: 0.09842 (0.09091) | LR: 0.00000944 | TIME: 0:10:13 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0320/1467 | LOSS: 0.05667 (0.09204) | LR: 0.00000921 | TIME: 0:11:44 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0360/1467 | LOSS: 0.10268 (0.09385) | LR: 0.00000899 | TIME: 0:13:00 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0400/1467 | LOSS: 0.03945 (0.09516) | LR: 0.00000876 | TIME: 0:14:25 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0440/1467 | LOSS: 0.05639 (0.09505) | LR: 0.00000853 | TIME: 0:15:53 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0480/1467 | LOSS: 0.09218 (0.09377) | LR: 0.00000831 | TIME: 0:17:28 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0520/1467 | LOSS: 0.07551 (0.09476) | LR: 0.00000808 | TIME: 0:18:45 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0560/1467 | LOSS: 0.11800 (0.09531) | LR: 0.00000786 | TIME: 0:20:06 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0600/1467 | LOSS: 0.08221 (0.09457) | LR: 0.00000764 | TIME: 0:21:31 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0640/1467 | LOSS: 0.14627 (0.09346) | LR: 0.00000741 | TIME: 0:23:13 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0680/1467 | LOSS: 0.20746 (0.09389) | LR: 0.00000719 | TIME: 0:24:32 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0720/1467 | LOSS: 0.07883 (0.09360) | LR: 0.00000698 | TIME: 0:26:05 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0760/1467 | LOSS: 0.25486 (0.09312) | LR: 0.00000676 | TIME: 0:27:33 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0800/1467 | LOSS: 0.06930 (0.09248) | LR: 0.00000654 | TIME: 0:28:55 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0840/1467 | LOSS: 0.09830 (0.09231) | LR: 0.00000633 | TIME: 0:30:24 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0880/1467 | LOSS: 0.05732 (0.09270) | LR: 0.00000612 | TIME: 0:31:58 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0920/1467 | LOSS: 0.14487 (0.09233) | LR: 0.00000591 | TIME: 0:33:35 |
[TRAIN F1] EPOCH: 3/4 | STEP: 0960/1467 | LOSS: 0.05480 (0.09176) | LR: 0.00000570 | TIME: 0:35:11 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1000/1467 | LOSS: 0.17263 (0.09153) | LR: 0.00000550 | TIME: 0:36:45 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1040/1467 | LOSS: 0.02830 (0.09117) | LR: 0.00000529 | TIME: 0:38:10 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1080/1467 | LOSS: 0.04511 (0.09084) | LR: 0.00000509 | TIME: 0:39:45 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1120/1467 | LOSS: 0.04249 (0.09107) | LR: 0.00000490 | TIME: 0:41:19 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1160/1467 | LOSS: 0.12965 (0.09087) | LR: 0.00000470 | TIME: 0:42:50 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1200/1467 | LOSS: 0.10612 (0.09066) | LR: 0.00000451 | TIME: 0:44:21 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1240/1467 | LOSS: 0.06055 (0.09057) | LR: 0.00000432 | TIME: 0:45:57 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1280/1467 | LOSS: 0.03941 (0.09043) | LR: 0.00000413 | TIME: 0:47:22 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1320/1467 | LOSS: 0.09034 (0.08981) | LR: 0.00000395 | TIME: 0:48:41 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1360/1467 | LOSS: 0.06751 (0.08942) | LR: 0.00000377 | TIME: 0:49:58 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1400/1467 | LOSS: 0.02116 (0.08950) | LR: 0.00000359 | TIME: 0:51:24 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1440/1467 | LOSS: 0.05568 (0.08936) | LR: 0.00000342 | TIME: 0:52:42 |
[TRAIN F1] EPOCH: 3/4 | STEP: 1466/1467 | LOSS: 0.04899 (0.08939) | LR: 0.00000331 | TIME: 0:53:38 |

VALID_LOOP
[VALID F1] EPOCH: 3/4 | STEP: 000/489 | LOSS: 0.08173 (0.08173) | TIME: 0:00:01 |
[VALID F1] EPOCH: 3/4 | STEP: 040/489 | LOSS: 0.08054 (0.10233) | TIME: 0:00:38 |
[VALID F1] EPOCH: 3/4 | STEP: 080/489 | LOSS: 0.03377 (0.10432) | TIME: 0:01:14 |
[VALID F1] EPOCH: 3/4 | STEP: 120/489 | LOSS: 0.09184 (0.10557) | TIME: 0:01:51 |
[VALID F1] EPOCH: 3/4 | STEP: 160/489 | LOSS: 0.08806 (0.11001) | TIME: 0:02:27 |
[VALID F1] EPOCH: 3/4 | STEP: 200/489 | LOSS: 0.09227 (0.10686) | TIME: 0:03:04 |
[VALID F1] EPOCH: 3/4 | STEP: 240/489 | LOSS: 0.11881 (0.10769) | TIME: 0:03:40 |
[VALID F1] EPOCH: 3/4 | STEP: 280/489 | LOSS: 0.08558 (0.10662) | TIME: 0:04:17 |
[VALID F1] EPOCH: 3/4 | STEP: 320/489 | LOSS: 0.06974 (0.10751) | TIME: 0:04:53 |
[VALID F1] EPOCH: 3/4 | STEP: 360/489 | LOSS: 0.07458 (0.10579) | TIME: 0:05:30 |
[VALID F1] EPOCH: 3/4 | STEP: 400/489 | LOSS: 0.11920 (0.10452) | TIME: 0:06:06 |
[VALID F1] EPOCH: 3/4 | STEP: 440/489 | LOSS: 0.11920 (0.10531) | TIME: 0:06:43 |
[VALID F1] EPOCH: 3/4 | STEP: 480/489 | LOSS: 0.08404 (0.10555) | TIME: 0:07:19 |
[VALID F1] EPOCH: 3/4 | STEP: 488/489 | LOSS: 0.02702 (0.10528) | TIME: 0:07:26 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.08939 |      0.10528 |  0.45974 | 0.494 | 0.446 | 0.424 | 0.464 | 0.470 | 0.460 | 1:01:05 |

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 4/4 | STEP: 0000/1467 | LOSS: 0.09474 (0.09474) | LR: 0.00000330 | TIME: 0:00:02 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0040/1467 | LOSS: 0.03973 (0.07601) | LR: 0.00000314 | TIME: 0:01:32 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0080/1467 | LOSS: 0.07251 (0.07615) | LR: 0.00000297 | TIME: 0:02:58 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0120/1467 | LOSS: 0.08121 (0.07507) | LR: 0.00000281 | TIME: 0:04:18 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0160/1467 | LOSS: 0.09940 (0.07444) | LR: 0.00000265 | TIME: 0:05:44 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0200/1467 | LOSS: 0.09923 (0.07448) | LR: 0.00000250 | TIME: 0:07:12 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0240/1467 | LOSS: 0.06355 (0.07489) | LR: 0.00000235 | TIME: 0:08:39 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0280/1467 | LOSS: 0.11596 (0.07526) | LR: 0.00000221 | TIME: 0:10:15 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0320/1467 | LOSS: 0.07256 (0.07427) | LR: 0.00000207 | TIME: 0:11:32 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0360/1467 | LOSS: 0.09663 (0.07555) | LR: 0.00000193 | TIME: 0:13:05 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0400/1467 | LOSS: 0.05148 (0.07517) | LR: 0.00000180 | TIME: 0:14:47 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0440/1467 | LOSS: 0.04780 (0.07537) | LR: 0.00000167 | TIME: 0:16:11 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0480/1467 | LOSS: 0.02582 (0.07466) | LR: 0.00000154 | TIME: 0:17:48 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0520/1467 | LOSS: 0.09113 (0.07471) | LR: 0.00000142 | TIME: 0:19:18 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0560/1467 | LOSS: 0.06263 (0.07423) | LR: 0.00000131 | TIME: 0:20:38 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0600/1467 | LOSS: 0.03882 (0.07392) | LR: 0.00000120 | TIME: 0:22:06 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0640/1467 | LOSS: 0.05246 (0.07383) | LR: 0.00000109 | TIME: 0:23:34 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0680/1467 | LOSS: 0.07485 (0.07405) | LR: 0.00000099 | TIME: 0:25:02 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0720/1467 | LOSS: 0.05497 (0.07344) | LR: 0.00000089 | TIME: 0:26:25 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0760/1467 | LOSS: 0.09136 (0.07365) | LR: 0.00000080 | TIME: 0:28:00 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0800/1467 | LOSS: 0.04058 (0.07310) | LR: 0.00000071 | TIME: 0:29:17 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0840/1467 | LOSS: 0.04829 (0.07306) | LR: 0.00000063 | TIME: 0:30:43 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0880/1467 | LOSS: 0.05362 (0.07335) | LR: 0.00000055 | TIME: 0:32:15 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0920/1467 | LOSS: 0.03159 (0.07342) | LR: 0.00000048 | TIME: 0:33:44 |
[TRAIN F1] EPOCH: 4/4 | STEP: 0960/1467 | LOSS: 0.06476 (0.07348) | LR: 0.00000041 | TIME: 0:35:17 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1000/1467 | LOSS: 0.08029 (0.07367) | LR: 0.00000035 | TIME: 0:36:53 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1040/1467 | LOSS: 0.06121 (0.07355) | LR: 0.00000029 | TIME: 0:38:37 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1080/1467 | LOSS: 0.02674 (0.07372) | LR: 0.00000024 | TIME: 0:40:12 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1120/1467 | LOSS: 0.06620 (0.07372) | LR: 0.00000019 | TIME: 0:41:34 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1160/1467 | LOSS: 0.09385 (0.07365) | LR: 0.00000015 | TIME: 0:43:06 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1200/1467 | LOSS: 0.04130 (0.07377) | LR: 0.00000012 | TIME: 0:44:46 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1240/1467 | LOSS: 0.08987 (0.07391) | LR: 0.00000008 | TIME: 0:46:05 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1280/1467 | LOSS: 0.05398 (0.07418) | LR: 0.00000006 | TIME: 0:47:35 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1320/1467 | LOSS: 0.11702 (0.07382) | LR: 0.00000003 | TIME: 0:48:59 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1360/1467 | LOSS: 0.10316 (0.07354) | LR: 0.00000002 | TIME: 0:50:26 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1400/1467 | LOSS: 0.07867 (0.07355) | LR: 0.00000001 | TIME: 0:52:06 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1440/1467 | LOSS: 0.02577 (0.07361) | LR: 0.00000000 | TIME: 0:53:36 |
[TRAIN F1] EPOCH: 4/4 | STEP: 1466/1467 | LOSS: 0.05221 (0.07358) | LR: 0.00000000 | TIME: 0:54:32 |

VALID_LOOP
[VALID F1] EPOCH: 4/4 | STEP: 000/489 | LOSS: 0.07312 (0.07312) | TIME: 0:00:01 |
[VALID F1] EPOCH: 4/4 | STEP: 040/489 | LOSS: 0.06696 (0.09861) | TIME: 0:00:39 |
[VALID F1] EPOCH: 4/4 | STEP: 080/489 | LOSS: 0.03507 (0.10023) | TIME: 0:01:16 |
[VALID F1] EPOCH: 4/4 | STEP: 120/489 | LOSS: 0.08893 (0.10121) | TIME: 0:01:54 |
[VALID F1] EPOCH: 4/4 | STEP: 160/489 | LOSS: 0.06389 (0.10605) | TIME: 0:02:31 |
[VALID F1] EPOCH: 4/4 | STEP: 200/489 | LOSS: 0.06734 (0.10295) | TIME: 0:03:09 |
[VALID F1] EPOCH: 4/4 | STEP: 240/489 | LOSS: 0.14185 (0.10446) | TIME: 0:03:46 |
[VALID F1] EPOCH: 4/4 | STEP: 280/489 | LOSS: 0.07472 (0.10322) | TIME: 0:04:23 |
[VALID F1] EPOCH: 4/4 | STEP: 320/489 | LOSS: 0.07313 (0.10441) | TIME: 0:05:01 |
[VALID F1] EPOCH: 4/4 | STEP: 360/489 | LOSS: 0.07096 (0.10316) | TIME: 0:05:38 |
[VALID F1] EPOCH: 4/4 | STEP: 400/489 | LOSS: 0.10465 (0.10197) | TIME: 0:06:16 |
[VALID F1] EPOCH: 4/4 | STEP: 440/489 | LOSS: 0.14642 (0.10305) | TIME: 0:06:53 |
[VALID F1] EPOCH: 4/4 | STEP: 480/489 | LOSS: 0.08435 (0.10353) | TIME: 0:07:31 |
[VALID F1] EPOCH: 4/4 | STEP: 488/489 | LOSS: 0.03240 (0.10318) | TIME: 0:07:38 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.07358 |      0.10318 |  0.45514 | 0.483 | 0.444 | 0.424 | 0.458 | 0.470 | 0.452 | 1:02:11 |


[SAVED] EPOCH: 4 | MCRMSE: 0.45514413714408875


----------------------------------- FOLD 1 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45514     0.48264   0.44366       0.42393        0.45792    0.47034        0.45238

################################### END OF FOlD 1 ###################################


Date: 2022-11-17 13:00:39.074205+07:00 (GMT+7)
Mode: EXPERIMENTING_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 5864}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 1/4 | STEP: 0000/1466 | LOSS: 2.48203 (2.48203) | LR: 0.00000005 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0040/1466 | LOSS: 1.93698 (2.39308) | LR: 0.00000224 | TIME: 0:01:34 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0080/1466 | LOSS: 0.25835 (1.82593) | LR: 0.00000443 | TIME: 0:02:52 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0120/1466 | LOSS: 0.21098 (1.30883) | LR: 0.00000661 | TIME: 0:04:21 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0160/1466 | LOSS: 0.07210 (1.02751) | LR: 0.00000880 | TIME: 0:05:54 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0200/1466 | LOSS: 0.19339 (0.85754) | LR: 0.00001098 | TIME: 0:07:26 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0240/1466 | LOSS: 0.09709 (0.74224) | LR: 0.00001317 | TIME: 0:08:54 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0280/1466 | LOSS: 0.08971 (0.66105) | LR: 0.00001536 | TIME: 0:10:26 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0320/1466 | LOSS: 0.10887 (0.60077) | LR: 0.00001754 | TIME: 0:12:06 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0360/1466 | LOSS: 0.13210 (0.55276) | LR: 0.00001973 | TIME: 0:13:35 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0400/1466 | LOSS: 0.32202 (0.51163) | LR: 0.00002000 | TIME: 0:15:18 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0440/1466 | LOSS: 0.11309 (0.47830) | LR: 0.00001999 | TIME: 0:16:54 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0480/1466 | LOSS: 0.14057 (0.45041) | LR: 0.00001998 | TIME: 0:18:23 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0520/1466 | LOSS: 0.14099 (0.42817) | LR: 0.00001996 | TIME: 0:20:05 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0560/1466 | LOSS: 0.10999 (0.40797) | LR: 0.00001994 | TIME: 0:21:42 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0600/1466 | LOSS: 0.04590 (0.38987) | LR: 0.00001991 | TIME: 0:23:06 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0640/1466 | LOSS: 0.24868 (0.37560) | LR: 0.00001988 | TIME: 0:24:51 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0680/1466 | LOSS: 0.07224 (0.36160) | LR: 0.00001984 | TIME: 0:26:14 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0720/1466 | LOSS: 0.15414 (0.34926) | LR: 0.00001979 | TIME: 0:27:51 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0760/1466 | LOSS: 0.05278 (0.33845) | LR: 0.00001975 | TIME: 0:29:31 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0800/1466 | LOSS: 0.08241 (0.32757) | LR: 0.00001969 | TIME: 0:30:51 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0840/1466 | LOSS: 0.09012 (0.31879) | LR: 0.00001963 | TIME: 0:32:25 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0880/1466 | LOSS: 0.29590 (0.31007) | LR: 0.00001957 | TIME: 0:33:57 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0920/1466 | LOSS: 0.37993 (0.30206) | LR: 0.00001950 | TIME: 0:35:34 |
[TRAIN F2] EPOCH: 1/4 | STEP: 0960/1466 | LOSS: 0.06300 (0.29484) | LR: 0.00001943 | TIME: 0:37:05 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1000/1466 | LOSS: 0.17472 (0.28884) | LR: 0.00001935 | TIME: 0:38:41 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1040/1466 | LOSS: 0.22740 (0.28277) | LR: 0.00001927 | TIME: 0:40:01 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1080/1466 | LOSS: 0.13510 (0.27716) | LR: 0.00001918 | TIME: 0:41:24 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1120/1466 | LOSS: 0.08564 (0.27214) | LR: 0.00001908 | TIME: 0:42:57 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1160/1466 | LOSS: 0.11424 (0.26695) | LR: 0.00001899 | TIME: 0:44:30 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1200/1466 | LOSS: 0.44048 (0.26250) | LR: 0.00001888 | TIME: 0:45:52 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1240/1466 | LOSS: 0.09890 (0.25874) | LR: 0.00001878 | TIME: 0:47:34 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1280/1466 | LOSS: 0.11586 (0.25513) | LR: 0.00001866 | TIME: 0:49:03 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1320/1466 | LOSS: 0.10291 (0.25105) | LR: 0.00001855 | TIME: 0:50:30 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1360/1466 | LOSS: 0.12931 (0.24735) | LR: 0.00001843 | TIME: 0:51:55 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1400/1466 | LOSS: 0.08862 (0.24393) | LR: 0.00001830 | TIME: 0:53:28 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1440/1466 | LOSS: 0.07133 (0.24066) | LR: 0.00001817 | TIME: 0:54:57 |
[TRAIN F2] EPOCH: 1/4 | STEP: 1465/1466 | LOSS: 0.21003 (0.23897) | LR: 0.00001809 | TIME: 0:56:06 |

VALID_LOOP
[VALID F2] EPOCH: 1/4 | STEP: 000/489 | LOSS: 0.24925 (0.24925) | TIME: 0:00:01 |
[VALID F2] EPOCH: 1/4 | STEP: 040/489 | LOSS: 0.08191 (0.11716) | TIME: 0:00:38 |
[VALID F2] EPOCH: 1/4 | STEP: 080/489 | LOSS: 0.12769 (0.12570) | TIME: 0:01:15 |
[VALID F2] EPOCH: 1/4 | STEP: 120/489 | LOSS: 0.04111 (0.11921) | TIME: 0:01:52 |
[VALID F2] EPOCH: 1/4 | STEP: 160/489 | LOSS: 0.06574 (0.11872) | TIME: 0:02:28 |
[VALID F2] EPOCH: 1/4 | STEP: 200/489 | LOSS: 0.15009 (0.11871) | TIME: 0:03:05 |
[VALID F2] EPOCH: 1/4 | STEP: 240/489 | LOSS: 0.38990 (0.12133) | TIME: 0:03:42 |
[VALID F2] EPOCH: 1/4 | STEP: 280/489 | LOSS: 0.17866 (0.12192) | TIME: 0:04:19 |
[VALID F2] EPOCH: 1/4 | STEP: 320/489 | LOSS: 0.05855 (0.12030) | TIME: 0:04:55 |
[VALID F2] EPOCH: 1/4 | STEP: 360/489 | LOSS: 0.13717 (0.12000) | TIME: 0:05:32 |
[VALID F2] EPOCH: 1/4 | STEP: 400/489 | LOSS: 0.10494 (0.12052) | TIME: 0:06:09 |
[VALID F2] EPOCH: 1/4 | STEP: 440/489 | LOSS: 0.05616 (0.12047) | TIME: 0:06:45 |
[VALID F2] EPOCH: 1/4 | STEP: 480/489 | LOSS: 0.16702 (0.11990) | TIME: 0:07:22 |
[VALID F2] EPOCH: 1/4 | STEP: 488/489 | LOSS: 0.19780 (0.11972) | TIME: 0:07:29 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.23897 |      0.11972 |   0.4913 | 0.487 | 0.460 | 0.490 | 0.470 | 0.521 | 0.519 | 1:03:36 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4913041293621063

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 2/4 | STEP: 0000/1466 | LOSS: 0.22130 (0.22130) | LR: 0.00001809 | TIME: 0:00:02 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0040/1466 | LOSS: 0.08229 (0.10558) | LR: 0.00001795 | TIME: 0:01:32 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0080/1466 | LOSS: 0.04186 (0.10961) | LR: 0.00001781 | TIME: 0:02:55 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0120/1466 | LOSS: 0.06179 (0.11113) | LR: 0.00001766 | TIME: 0:04:36 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0160/1466 | LOSS: 0.20724 (0.11221) | LR: 0.00001751 | TIME: 0:05:57 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0200/1466 | LOSS: 0.04816 (0.11343) | LR: 0.00001736 | TIME: 0:07:24 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0240/1466 | LOSS: 0.18011 (0.11357) | LR: 0.00001721 | TIME: 0:08:56 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0280/1466 | LOSS: 0.16901 (0.11495) | LR: 0.00001704 | TIME: 0:10:20 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0320/1466 | LOSS: 0.06378 (0.11537) | LR: 0.00001688 | TIME: 0:12:00 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0360/1466 | LOSS: 0.14020 (0.11561) | LR: 0.00001671 | TIME: 0:13:31 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0400/1466 | LOSS: 0.08821 (0.11468) | LR: 0.00001654 | TIME: 0:15:03 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0440/1466 | LOSS: 0.11939 (0.11502) | LR: 0.00001637 | TIME: 0:16:34 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0480/1466 | LOSS: 0.12134 (0.11467) | LR: 0.00001619 | TIME: 0:18:04 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0520/1466 | LOSS: 0.07204 (0.11640) | LR: 0.00001601 | TIME: 0:19:30 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0560/1466 | LOSS: 0.17469 (0.11504) | LR: 0.00001582 | TIME: 0:21:02 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0600/1466 | LOSS: 0.26570 (0.11503) | LR: 0.00001564 | TIME: 0:22:38 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0640/1466 | LOSS: 0.16898 (0.11550) | LR: 0.00001545 | TIME: 0:24:28 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0680/1466 | LOSS: 0.14500 (0.11455) | LR: 0.00001525 | TIME: 0:25:50 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0720/1466 | LOSS: 0.10980 (0.11452) | LR: 0.00001506 | TIME: 0:27:24 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0760/1466 | LOSS: 0.04982 (0.11453) | LR: 0.00001486 | TIME: 0:28:51 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0800/1466 | LOSS: 0.11673 (0.11418) | LR: 0.00001466 | TIME: 0:30:18 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0840/1466 | LOSS: 0.15416 (0.11535) | LR: 0.00001445 | TIME: 0:31:44 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0880/1466 | LOSS: 0.13212 (0.11500) | LR: 0.00001425 | TIME: 0:33:13 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0920/1466 | LOSS: 0.05607 (0.11459) | LR: 0.00001404 | TIME: 0:34:59 |
[TRAIN F2] EPOCH: 2/4 | STEP: 0960/1466 | LOSS: 0.10449 (0.11399) | LR: 0.00001383 | TIME: 0:36:32 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1000/1466 | LOSS: 0.08760 (0.11415) | LR: 0.00001362 | TIME: 0:38:04 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1040/1466 | LOSS: 0.22875 (0.11459) | LR: 0.00001340 | TIME: 0:39:43 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1080/1466 | LOSS: 0.10044 (0.11494) | LR: 0.00001319 | TIME: 0:41:17 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1120/1466 | LOSS: 0.11044 (0.11459) | LR: 0.00001297 | TIME: 0:42:42 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1160/1466 | LOSS: 0.10818 (0.11413) | LR: 0.00001275 | TIME: 0:44:10 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1200/1466 | LOSS: 0.07823 (0.11337) | LR: 0.00001253 | TIME: 0:45:47 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1240/1466 | LOSS: 0.13761 (0.11318) | LR: 0.00001231 | TIME: 0:47:10 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1280/1466 | LOSS: 0.07993 (0.11289) | LR: 0.00001209 | TIME: 0:48:42 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1320/1466 | LOSS: 0.05134 (0.11253) | LR: 0.00001186 | TIME: 0:50:08 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1360/1466 | LOSS: 0.19875 (0.11214) | LR: 0.00001164 | TIME: 0:51:33 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1400/1466 | LOSS: 0.09583 (0.11182) | LR: 0.00001141 | TIME: 0:53:06 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1440/1466 | LOSS: 0.05659 (0.11155) | LR: 0.00001119 | TIME: 0:54:24 |
[TRAIN F2] EPOCH: 2/4 | STEP: 1465/1466 | LOSS: 0.04586 (0.11115) | LR: 0.00001104 | TIME: 0:55:24 |

VALID_LOOP
[VALID F2] EPOCH: 2/4 | STEP: 000/489 | LOSS: 0.17923 (0.17923) | TIME: 0:00:01 |
[VALID F2] EPOCH: 2/4 | STEP: 040/489 | LOSS: 0.06354 (0.09883) | TIME: 0:00:38 |
[VALID F2] EPOCH: 2/4 | STEP: 080/489 | LOSS: 0.08213 (0.11087) | TIME: 0:01:14 |
[VALID F2] EPOCH: 2/4 | STEP: 120/489 | LOSS: 0.06771 (0.10727) | TIME: 0:01:51 |
[VALID F2] EPOCH: 2/4 | STEP: 160/489 | LOSS: 0.03468 (0.10617) | TIME: 0:02:27 |
[VALID F2] EPOCH: 2/4 | STEP: 200/489 | LOSS: 0.10384 (0.10609) | TIME: 0:03:04 |
[VALID F2] EPOCH: 2/4 | STEP: 240/489 | LOSS: 0.43675 (0.10741) | TIME: 0:03:40 |
[VALID F2] EPOCH: 2/4 | STEP: 280/489 | LOSS: 0.23489 (0.10710) | TIME: 0:04:16 |
[VALID F2] EPOCH: 2/4 | STEP: 320/489 | LOSS: 0.04171 (0.10479) | TIME: 0:04:53 |
[VALID F2] EPOCH: 2/4 | STEP: 360/489 | LOSS: 0.17916 (0.10365) | TIME: 0:05:29 |
[VALID F2] EPOCH: 2/4 | STEP: 400/489 | LOSS: 0.10755 (0.10510) | TIME: 0:06:06 |
[VALID F2] EPOCH: 2/4 | STEP: 440/489 | LOSS: 0.04856 (0.10524) | TIME: 0:06:42 |
[VALID F2] EPOCH: 2/4 | STEP: 480/489 | LOSS: 0.15269 (0.10602) | TIME: 0:07:18 |
[VALID F2] EPOCH: 2/4 | STEP: 488/489 | LOSS: 0.09573 (0.10561) | TIME: 0:07:26 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11115 |      0.10561 |  0.46097 | 0.482 | 0.453 | 0.418 | 0.469 | 0.478 | 0.465 | 1:02:50 |


[SAVED] EPOCH: 2 | MCRMSE: 0.46096542477607727

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 3/4 | STEP: 0000/1466 | LOSS: 0.06925 (0.06925) | LR: 0.00001104 | TIME: 0:00:02 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0040/1466 | LOSS: 0.08360 (0.09827) | LR: 0.00001081 | TIME: 0:01:34 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0080/1466 | LOSS: 0.05822 (0.09614) | LR: 0.00001058 | TIME: 0:03:13 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0120/1466 | LOSS: 0.09397 (0.09729) | LR: 0.00001035 | TIME: 0:04:51 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0160/1466 | LOSS: 0.05699 (0.09378) | LR: 0.00001013 | TIME: 0:06:23 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0200/1466 | LOSS: 0.12491 (0.09524) | LR: 0.00000990 | TIME: 0:07:50 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0240/1466 | LOSS: 0.07733 (0.09351) | LR: 0.00000967 | TIME: 0:09:25 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0280/1466 | LOSS: 0.05701 (0.09364) | LR: 0.00000944 | TIME: 0:10:50 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0320/1466 | LOSS: 0.07237 (0.09265) | LR: 0.00000921 | TIME: 0:12:16 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0360/1466 | LOSS: 0.13641 (0.09254) | LR: 0.00000898 | TIME: 0:13:47 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0400/1466 | LOSS: 0.13874 (0.09274) | LR: 0.00000876 | TIME: 0:15:10 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0440/1466 | LOSS: 0.06192 (0.09170) | LR: 0.00000853 | TIME: 0:16:38 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0480/1466 | LOSS: 0.09045 (0.09133) | LR: 0.00000831 | TIME: 0:18:13 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0520/1466 | LOSS: 0.07529 (0.09105) | LR: 0.00000808 | TIME: 0:19:35 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0560/1466 | LOSS: 0.04848 (0.09002) | LR: 0.00000786 | TIME: 0:21:09 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0600/1466 | LOSS: 0.05484 (0.08971) | LR: 0.00000763 | TIME: 0:22:32 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0640/1466 | LOSS: 0.11548 (0.08960) | LR: 0.00000741 | TIME: 0:24:16 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0680/1466 | LOSS: 0.09087 (0.08992) | LR: 0.00000719 | TIME: 0:25:51 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0720/1466 | LOSS: 0.12196 (0.08979) | LR: 0.00000697 | TIME: 0:27:18 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0760/1466 | LOSS: 0.04268 (0.09004) | LR: 0.00000676 | TIME: 0:28:44 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0800/1466 | LOSS: 0.08779 (0.09068) | LR: 0.00000654 | TIME: 0:30:24 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0840/1466 | LOSS: 0.13025 (0.09048) | LR: 0.00000633 | TIME: 0:31:45 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0880/1466 | LOSS: 0.07306 (0.09041) | LR: 0.00000612 | TIME: 0:33:17 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0920/1466 | LOSS: 0.11302 (0.09053) | LR: 0.00000591 | TIME: 0:34:58 |
[TRAIN F2] EPOCH: 3/4 | STEP: 0960/1466 | LOSS: 0.11201 (0.09028) | LR: 0.00000570 | TIME: 0:36:34 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1000/1466 | LOSS: 0.07338 (0.09002) | LR: 0.00000549 | TIME: 0:38:04 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1040/1466 | LOSS: 0.06521 (0.08979) | LR: 0.00000529 | TIME: 0:39:39 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1080/1466 | LOSS: 0.11461 (0.09043) | LR: 0.00000509 | TIME: 0:41:28 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1120/1466 | LOSS: 0.06640 (0.09052) | LR: 0.00000489 | TIME: 0:42:45 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1160/1466 | LOSS: 0.06402 (0.09016) | LR: 0.00000470 | TIME: 0:44:07 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1200/1466 | LOSS: 0.04853 (0.09001) | LR: 0.00000451 | TIME: 0:45:36 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1240/1466 | LOSS: 0.08019 (0.09003) | LR: 0.00000432 | TIME: 0:47:02 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1280/1466 | LOSS: 0.02685 (0.08989) | LR: 0.00000413 | TIME: 0:48:29 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1320/1466 | LOSS: 0.13998 (0.08953) | LR: 0.00000395 | TIME: 0:49:56 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1360/1466 | LOSS: 0.07467 (0.08926) | LR: 0.00000377 | TIME: 0:51:23 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1400/1466 | LOSS: 0.07848 (0.08943) | LR: 0.00000359 | TIME: 0:53:00 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1440/1466 | LOSS: 0.09667 (0.08931) | LR: 0.00000341 | TIME: 0:54:33 |
[TRAIN F2] EPOCH: 3/4 | STEP: 1465/1466 | LOSS: 0.05993 (0.08925) | LR: 0.00000331 | TIME: 0:55:32 |

VALID_LOOP
[VALID F2] EPOCH: 3/4 | STEP: 000/489 | LOSS: 0.21277 (0.21277) | TIME: 0:00:01 |
[VALID F2] EPOCH: 3/4 | STEP: 040/489 | LOSS: 0.10854 (0.10448) | TIME: 0:00:38 |
[VALID F2] EPOCH: 3/4 | STEP: 080/489 | LOSS: 0.11367 (0.11378) | TIME: 0:01:14 |
[VALID F2] EPOCH: 3/4 | STEP: 120/489 | LOSS: 0.06901 (0.10906) | TIME: 0:01:51 |
[VALID F2] EPOCH: 3/4 | STEP: 160/489 | LOSS: 0.04395 (0.10793) | TIME: 0:02:27 |
[VALID F2] EPOCH: 3/4 | STEP: 200/489 | LOSS: 0.12773 (0.10837) | TIME: 0:03:04 |
[VALID F2] EPOCH: 3/4 | STEP: 240/489 | LOSS: 0.41666 (0.10989) | TIME: 0:03:40 |
[VALID F2] EPOCH: 3/4 | STEP: 280/489 | LOSS: 0.17468 (0.10993) | TIME: 0:04:16 |
[VALID F2] EPOCH: 3/4 | STEP: 320/489 | LOSS: 0.05243 (0.10773) | TIME: 0:04:53 |
[VALID F2] EPOCH: 3/4 | STEP: 360/489 | LOSS: 0.17176 (0.10689) | TIME: 0:05:29 |
[VALID F2] EPOCH: 3/4 | STEP: 400/489 | LOSS: 0.10015 (0.10818) | TIME: 0:06:06 |
[VALID F2] EPOCH: 3/4 | STEP: 440/489 | LOSS: 0.04751 (0.10851) | TIME: 0:06:42 |
[VALID F2] EPOCH: 3/4 | STEP: 480/489 | LOSS: 0.16347 (0.10833) | TIME: 0:07:18 |
[VALID F2] EPOCH: 3/4 | STEP: 488/489 | LOSS: 0.12322 (0.10790) | TIME: 0:07:26 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.08925 |       0.1079 |  0.46575 | 0.492 | 0.449 | 0.433 | 0.469 | 0.482 | 0.471 | 1:02:58 |

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 4/4 | STEP: 0000/1466 | LOSS: 0.03754 (0.03754) | LR: 0.00000330 | TIME: 0:00:06 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0040/1466 | LOSS: 0.06425 (0.08540) | LR: 0.00000314 | TIME: 0:01:43 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0080/1466 | LOSS: 0.08921 (0.07950) | LR: 0.00000297 | TIME: 0:03:25 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0120/1466 | LOSS: 0.13765 (0.08123) | LR: 0.00000281 | TIME: 0:04:48 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0160/1466 | LOSS: 0.06845 (0.08131) | LR: 0.00000265 | TIME: 0:06:15 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0200/1466 | LOSS: 0.05089 (0.08111) | LR: 0.00000250 | TIME: 0:07:44 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0240/1466 | LOSS: 0.15444 (0.08224) | LR: 0.00000235 | TIME: 0:09:10 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0280/1466 | LOSS: 0.07172 (0.08104) | LR: 0.00000221 | TIME: 0:10:45 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0320/1466 | LOSS: 0.15157 (0.07955) | LR: 0.00000207 | TIME: 0:12:10 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0360/1466 | LOSS: 0.06884 (0.07884) | LR: 0.00000193 | TIME: 0:13:52 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0400/1466 | LOSS: 0.03008 (0.07728) | LR: 0.00000180 | TIME: 0:15:17 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0440/1466 | LOSS: 0.05287 (0.07687) | LR: 0.00000167 | TIME: 0:16:50 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0480/1466 | LOSS: 0.12910 (0.07617) | LR: 0.00000154 | TIME: 0:18:26 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0520/1466 | LOSS: 0.06052 (0.07638) | LR: 0.00000142 | TIME: 0:20:00 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0560/1466 | LOSS: 0.06614 (0.07581) | LR: 0.00000131 | TIME: 0:21:34 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0600/1466 | LOSS: 0.14987 (0.07600) | LR: 0.00000120 | TIME: 0:23:14 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0640/1466 | LOSS: 0.08183 (0.07593) | LR: 0.00000109 | TIME: 0:24:49 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0680/1466 | LOSS: 0.05259 (0.07607) | LR: 0.00000099 | TIME: 0:26:23 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0720/1466 | LOSS: 0.06852 (0.07542) | LR: 0.00000089 | TIME: 0:27:52 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0760/1466 | LOSS: 0.05430 (0.07506) | LR: 0.00000080 | TIME: 0:29:31 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0800/1466 | LOSS: 0.11637 (0.07483) | LR: 0.00000071 | TIME: 0:31:07 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0840/1466 | LOSS: 0.10809 (0.07485) | LR: 0.00000063 | TIME: 0:32:39 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0880/1466 | LOSS: 0.15714 (0.07493) | LR: 0.00000055 | TIME: 0:34:23 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0920/1466 | LOSS: 0.05516 (0.07519) | LR: 0.00000048 | TIME: 0:35:44 |
[TRAIN F2] EPOCH: 4/4 | STEP: 0960/1466 | LOSS: 0.05018 (0.07536) | LR: 0.00000041 | TIME: 0:37:24 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1000/1466 | LOSS: 0.07302 (0.07526) | LR: 0.00000035 | TIME: 0:38:55 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1040/1466 | LOSS: 0.12558 (0.07506) | LR: 0.00000029 | TIME: 0:40:25 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1080/1466 | LOSS: 0.09922 (0.07505) | LR: 0.00000024 | TIME: 0:41:51 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1120/1466 | LOSS: 0.06024 (0.07509) | LR: 0.00000019 | TIME: 0:43:21 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1160/1466 | LOSS: 0.18020 (0.07507) | LR: 0.00000015 | TIME: 0:44:39 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1200/1466 | LOSS: 0.13368 (0.07488) | LR: 0.00000011 | TIME: 0:46:17 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1240/1466 | LOSS: 0.02526 (0.07495) | LR: 0.00000008 | TIME: 0:47:58 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1280/1466 | LOSS: 0.10708 (0.07500) | LR: 0.00000006 | TIME: 0:49:42 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1320/1466 | LOSS: 0.07076 (0.07494) | LR: 0.00000003 | TIME: 0:51:03 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1360/1466 | LOSS: 0.08365 (0.07463) | LR: 0.00000002 | TIME: 0:52:29 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1400/1466 | LOSS: 0.12823 (0.07457) | LR: 0.00000001 | TIME: 0:53:47 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1440/1466 | LOSS: 0.06354 (0.07440) | LR: 0.00000000 | TIME: 0:55:14 |
[TRAIN F2] EPOCH: 4/4 | STEP: 1465/1466 | LOSS: 0.07109 (0.07435) | LR: 0.00000000 | TIME: 0:56:15 |

VALID_LOOP
[VALID F2] EPOCH: 4/4 | STEP: 000/489 | LOSS: 0.18783 (0.18783) | TIME: 0:00:01 |
[VALID F2] EPOCH: 4/4 | STEP: 040/489 | LOSS: 0.10032 (0.10138) | TIME: 0:00:39 |
[VALID F2] EPOCH: 4/4 | STEP: 080/489 | LOSS: 0.08886 (0.11069) | TIME: 0:01:16 |
[VALID F2] EPOCH: 4/4 | STEP: 120/489 | LOSS: 0.06675 (0.10593) | TIME: 0:01:53 |
[VALID F2] EPOCH: 4/4 | STEP: 160/489 | LOSS: 0.03569 (0.10461) | TIME: 0:02:31 |
[VALID F2] EPOCH: 4/4 | STEP: 200/489 | LOSS: 0.11149 (0.10448) | TIME: 0:03:08 |
[VALID F2] EPOCH: 4/4 | STEP: 240/489 | LOSS: 0.44785 (0.10570) | TIME: 0:03:45 |
[VALID F2] EPOCH: 4/4 | STEP: 280/489 | LOSS: 0.19597 (0.10583) | TIME: 0:04:23 |
[VALID F2] EPOCH: 4/4 | STEP: 320/489 | LOSS: 0.05410 (0.10338) | TIME: 0:05:00 |
[VALID F2] EPOCH: 4/4 | STEP: 360/489 | LOSS: 0.19543 (0.10213) | TIME: 0:05:37 |
[VALID F2] EPOCH: 4/4 | STEP: 400/489 | LOSS: 0.11494 (0.10370) | TIME: 0:06:15 |
[VALID F2] EPOCH: 4/4 | STEP: 440/489 | LOSS: 0.04362 (0.10409) | TIME: 0:06:52 |
[VALID F2] EPOCH: 4/4 | STEP: 480/489 | LOSS: 0.15031 (0.10425) | TIME: 0:07:29 |
[VALID F2] EPOCH: 4/4 | STEP: 488/489 | LOSS: 0.08082 (0.10376) | TIME: 0:07:37 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.07435 |      0.10376 |  0.45658 | 0.478 | 0.449 | 0.413 | 0.468 | 0.477 | 0.455 | 1:03:53 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4565786123275757


----------------------------------- FOLD 2 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45658      0.4784   0.44885       0.41284        0.46768    0.47696        0.45474

################################### END OF FOlD 2 ###################################


Date: 2022-11-16 14:55:13.290436+07:00 (GMT+7)
Mode: EXPERIMENTING_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 5864}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 1/4 | STEP: 0000/1466 | LOSS: 2.78287 (2.78287) | LR: 0.00000005 | TIME: 0:00:01 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0040/1466 | LOSS: 1.81067 (2.39187) | LR: 0.00000224 | TIME: 0:00:39 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0080/1466 | LOSS: 0.40343 (1.77452) | LR: 0.00000443 | TIME: 0:01:18 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0120/1466 | LOSS: 0.16630 (1.27383) | LR: 0.00000661 | TIME: 0:01:56 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0160/1466 | LOSS: 0.22641 (1.00802) | LR: 0.00000880 | TIME: 0:02:36 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0200/1466 | LOSS: 0.15853 (0.84027) | LR: 0.00001098 | TIME: 0:03:20 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0240/1466 | LOSS: 0.10765 (0.72817) | LR: 0.00001317 | TIME: 0:03:53 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0280/1466 | LOSS: 0.09917 (0.64566) | LR: 0.00001536 | TIME: 0:04:31 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0320/1466 | LOSS: 0.10888 (0.58603) | LR: 0.00001754 | TIME: 0:05:06 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0360/1466 | LOSS: 0.10931 (0.54010) | LR: 0.00001973 | TIME: 0:05:42 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0400/1466 | LOSS: 0.43521 (0.50429) | LR: 0.00002000 | TIME: 0:06:19 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0440/1466 | LOSS: 0.20230 (0.47290) | LR: 0.00001999 | TIME: 0:06:56 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0480/1466 | LOSS: 0.13315 (0.44743) | LR: 0.00001998 | TIME: 0:07:36 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0520/1466 | LOSS: 0.06053 (0.42315) | LR: 0.00001996 | TIME: 0:08:12 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0560/1466 | LOSS: 0.11314 (0.40448) | LR: 0.00001994 | TIME: 0:08:52 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0600/1466 | LOSS: 0.13817 (0.38792) | LR: 0.00001991 | TIME: 0:09:30 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0640/1466 | LOSS: 0.12109 (0.37360) | LR: 0.00001988 | TIME: 0:10:06 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0680/1466 | LOSS: 0.12033 (0.35981) | LR: 0.00001984 | TIME: 0:10:45 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0720/1466 | LOSS: 0.08152 (0.34811) | LR: 0.00001979 | TIME: 0:11:24 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0760/1466 | LOSS: 0.06954 (0.33654) | LR: 0.00001975 | TIME: 0:12:02 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0800/1466 | LOSS: 0.07161 (0.32640) | LR: 0.00001969 | TIME: 0:12:44 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0840/1466 | LOSS: 0.19992 (0.31689) | LR: 0.00001963 | TIME: 0:13:21 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0880/1466 | LOSS: 0.17304 (0.30852) | LR: 0.00001957 | TIME: 0:13:58 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0920/1466 | LOSS: 0.07923 (0.30157) | LR: 0.00001950 | TIME: 0:14:38 |
[TRAIN F3] EPOCH: 1/4 | STEP: 0960/1466 | LOSS: 0.11511 (0.29530) | LR: 0.00001943 | TIME: 0:15:19 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1000/1466 | LOSS: 0.12862 (0.28912) | LR: 0.00001935 | TIME: 0:15:56 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1040/1466 | LOSS: 0.16050 (0.28284) | LR: 0.00001927 | TIME: 0:16:33 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1080/1466 | LOSS: 0.12559 (0.27732) | LR: 0.00001918 | TIME: 0:17:13 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1120/1466 | LOSS: 0.24437 (0.27307) | LR: 0.00001908 | TIME: 0:17:54 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1160/1466 | LOSS: 0.14417 (0.26802) | LR: 0.00001899 | TIME: 0:18:29 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1200/1466 | LOSS: 0.13230 (0.26421) | LR: 0.00001888 | TIME: 0:19:05 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1240/1466 | LOSS: 0.12599 (0.26025) | LR: 0.00001878 | TIME: 0:19:39 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1280/1466 | LOSS: 0.11845 (0.25654) | LR: 0.00001866 | TIME: 0:20:16 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1320/1466 | LOSS: 0.07510 (0.25291) | LR: 0.00001855 | TIME: 0:20:53 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1360/1466 | LOSS: 0.06742 (0.24916) | LR: 0.00001843 | TIME: 0:21:29 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1400/1466 | LOSS: 0.16309 (0.24556) | LR: 0.00001830 | TIME: 0:22:03 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1440/1466 | LOSS: 0.10897 (0.24228) | LR: 0.00001817 | TIME: 0:22:40 |
[TRAIN F3] EPOCH: 1/4 | STEP: 1465/1466 | LOSS: 0.15586 (0.24004) | LR: 0.00001809 | TIME: 0:23:02 |

VALID_LOOP
[VALID F3] EPOCH: 1/4 | STEP: 000/489 | LOSS: 0.24087 (0.24087) | TIME: 0:00:01 |
[VALID F3] EPOCH: 1/4 | STEP: 040/489 | LOSS: 0.16551 (0.11469) | TIME: 0:00:10 |
[VALID F3] EPOCH: 1/4 | STEP: 080/489 | LOSS: 0.13868 (0.11354) | TIME: 0:00:19 |
[VALID F3] EPOCH: 1/4 | STEP: 120/489 | LOSS: 0.13220 (0.10869) | TIME: 0:00:28 |
[VALID F3] EPOCH: 1/4 | STEP: 160/489 | LOSS: 0.10485 (0.11090) | TIME: 0:00:38 |
[VALID F3] EPOCH: 1/4 | STEP: 200/489 | LOSS: 0.05644 (0.10980) | TIME: 0:00:47 |
[VALID F3] EPOCH: 1/4 | STEP: 240/489 | LOSS: 0.08192 (0.10936) | TIME: 0:00:56 |
[VALID F3] EPOCH: 1/4 | STEP: 280/489 | LOSS: 0.07148 (0.11024) | TIME: 0:01:05 |
[VALID F3] EPOCH: 1/4 | STEP: 320/489 | LOSS: 0.14338 (0.11109) | TIME: 0:01:14 |
[VALID F3] EPOCH: 1/4 | STEP: 360/489 | LOSS: 0.08500 (0.11096) | TIME: 0:01:23 |
[VALID F3] EPOCH: 1/4 | STEP: 400/489 | LOSS: 0.06480 (0.11066) | TIME: 0:01:32 |
[VALID F3] EPOCH: 1/4 | STEP: 440/489 | LOSS: 0.08166 (0.11125) | TIME: 0:01:40 |
[VALID F3] EPOCH: 1/4 | STEP: 480/489 | LOSS: 0.18244 (0.11113) | TIME: 0:01:49 |
[VALID F3] EPOCH: 1/4 | STEP: 488/489 | LOSS: 0.02187 (0.11069) | TIME: 0:01:50 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.24004 |      0.11069 |  0.47036 | 0.546 | 0.457 | 0.422 | 0.487 | 0.475 | 0.434 | 0:24:52 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4703574478626251

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 2/4 | STEP: 0000/1466 | LOSS: 0.06783 (0.06783) | LR: 0.00001809 | TIME: 0:00:02 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0040/1466 | LOSS: 0.07394 (0.12972) | LR: 0.00001795 | TIME: 0:00:36 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0080/1466 | LOSS: 0.08422 (0.12646) | LR: 0.00001781 | TIME: 0:01:11 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0120/1466 | LOSS: 0.18373 (0.12284) | LR: 0.00001766 | TIME: 0:01:46 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0160/1466 | LOSS: 0.08887 (0.12108) | LR: 0.00001751 | TIME: 0:02:22 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0200/1466 | LOSS: 0.04262 (0.11949) | LR: 0.00001736 | TIME: 0:02:59 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0240/1466 | LOSS: 0.13302 (0.11827) | LR: 0.00001721 | TIME: 0:03:38 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0280/1466 | LOSS: 0.11234 (0.11720) | LR: 0.00001704 | TIME: 0:04:17 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0320/1466 | LOSS: 0.27595 (0.11857) | LR: 0.00001688 | TIME: 0:04:54 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0360/1466 | LOSS: 0.07585 (0.11684) | LR: 0.00001671 | TIME: 0:05:36 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0400/1466 | LOSS: 0.05012 (0.11704) | LR: 0.00001654 | TIME: 0:06:15 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0440/1466 | LOSS: 0.05206 (0.11668) | LR: 0.00001637 | TIME: 0:06:51 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0480/1466 | LOSS: 0.13872 (0.11654) | LR: 0.00001619 | TIME: 0:07:30 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0520/1466 | LOSS: 0.05362 (0.11548) | LR: 0.00001601 | TIME: 0:08:08 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0560/1466 | LOSS: 0.09046 (0.11569) | LR: 0.00001582 | TIME: 0:08:46 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0600/1466 | LOSS: 0.23344 (0.11650) | LR: 0.00001564 | TIME: 0:09:23 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0640/1466 | LOSS: 0.08572 (0.11637) | LR: 0.00001545 | TIME: 0:09:58 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0680/1466 | LOSS: 0.10039 (0.11719) | LR: 0.00001525 | TIME: 0:10:38 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0720/1466 | LOSS: 0.16837 (0.11709) | LR: 0.00001506 | TIME: 0:11:16 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0760/1466 | LOSS: 0.08806 (0.11743) | LR: 0.00001486 | TIME: 0:11:52 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0800/1466 | LOSS: 0.11888 (0.11677) | LR: 0.00001466 | TIME: 0:12:29 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0840/1466 | LOSS: 0.12222 (0.11632) | LR: 0.00001445 | TIME: 0:13:06 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0880/1466 | LOSS: 0.07626 (0.11586) | LR: 0.00001425 | TIME: 0:13:39 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0920/1466 | LOSS: 0.07903 (0.11543) | LR: 0.00001404 | TIME: 0:14:18 |
[TRAIN F3] EPOCH: 2/4 | STEP: 0960/1466 | LOSS: 0.06884 (0.11502) | LR: 0.00001383 | TIME: 0:14:53 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1000/1466 | LOSS: 0.11177 (0.11500) | LR: 0.00001362 | TIME: 0:15:31 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1040/1466 | LOSS: 0.17273 (0.11516) | LR: 0.00001340 | TIME: 0:16:09 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1080/1466 | LOSS: 0.03760 (0.11504) | LR: 0.00001319 | TIME: 0:16:45 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1120/1466 | LOSS: 0.06553 (0.11511) | LR: 0.00001297 | TIME: 0:17:26 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1160/1466 | LOSS: 0.15140 (0.11550) | LR: 0.00001275 | TIME: 0:18:01 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1200/1466 | LOSS: 0.14395 (0.11501) | LR: 0.00001253 | TIME: 0:18:36 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1240/1466 | LOSS: 0.11133 (0.11531) | LR: 0.00001231 | TIME: 0:19:11 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1280/1466 | LOSS: 0.15594 (0.11537) | LR: 0.00001209 | TIME: 0:19:47 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1320/1466 | LOSS: 0.07013 (0.11528) | LR: 0.00001186 | TIME: 0:20:24 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1360/1466 | LOSS: 0.17517 (0.11505) | LR: 0.00001164 | TIME: 0:21:01 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1400/1466 | LOSS: 0.08204 (0.11461) | LR: 0.00001141 | TIME: 0:21:37 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1440/1466 | LOSS: 0.06517 (0.11405) | LR: 0.00001119 | TIME: 0:22:17 |
[TRAIN F3] EPOCH: 2/4 | STEP: 1465/1466 | LOSS: 0.04637 (0.11385) | LR: 0.00001104 | TIME: 0:22:38 |

VALID_LOOP
[VALID F3] EPOCH: 2/4 | STEP: 000/489 | LOSS: 0.22118 (0.22118) | TIME: 0:00:01 |
[VALID F3] EPOCH: 2/4 | STEP: 040/489 | LOSS: 0.16786 (0.10482) | TIME: 0:00:09 |
[VALID F3] EPOCH: 2/4 | STEP: 080/489 | LOSS: 0.13213 (0.10613) | TIME: 0:00:18 |
[VALID F3] EPOCH: 2/4 | STEP: 120/489 | LOSS: 0.13278 (0.10267) | TIME: 0:00:26 |
[VALID F3] EPOCH: 2/4 | STEP: 160/489 | LOSS: 0.11775 (0.10465) | TIME: 0:00:35 |
[VALID F3] EPOCH: 2/4 | STEP: 200/489 | LOSS: 0.02444 (0.10315) | TIME: 0:00:43 |
[VALID F3] EPOCH: 2/4 | STEP: 240/489 | LOSS: 0.06751 (0.10307) | TIME: 0:00:52 |
[VALID F3] EPOCH: 2/4 | STEP: 280/489 | LOSS: 0.03803 (0.10358) | TIME: 0:01:00 |
[VALID F3] EPOCH: 2/4 | STEP: 320/489 | LOSS: 0.17377 (0.10432) | TIME: 0:01:09 |
[VALID F3] EPOCH: 2/4 | STEP: 360/489 | LOSS: 0.09587 (0.10467) | TIME: 0:01:18 |
[VALID F3] EPOCH: 2/4 | STEP: 400/489 | LOSS: 0.07857 (0.10446) | TIME: 0:01:26 |
[VALID F3] EPOCH: 2/4 | STEP: 440/489 | LOSS: 0.09924 (0.10495) | TIME: 0:01:35 |
[VALID F3] EPOCH: 2/4 | STEP: 480/489 | LOSS: 0.13751 (0.10494) | TIME: 0:01:43 |
[VALID F3] EPOCH: 2/4 | STEP: 488/489 | LOSS: 0.04877 (0.10455) | TIME: 0:01:45 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11385 |      0.10455 |  0.45792 | 0.492 | 0.458 | 0.416 | 0.448 | 0.474 | 0.460 | 0:24:23 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4579158127307892

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 3/4 | STEP: 0000/1466 | LOSS: 0.21182 (0.21182) | LR: 0.00001104 | TIME: 0:00:02 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0040/1466 | LOSS: 0.11049 (0.09260) | LR: 0.00001081 | TIME: 0:00:34 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0080/1466 | LOSS: 0.11917 (0.09541) | LR: 0.00001058 | TIME: 0:01:06 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0120/1466 | LOSS: 0.09603 (0.09527) | LR: 0.00001035 | TIME: 0:01:45 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0160/1466 | LOSS: 0.08953 (0.09512) | LR: 0.00001013 | TIME: 0:02:21 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0200/1466 | LOSS: 0.04212 (0.09328) | LR: 0.00000990 | TIME: 0:02:57 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0240/1466 | LOSS: 0.11092 (0.09386) | LR: 0.00000967 | TIME: 0:03:30 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0280/1466 | LOSS: 0.07715 (0.09456) | LR: 0.00000944 | TIME: 0:04:03 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0320/1466 | LOSS: 0.05008 (0.09428) | LR: 0.00000921 | TIME: 0:04:38 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0360/1466 | LOSS: 0.09751 (0.09349) | LR: 0.00000898 | TIME: 0:05:12 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0400/1466 | LOSS: 0.15079 (0.09328) | LR: 0.00000876 | TIME: 0:05:48 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0440/1466 | LOSS: 0.08569 (0.09297) | LR: 0.00000853 | TIME: 0:06:23 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0480/1466 | LOSS: 0.08786 (0.09334) | LR: 0.00000831 | TIME: 0:07:01 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0520/1466 | LOSS: 0.09078 (0.09278) | LR: 0.00000808 | TIME: 0:07:31 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0560/1466 | LOSS: 0.15641 (0.09208) | LR: 0.00000786 | TIME: 0:08:04 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0600/1466 | LOSS: 0.10450 (0.09125) | LR: 0.00000763 | TIME: 0:08:37 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0640/1466 | LOSS: 0.11294 (0.09138) | LR: 0.00000741 | TIME: 0:09:08 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0680/1466 | LOSS: 0.02578 (0.09148) | LR: 0.00000719 | TIME: 0:09:40 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0720/1466 | LOSS: 0.07538 (0.09126) | LR: 0.00000697 | TIME: 0:10:13 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0760/1466 | LOSS: 0.05470 (0.09089) | LR: 0.00000676 | TIME: 0:10:51 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0800/1466 | LOSS: 0.08483 (0.09100) | LR: 0.00000654 | TIME: 0:11:26 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0840/1466 | LOSS: 0.08731 (0.09099) | LR: 0.00000633 | TIME: 0:12:05 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0880/1466 | LOSS: 0.04985 (0.09117) | LR: 0.00000612 | TIME: 0:12:43 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0920/1466 | LOSS: 0.07106 (0.09106) | LR: 0.00000591 | TIME: 0:13:16 |
[TRAIN F3] EPOCH: 3/4 | STEP: 0960/1466 | LOSS: 0.12847 (0.09067) | LR: 0.00000570 | TIME: 0:13:55 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1000/1466 | LOSS: 0.07298 (0.09073) | LR: 0.00000549 | TIME: 0:14:30 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1040/1466 | LOSS: 0.03200 (0.09093) | LR: 0.00000529 | TIME: 0:15:08 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1080/1466 | LOSS: 0.07035 (0.09083) | LR: 0.00000509 | TIME: 0:15:42 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1120/1466 | LOSS: 0.16053 (0.09089) | LR: 0.00000489 | TIME: 0:16:22 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1160/1466 | LOSS: 0.03492 (0.09086) | LR: 0.00000470 | TIME: 0:17:01 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1200/1466 | LOSS: 0.05336 (0.09065) | LR: 0.00000451 | TIME: 0:17:37 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1240/1466 | LOSS: 0.05922 (0.09097) | LR: 0.00000432 | TIME: 0:18:18 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1280/1466 | LOSS: 0.10156 (0.09056) | LR: 0.00000413 | TIME: 0:18:53 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1320/1466 | LOSS: 0.06562 (0.09079) | LR: 0.00000395 | TIME: 0:19:34 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1360/1466 | LOSS: 0.02180 (0.09094) | LR: 0.00000377 | TIME: 0:20:13 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1400/1466 | LOSS: 0.10128 (0.09057) | LR: 0.00000359 | TIME: 0:20:53 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1440/1466 | LOSS: 0.08234 (0.09058) | LR: 0.00000341 | TIME: 0:21:30 |
[TRAIN F3] EPOCH: 3/4 | STEP: 1465/1466 | LOSS: 0.11887 (0.09075) | LR: 0.00000331 | TIME: 0:21:51 |

VALID_LOOP
[VALID F3] EPOCH: 3/4 | STEP: 000/489 | LOSS: 0.18778 (0.18778) | TIME: 0:00:01 |
[VALID F3] EPOCH: 3/4 | STEP: 040/489 | LOSS: 0.11880 (0.09724) | TIME: 0:00:10 |
[VALID F3] EPOCH: 3/4 | STEP: 080/489 | LOSS: 0.09605 (0.09961) | TIME: 0:00:18 |
[VALID F3] EPOCH: 3/4 | STEP: 120/489 | LOSS: 0.12157 (0.09846) | TIME: 0:00:27 |
[VALID F3] EPOCH: 3/4 | STEP: 160/489 | LOSS: 0.12245 (0.09982) | TIME: 0:00:36 |
[VALID F3] EPOCH: 3/4 | STEP: 200/489 | LOSS: 0.03250 (0.09934) | TIME: 0:00:45 |
[VALID F3] EPOCH: 3/4 | STEP: 240/489 | LOSS: 0.07441 (0.09935) | TIME: 0:00:54 |
[VALID F3] EPOCH: 3/4 | STEP: 280/489 | LOSS: 0.04352 (0.09977) | TIME: 0:01:03 |
[VALID F3] EPOCH: 3/4 | STEP: 320/489 | LOSS: 0.19310 (0.10041) | TIME: 0:01:12 |
[VALID F3] EPOCH: 3/4 | STEP: 360/489 | LOSS: 0.06998 (0.10098) | TIME: 0:01:21 |
[VALID F3] EPOCH: 3/4 | STEP: 400/489 | LOSS: 0.06163 (0.10068) | TIME: 0:01:29 |
[VALID F3] EPOCH: 3/4 | STEP: 440/489 | LOSS: 0.08877 (0.10120) | TIME: 0:01:38 |
[VALID F3] EPOCH: 3/4 | STEP: 480/489 | LOSS: 0.11461 (0.10137) | TIME: 0:01:47 |
[VALID F3] EPOCH: 3/4 | STEP: 488/489 | LOSS: 0.04753 (0.10095) | TIME: 0:01:49 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.09075 |      0.10095 |  0.44987 | 0.484 | 0.450 | 0.414 | 0.444 | 0.475 | 0.433 | 0:23:40 |


[SAVED] EPOCH: 3 | MCRMSE: 0.44986870884895325

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 4/4 | STEP: 0000/1466 | LOSS: 0.12047 (0.12047) | LR: 0.00000330 | TIME: 0:00:02 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0040/1466 | LOSS: 0.07844 (0.07753) | LR: 0.00000314 | TIME: 0:00:36 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0080/1466 | LOSS: 0.13769 (0.08110) | LR: 0.00000297 | TIME: 0:01:13 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0120/1466 | LOSS: 0.02991 (0.07718) | LR: 0.00000281 | TIME: 0:01:53 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0160/1466 | LOSS: 0.06712 (0.07465) | LR: 0.00000265 | TIME: 0:02:26 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0200/1466 | LOSS: 0.09113 (0.07461) | LR: 0.00000250 | TIME: 0:03:05 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0240/1466 | LOSS: 0.05525 (0.07439) | LR: 0.00000235 | TIME: 0:03:40 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0280/1466 | LOSS: 0.06887 (0.07502) | LR: 0.00000221 | TIME: 0:04:16 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0320/1466 | LOSS: 0.09565 (0.07559) | LR: 0.00000207 | TIME: 0:04:55 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0360/1466 | LOSS: 0.05543 (0.07611) | LR: 0.00000193 | TIME: 0:05:34 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0400/1466 | LOSS: 0.08935 (0.07600) | LR: 0.00000180 | TIME: 0:06:16 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0440/1466 | LOSS: 0.02965 (0.07494) | LR: 0.00000167 | TIME: 0:06:48 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0480/1466 | LOSS: 0.03816 (0.07423) | LR: 0.00000154 | TIME: 0:07:24 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0520/1466 | LOSS: 0.09683 (0.07434) | LR: 0.00000142 | TIME: 0:08:06 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0560/1466 | LOSS: 0.04539 (0.07422) | LR: 0.00000131 | TIME: 0:08:38 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0600/1466 | LOSS: 0.06040 (0.07434) | LR: 0.00000120 | TIME: 0:09:13 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0640/1466 | LOSS: 0.03628 (0.07405) | LR: 0.00000109 | TIME: 0:09:50 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0680/1466 | LOSS: 0.06404 (0.07495) | LR: 0.00000099 | TIME: 0:10:25 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0720/1466 | LOSS: 0.06491 (0.07438) | LR: 0.00000089 | TIME: 0:11:03 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0760/1466 | LOSS: 0.04491 (0.07441) | LR: 0.00000080 | TIME: 0:11:38 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0800/1466 | LOSS: 0.08600 (0.07474) | LR: 0.00000071 | TIME: 0:12:11 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0840/1466 | LOSS: 0.02441 (0.07474) | LR: 0.00000063 | TIME: 0:12:42 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0880/1466 | LOSS: 0.07581 (0.07481) | LR: 0.00000055 | TIME: 0:13:19 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0920/1466 | LOSS: 0.01953 (0.07479) | LR: 0.00000048 | TIME: 0:13:57 |
[TRAIN F3] EPOCH: 4/4 | STEP: 0960/1466 | LOSS: 0.02329 (0.07402) | LR: 0.00000041 | TIME: 0:14:34 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1000/1466 | LOSS: 0.07110 (0.07378) | LR: 0.00000035 | TIME: 0:15:08 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1040/1466 | LOSS: 0.08938 (0.07372) | LR: 0.00000029 | TIME: 0:15:44 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1080/1466 | LOSS: 0.02200 (0.07370) | LR: 0.00000024 | TIME: 0:16:16 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1120/1466 | LOSS: 0.06038 (0.07381) | LR: 0.00000019 | TIME: 0:16:55 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1160/1466 | LOSS: 0.04468 (0.07412) | LR: 0.00000015 | TIME: 0:17:30 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1200/1466 | LOSS: 0.01481 (0.07401) | LR: 0.00000011 | TIME: 0:18:10 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1240/1466 | LOSS: 0.07410 (0.07419) | LR: 0.00000008 | TIME: 0:18:48 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1280/1466 | LOSS: 0.01539 (0.07399) | LR: 0.00000006 | TIME: 0:19:22 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1320/1466 | LOSS: 0.05548 (0.07425) | LR: 0.00000003 | TIME: 0:19:56 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1360/1466 | LOSS: 0.11853 (0.07421) | LR: 0.00000002 | TIME: 0:20:29 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1400/1466 | LOSS: 0.08475 (0.07408) | LR: 0.00000001 | TIME: 0:21:05 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1440/1466 | LOSS: 0.08862 (0.07388) | LR: 0.00000000 | TIME: 0:21:43 |
[TRAIN F3] EPOCH: 4/4 | STEP: 1465/1466 | LOSS: 0.03049 (0.07365) | LR: 0.00000000 | TIME: 0:22:04 |

VALID_LOOP
[VALID F3] EPOCH: 4/4 | STEP: 000/489 | LOSS: 0.17663 (0.17663) | TIME: 0:00:01 |
[VALID F3] EPOCH: 4/4 | STEP: 040/489 | LOSS: 0.10946 (0.09605) | TIME: 0:00:09 |
[VALID F3] EPOCH: 4/4 | STEP: 080/489 | LOSS: 0.08566 (0.09894) | TIME: 0:00:18 |
[VALID F3] EPOCH: 4/4 | STEP: 120/489 | LOSS: 0.12153 (0.09876) | TIME: 0:00:25 |
[VALID F3] EPOCH: 4/4 | STEP: 160/489 | LOSS: 0.10910 (0.10040) | TIME: 0:00:33 |
[VALID F3] EPOCH: 4/4 | STEP: 200/489 | LOSS: 0.03249 (0.09993) | TIME: 0:00:41 |
[VALID F3] EPOCH: 4/4 | STEP: 240/489 | LOSS: 0.07252 (0.09972) | TIME: 0:00:49 |
[VALID F3] EPOCH: 4/4 | STEP: 280/489 | LOSS: 0.04466 (0.10009) | TIME: 0:00:56 |
[VALID F3] EPOCH: 4/4 | STEP: 320/489 | LOSS: 0.20552 (0.10079) | TIME: 0:01:04 |
[VALID F3] EPOCH: 4/4 | STEP: 360/489 | LOSS: 0.06740 (0.10133) | TIME: 0:01:12 |
[VALID F3] EPOCH: 4/4 | STEP: 400/489 | LOSS: 0.05672 (0.10104) | TIME: 0:01:20 |
[VALID F3] EPOCH: 4/4 | STEP: 440/489 | LOSS: 0.08515 (0.10157) | TIME: 0:01:27 |
[VALID F3] EPOCH: 4/4 | STEP: 480/489 | LOSS: 0.10826 (0.10192) | TIME: 0:01:35 |
[VALID F3] EPOCH: 4/4 | STEP: 488/489 | LOSS: 0.04620 (0.10149) | TIME: 0:01:37 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.07365 |      0.10149 |  0.45107 | 0.487 | 0.452 | 0.414 | 0.445 | 0.474 | 0.436 | 0:23:41 |


----------------------------------- FOLD 3 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.44987     0.48361   0.44958       0.41362        0.44395    0.47548        0.43297

################################### END OF FOlD 3 ###################################

