Date: 2022-11-28 19:17:34.771940+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 4398}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 768

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 1/3 | STEP: 0000/1466 | LOSS: 0.08621 (0.08621) | LR: 0.00000005 | TIME: 0:00:01 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0040/1466 | LOSS: 0.05444 (0.13308) | LR: 0.00000224 | TIME: 0:00:33 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0080/1466 | LOSS: 0.09506 (0.13923) | LR: 0.00000443 | TIME: 0:01:04 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0120/1466 | LOSS: 0.07774 (0.12963) | LR: 0.00000661 | TIME: 0:01:36 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0160/1466 | LOSS: 0.09532 (0.13052) | LR: 0.00000880 | TIME: 0:02:09 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0200/1466 | LOSS: 0.10915 (0.12971) | LR: 0.00001098 | TIME: 0:02:41 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0240/1466 | LOSS: 0.13330 (0.12871) | LR: 0.00001317 | TIME: 0:03:12 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0280/1466 | LOSS: 0.13103 (0.12784) | LR: 0.00001536 | TIME: 0:03:41 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0320/1466 | LOSS: 0.05172 (0.12634) | LR: 0.00001754 | TIME: 0:04:10 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0360/1466 | LOSS: 0.12192 (0.12457) | LR: 0.00001973 | TIME: 0:04:41 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0400/1466 | LOSS: 0.15942 (0.12712) | LR: 0.00002000 | TIME: 0:05:12 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0440/1466 | LOSS: 0.10566 (0.12703) | LR: 0.00001998 | TIME: 0:05:43 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0480/1466 | LOSS: 0.16324 (0.12916) | LR: 0.00001996 | TIME: 0:06:15 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0520/1466 | LOSS: 0.31999 (0.12950) | LR: 0.00001993 | TIME: 0:06:47 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0560/1466 | LOSS: 0.20152 (0.12969) | LR: 0.00001988 | TIME: 0:07:21 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0600/1466 | LOSS: 0.14711 (0.12876) | LR: 0.00001983 | TIME: 0:07:54 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0640/1466 | LOSS: 0.10222 (0.12899) | LR: 0.00001977 | TIME: 0:08:25 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0680/1466 | LOSS: 0.14893 (0.12898) | LR: 0.00001970 | TIME: 0:08:57 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0720/1466 | LOSS: 0.13433 (0.12982) | LR: 0.00001962 | TIME: 0:09:26 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0760/1466 | LOSS: 0.08895 (0.12936) | LR: 0.00001953 | TIME: 0:09:57 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0800/1466 | LOSS: 0.09486 (0.12953) | LR: 0.00001943 | TIME: 0:10:29 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0840/1466 | LOSS: 0.16726 (0.13005) | LR: 0.00001932 | TIME: 0:11:02 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0880/1466 | LOSS: 0.09496 (0.12974) | LR: 0.00001921 | TIME: 0:11:35 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0920/1466 | LOSS: 0.20256 (0.13013) | LR: 0.00001908 | TIME: 0:12:07 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0960/1466 | LOSS: 0.02437 (0.12959) | LR: 0.00001894 | TIME: 0:12:38 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1000/1466 | LOSS: 0.06816 (0.12954) | LR: 0.00001880 | TIME: 0:13:11 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1040/1466 | LOSS: 0.08595 (0.12869) | LR: 0.00001865 | TIME: 0:13:42 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1080/1466 | LOSS: 0.07801 (0.12905) | LR: 0.00001849 | TIME: 0:14:13 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1120/1466 | LOSS: 0.07859 (0.12890) | LR: 0.00001832 | TIME: 0:14:44 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1160/1466 | LOSS: 0.13794 (0.12885) | LR: 0.00001814 | TIME: 0:15:15 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1200/1466 | LOSS: 0.15238 (0.12862) | LR: 0.00001796 | TIME: 0:15:45 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1240/1466 | LOSS: 0.12626 (0.12857) | LR: 0.00001776 | TIME: 0:16:16 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1280/1466 | LOSS: 0.13435 (0.12800) | LR: 0.00001756 | TIME: 0:16:48 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1320/1466 | LOSS: 0.25500 (0.12796) | LR: 0.00001736 | TIME: 0:17:18 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1360/1466 | LOSS: 0.23546 (0.12773) | LR: 0.00001714 | TIME: 0:17:48 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1400/1466 | LOSS: 0.09825 (0.12703) | LR: 0.00001692 | TIME: 0:18:19 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1440/1466 | LOSS: 0.24492 (0.12655) | LR: 0.00001669 | TIME: 0:18:51 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1465/1466 | LOSS: 0.17355 (0.12623) | LR: 0.00001655 | TIME: 0:19:12 |

VALID_LOOP
[VALID F0] EPOCH: 1/3 | STEP: 000/489 | LOSS: 0.15415 (0.15415) | TIME: 0:00:01 |
[VALID F0] EPOCH: 1/3 | STEP: 040/489 | LOSS: 0.07195 (0.13352) | TIME: 0:00:05 |
[VALID F0] EPOCH: 1/3 | STEP: 080/489 | LOSS: 0.08836 (0.14071) | TIME: 0:00:10 |
[VALID F0] EPOCH: 1/3 | STEP: 120/489 | LOSS: 0.20362 (0.13914) | TIME: 0:00:15 |
[VALID F0] EPOCH: 1/3 | STEP: 160/489 | LOSS: 0.09188 (0.13993) | TIME: 0:00:20 |
[VALID F0] EPOCH: 1/3 | STEP: 200/489 | LOSS: 0.22788 (0.14271) | TIME: 0:00:24 |
[VALID F0] EPOCH: 1/3 | STEP: 240/489 | LOSS: 0.13664 (0.13857) | TIME: 0:00:29 |
[VALID F0] EPOCH: 1/3 | STEP: 280/489 | LOSS: 0.06820 (0.13795) | TIME: 0:00:34 |
[VALID F0] EPOCH: 1/3 | STEP: 320/489 | LOSS: 0.04120 (0.13734) | TIME: 0:00:39 |
[VALID F0] EPOCH: 1/3 | STEP: 360/489 | LOSS: 0.20519 (0.13581) | TIME: 0:00:44 |
[VALID F0] EPOCH: 1/3 | STEP: 400/489 | LOSS: 0.16583 (0.13458) | TIME: 0:00:49 |
[VALID F0] EPOCH: 1/3 | STEP: 440/489 | LOSS: 0.20292 (0.13481) | TIME: 0:00:54 |
[VALID F0] EPOCH: 1/3 | STEP: 480/489 | LOSS: 0.08723 (0.13651) | TIME: 0:00:58 |
[VALID F0] EPOCH: 1/3 | STEP: 488/489 | LOSS: 0.09075 (0.13662) | TIME: 0:00:59 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |      0.12623 |      0.13662 |  0.52252 | 0.617 | 0.447 | 0.460 | 0.603 | 0.487 | 0.521 | 0:20:11 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5225169658660889

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 2/3 | STEP: 0000/1466 | LOSS: 0.13377 (0.13377) | LR: 0.00001654 | TIME: 0:00:01 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0040/1466 | LOSS: 0.08117 (0.11561) | LR: 0.00001630 | TIME: 0:00:35 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0080/1466 | LOSS: 0.06453 (0.10381) | LR: 0.00001606 | TIME: 0:01:05 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0120/1466 | LOSS: 0.06638 (0.11229) | LR: 0.00001581 | TIME: 0:01:37 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0160/1466 | LOSS: 0.17981 (0.11060) | LR: 0.00001555 | TIME: 0:02:09 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0200/1466 | LOSS: 0.10653 (0.10835) | LR: 0.00001529 | TIME: 0:02:39 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0240/1466 | LOSS: 0.07040 (0.10866) | LR: 0.00001502 | TIME: 0:03:10 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0280/1466 | LOSS: 0.08255 (0.10822) | LR: 0.00001475 | TIME: 0:03:42 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0320/1466 | LOSS: 0.05298 (0.10888) | LR: 0.00001447 | TIME: 0:04:12 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0360/1466 | LOSS: 0.05233 (0.10832) | LR: 0.00001419 | TIME: 0:04:41 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0400/1466 | LOSS: 0.10779 (0.10813) | LR: 0.00001391 | TIME: 0:05:11 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0440/1466 | LOSS: 0.07298 (0.10686) | LR: 0.00001362 | TIME: 0:05:42 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0480/1466 | LOSS: 0.11527 (0.10647) | LR: 0.00001332 | TIME: 0:06:14 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0520/1466 | LOSS: 0.08356 (0.10566) | LR: 0.00001303 | TIME: 0:06:44 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0560/1466 | LOSS: 0.18721 (0.10615) | LR: 0.00001273 | TIME: 0:07:16 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0600/1466 | LOSS: 0.17381 (0.10567) | LR: 0.00001243 | TIME: 0:07:48 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0640/1466 | LOSS: 0.10698 (0.10461) | LR: 0.00001213 | TIME: 0:08:22 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0680/1466 | LOSS: 0.06864 (0.10467) | LR: 0.00001182 | TIME: 0:08:54 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0720/1466 | LOSS: 0.10774 (0.10445) | LR: 0.00001151 | TIME: 0:09:27 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0760/1466 | LOSS: 0.08633 (0.10427) | LR: 0.00001120 | TIME: 0:10:02 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0800/1466 | LOSS: 0.09647 (0.10443) | LR: 0.00001089 | TIME: 0:10:36 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0840/1466 | LOSS: 0.06858 (0.10393) | LR: 0.00001058 | TIME: 0:11:09 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0880/1466 | LOSS: 0.08219 (0.10444) | LR: 0.00001027 | TIME: 0:11:45 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0920/1466 | LOSS: 0.06181 (0.10417) | LR: 0.00000996 | TIME: 0:12:20 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0960/1466 | LOSS: 0.10806 (0.10387) | LR: 0.00000965 | TIME: 0:12:53 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1000/1466 | LOSS: 0.10989 (0.10394) | LR: 0.00000934 | TIME: 0:13:27 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1040/1466 | LOSS: 0.13175 (0.10361) | LR: 0.00000903 | TIME: 0:14:04 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1080/1466 | LOSS: 0.12804 (0.10342) | LR: 0.00000872 | TIME: 0:14:39 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1120/1466 | LOSS: 0.07019 (0.10277) | LR: 0.00000841 | TIME: 0:15:15 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1160/1466 | LOSS: 0.09008 (0.10238) | LR: 0.00000810 | TIME: 0:15:48 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1200/1466 | LOSS: 0.12648 (0.10310) | LR: 0.00000780 | TIME: 0:16:24 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1240/1466 | LOSS: 0.14419 (0.10294) | LR: 0.00000749 | TIME: 0:16:58 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1280/1466 | LOSS: 0.07822 (0.10319) | LR: 0.00000719 | TIME: 0:17:32 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1320/1466 | LOSS: 0.13124 (0.10294) | LR: 0.00000690 | TIME: 0:18:07 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1360/1466 | LOSS: 0.13402 (0.10281) | LR: 0.00000660 | TIME: 0:18:42 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1400/1466 | LOSS: 0.09409 (0.10263) | LR: 0.00000631 | TIME: 0:19:14 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1440/1466 | LOSS: 0.25256 (0.10232) | LR: 0.00000602 | TIME: 0:19:48 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1465/1466 | LOSS: 0.06664 (0.10247) | LR: 0.00000584 | TIME: 0:20:10 |

VALID_LOOP
[VALID F0] EPOCH: 2/3 | STEP: 000/489 | LOSS: 0.06903 (0.06903) | TIME: 0:00:01 |
[VALID F0] EPOCH: 2/3 | STEP: 040/489 | LOSS: 0.07776 (0.10864) | TIME: 0:00:07 |
[VALID F0] EPOCH: 2/3 | STEP: 080/489 | LOSS: 0.06025 (0.11219) | TIME: 0:00:13 |
[VALID F0] EPOCH: 2/3 | STEP: 120/489 | LOSS: 0.17738 (0.11277) | TIME: 0:00:19 |
[VALID F0] EPOCH: 2/3 | STEP: 160/489 | LOSS: 0.07690 (0.11432) | TIME: 0:00:26 |
[VALID F0] EPOCH: 2/3 | STEP: 200/489 | LOSS: 0.21173 (0.11630) | TIME: 0:00:32 |
[VALID F0] EPOCH: 2/3 | STEP: 240/489 | LOSS: 0.07038 (0.11445) | TIME: 0:00:38 |
[VALID F0] EPOCH: 2/3 | STEP: 280/489 | LOSS: 0.06832 (0.11409) | TIME: 0:00:45 |
[VALID F0] EPOCH: 2/3 | STEP: 320/489 | LOSS: 0.05528 (0.11296) | TIME: 0:00:51 |
[VALID F0] EPOCH: 2/3 | STEP: 360/489 | LOSS: 0.12646 (0.11177) | TIME: 0:00:57 |
[VALID F0] EPOCH: 2/3 | STEP: 400/489 | LOSS: 0.14121 (0.11111) | TIME: 0:01:04 |
[VALID F0] EPOCH: 2/3 | STEP: 440/489 | LOSS: 0.16772 (0.11121) | TIME: 0:01:10 |
[VALID F0] EPOCH: 2/3 | STEP: 480/489 | LOSS: 0.06098 (0.11301) | TIME: 0:01:17 |
[VALID F0] EPOCH: 2/3 | STEP: 488/489 | LOSS: 0.07207 (0.11304) | TIME: 0:01:18 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10247 |      0.11304 |  0.47676 | 0.533 | 0.444 | 0.466 | 0.473 | 0.474 | 0.471 | 0:21:28 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4767592251300812

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 3/3 | STEP: 0000/1466 | LOSS: 0.08717 (0.08717) | LR: 0.00000584 | TIME: 0:00:01 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0040/1466 | LOSS: 0.03740 (0.08461) | LR: 0.00000556 | TIME: 0:00:35 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0080/1466 | LOSS: 0.05881 (0.08398) | LR: 0.00000528 | TIME: 0:01:08 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0120/1466 | LOSS: 0.06078 (0.08333) | LR: 0.00000501 | TIME: 0:01:38 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0160/1466 | LOSS: 0.05087 (0.08064) | LR: 0.00000474 | TIME: 0:02:11 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0200/1466 | LOSS: 0.12639 (0.07941) | LR: 0.00000448 | TIME: 0:02:44 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0240/1466 | LOSS: 0.07498 (0.08212) | LR: 0.00000422 | TIME: 0:03:17 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0280/1466 | LOSS: 0.06691 (0.08097) | LR: 0.00000397 | TIME: 0:03:51 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0320/1466 | LOSS: 0.03756 (0.08066) | LR: 0.00000372 | TIME: 0:04:22 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0360/1466 | LOSS: 0.03185 (0.08027) | LR: 0.00000348 | TIME: 0:04:54 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0400/1466 | LOSS: 0.04892 (0.08102) | LR: 0.00000325 | TIME: 0:05:27 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0440/1466 | LOSS: 0.19249 (0.08153) | LR: 0.00000302 | TIME: 0:06:01 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0480/1466 | LOSS: 0.07900 (0.08134) | LR: 0.00000280 | TIME: 0:06:34 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0520/1466 | LOSS: 0.08198 (0.08150) | LR: 0.00000259 | TIME: 0:07:04 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0560/1466 | LOSS: 0.13649 (0.08179) | LR: 0.00000238 | TIME: 0:07:35 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0600/1466 | LOSS: 0.08050 (0.08152) | LR: 0.00000219 | TIME: 0:08:09 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0640/1466 | LOSS: 0.03649 (0.08149) | LR: 0.00000200 | TIME: 0:08:42 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0680/1466 | LOSS: 0.04853 (0.08098) | LR: 0.00000181 | TIME: 0:09:13 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0720/1466 | LOSS: 0.02965 (0.08056) | LR: 0.00000164 | TIME: 0:09:45 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0760/1466 | LOSS: 0.06911 (0.08028) | LR: 0.00000147 | TIME: 0:10:18 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0800/1466 | LOSS: 0.05613 (0.08070) | LR: 0.00000131 | TIME: 0:10:53 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0840/1466 | LOSS: 0.04977 (0.08010) | LR: 0.00000116 | TIME: 0:11:25 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0880/1466 | LOSS: 0.09050 (0.08039) | LR: 0.00000102 | TIME: 0:11:59 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0920/1466 | LOSS: 0.07744 (0.08033) | LR: 0.00000089 | TIME: 0:12:33 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0960/1466 | LOSS: 0.09469 (0.08051) | LR: 0.00000076 | TIME: 0:13:06 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1000/1466 | LOSS: 0.04955 (0.08058) | LR: 0.00000065 | TIME: 0:13:39 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1040/1466 | LOSS: 0.07719 (0.08044) | LR: 0.00000054 | TIME: 0:14:12 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1080/1466 | LOSS: 0.07624 (0.08029) | LR: 0.00000045 | TIME: 0:14:43 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1120/1466 | LOSS: 0.04967 (0.07984) | LR: 0.00000036 | TIME: 0:15:18 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1160/1466 | LOSS: 0.03090 (0.07988) | LR: 0.00000028 | TIME: 0:15:51 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1200/1466 | LOSS: 0.06107 (0.07997) | LR: 0.00000021 | TIME: 0:16:26 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1240/1466 | LOSS: 0.08843 (0.07966) | LR: 0.00000015 | TIME: 0:16:59 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1280/1466 | LOSS: 0.05208 (0.07998) | LR: 0.00000010 | TIME: 0:17:31 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1320/1466 | LOSS: 0.06867 (0.07998) | LR: 0.00000006 | TIME: 0:18:05 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1360/1466 | LOSS: 0.06143 (0.08012) | LR: 0.00000003 | TIME: 0:18:40 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1400/1466 | LOSS: 0.04880 (0.08016) | LR: 0.00000001 | TIME: 0:19:12 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1440/1466 | LOSS: 0.05082 (0.07979) | LR: 0.00000000 | TIME: 0:19:47 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1465/1466 | LOSS: 0.08125 (0.07991) | LR: 0.00000000 | TIME: 0:20:10 |

VALID_LOOP
[VALID F0] EPOCH: 3/3 | STEP: 000/489 | LOSS: 0.06770 (0.06770) | TIME: 0:00:01 |
[VALID F0] EPOCH: 3/3 | STEP: 040/489 | LOSS: 0.10121 (0.10128) | TIME: 0:00:07 |
[VALID F0] EPOCH: 3/3 | STEP: 080/489 | LOSS: 0.04139 (0.10333) | TIME: 0:00:14 |
[VALID F0] EPOCH: 3/3 | STEP: 120/489 | LOSS: 0.18335 (0.10353) | TIME: 0:00:20 |
[VALID F0] EPOCH: 3/3 | STEP: 160/489 | LOSS: 0.05400 (0.10566) | TIME: 0:00:27 |
[VALID F0] EPOCH: 3/3 | STEP: 200/489 | LOSS: 0.18295 (0.10650) | TIME: 0:00:34 |
[VALID F0] EPOCH: 3/3 | STEP: 240/489 | LOSS: 0.07729 (0.10567) | TIME: 0:00:41 |
[VALID F0] EPOCH: 3/3 | STEP: 280/489 | LOSS: 0.06120 (0.10561) | TIME: 0:00:47 |
[VALID F0] EPOCH: 3/3 | STEP: 320/489 | LOSS: 0.05798 (0.10448) | TIME: 0:00:54 |
[VALID F0] EPOCH: 3/3 | STEP: 360/489 | LOSS: 0.10795 (0.10321) | TIME: 0:01:01 |
[VALID F0] EPOCH: 3/3 | STEP: 400/489 | LOSS: 0.15529 (0.10341) | TIME: 0:01:07 |
[VALID F0] EPOCH: 3/3 | STEP: 440/489 | LOSS: 0.13523 (0.10375) | TIME: 0:01:14 |
[VALID F0] EPOCH: 3/3 | STEP: 480/489 | LOSS: 0.05558 (0.10529) | TIME: 0:01:21 |
[VALID F0] EPOCH: 3/3 | STEP: 488/489 | LOSS: 0.06411 (0.10526) | TIME: 0:01:22 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.07991 |      0.10526 |  0.45956 | 0.499 | 0.434 | 0.428 | 0.462 | 0.473 | 0.462 | 0:21:33 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4595564901828766


----------------------------------- FOLD 0 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45956     0.49858    0.4343       0.42772        0.46158    0.47315          0.462

################################### END OF FOlD 0 ###################################


Date: 2022-11-28 20:36:16.903796+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 4401}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 768

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 1/3 | STEP: 0000/1467 | LOSS: 0.11409 (0.11409) | LR: 0.00000005 | TIME: 0:00:01 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0040/1467 | LOSS: 0.09200 (0.11621) | LR: 0.00000224 | TIME: 0:00:32 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0080/1467 | LOSS: 0.22632 (0.12029) | LR: 0.00000443 | TIME: 0:01:04 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0120/1467 | LOSS: 0.09765 (0.11797) | LR: 0.00000661 | TIME: 0:01:34 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0160/1467 | LOSS: 0.11417 (0.12023) | LR: 0.00000880 | TIME: 0:02:04 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0200/1467 | LOSS: 0.25038 (0.11975) | LR: 0.00001098 | TIME: 0:02:34 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0240/1467 | LOSS: 0.13859 (0.12019) | LR: 0.00001317 | TIME: 0:03:06 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0280/1467 | LOSS: 0.17929 (0.12352) | LR: 0.00001536 | TIME: 0:03:37 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0320/1467 | LOSS: 0.21416 (0.12648) | LR: 0.00001754 | TIME: 0:04:09 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0360/1467 | LOSS: 0.12677 (0.12710) | LR: 0.00001973 | TIME: 0:04:41 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0400/1467 | LOSS: 0.09467 (0.12900) | LR: 0.00002000 | TIME: 0:05:12 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0440/1467 | LOSS: 0.13894 (0.13138) | LR: 0.00001998 | TIME: 0:05:41 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0480/1467 | LOSS: 0.14027 (0.13473) | LR: 0.00001996 | TIME: 0:06:13 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0520/1467 | LOSS: 0.18071 (0.13538) | LR: 0.00001993 | TIME: 0:06:45 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0560/1467 | LOSS: 0.06636 (0.13564) | LR: 0.00001988 | TIME: 0:07:14 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0600/1467 | LOSS: 0.09332 (0.13644) | LR: 0.00001983 | TIME: 0:07:45 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0640/1467 | LOSS: 0.16663 (0.13686) | LR: 0.00001977 | TIME: 0:08:15 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0680/1467 | LOSS: 0.16944 (0.13585) | LR: 0.00001970 | TIME: 0:08:46 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0720/1467 | LOSS: 0.07430 (0.13554) | LR: 0.00001962 | TIME: 0:09:16 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0760/1467 | LOSS: 0.08175 (0.13382) | LR: 0.00001953 | TIME: 0:09:47 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0800/1467 | LOSS: 0.11370 (0.13297) | LR: 0.00001943 | TIME: 0:10:19 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0840/1467 | LOSS: 0.09017 (0.13295) | LR: 0.00001932 | TIME: 0:10:50 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0880/1467 | LOSS: 0.07026 (0.13264) | LR: 0.00001921 | TIME: 0:11:22 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0920/1467 | LOSS: 0.28953 (0.13326) | LR: 0.00001908 | TIME: 0:11:54 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0960/1467 | LOSS: 0.14406 (0.13419) | LR: 0.00001895 | TIME: 0:12:24 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1000/1467 | LOSS: 0.15023 (0.13393) | LR: 0.00001880 | TIME: 0:12:56 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1040/1467 | LOSS: 0.06055 (0.13383) | LR: 0.00001865 | TIME: 0:13:27 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1080/1467 | LOSS: 0.14190 (0.13330) | LR: 0.00001849 | TIME: 0:13:58 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1120/1467 | LOSS: 0.06773 (0.13261) | LR: 0.00001832 | TIME: 0:14:28 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1160/1467 | LOSS: 0.11287 (0.13231) | LR: 0.00001814 | TIME: 0:14:59 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1200/1467 | LOSS: 0.09177 (0.13203) | LR: 0.00001796 | TIME: 0:15:30 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1240/1467 | LOSS: 0.06492 (0.13177) | LR: 0.00001777 | TIME: 0:16:00 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1280/1467 | LOSS: 0.17812 (0.13147) | LR: 0.00001757 | TIME: 0:16:30 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1320/1467 | LOSS: 0.06137 (0.13095) | LR: 0.00001736 | TIME: 0:16:59 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1360/1467 | LOSS: 0.11553 (0.13033) | LR: 0.00001715 | TIME: 0:17:30 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1400/1467 | LOSS: 0.03303 (0.12964) | LR: 0.00001693 | TIME: 0:18:02 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1440/1467 | LOSS: 0.25523 (0.13020) | LR: 0.00001670 | TIME: 0:18:34 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1466/1467 | LOSS: 0.11663 (0.12990) | LR: 0.00001655 | TIME: 0:18:55 |

VALID_LOOP
[VALID F1] EPOCH: 1/3 | STEP: 000/489 | LOSS: 0.08913 (0.08913) | TIME: 0:00:01 |
[VALID F1] EPOCH: 1/3 | STEP: 040/489 | LOSS: 0.13111 (0.10426) | TIME: 0:00:05 |
[VALID F1] EPOCH: 1/3 | STEP: 080/489 | LOSS: 0.06938 (0.10810) | TIME: 0:00:10 |
[VALID F1] EPOCH: 1/3 | STEP: 120/489 | LOSS: 0.06186 (0.10649) | TIME: 0:00:15 |
[VALID F1] EPOCH: 1/3 | STEP: 160/489 | LOSS: 0.13356 (0.10764) | TIME: 0:00:19 |
[VALID F1] EPOCH: 1/3 | STEP: 200/489 | LOSS: 0.05451 (0.10839) | TIME: 0:00:24 |
[VALID F1] EPOCH: 1/3 | STEP: 240/489 | LOSS: 0.04867 (0.10950) | TIME: 0:00:29 |
[VALID F1] EPOCH: 1/3 | STEP: 280/489 | LOSS: 0.04607 (0.10904) | TIME: 0:00:34 |
[VALID F1] EPOCH: 1/3 | STEP: 320/489 | LOSS: 0.14524 (0.10995) | TIME: 0:00:39 |
[VALID F1] EPOCH: 1/3 | STEP: 360/489 | LOSS: 0.13035 (0.10991) | TIME: 0:00:43 |
[VALID F1] EPOCH: 1/3 | STEP: 400/489 | LOSS: 0.06216 (0.10946) | TIME: 0:00:48 |
[VALID F1] EPOCH: 1/3 | STEP: 440/489 | LOSS: 0.07139 (0.11030) | TIME: 0:00:53 |
[VALID F1] EPOCH: 1/3 | STEP: 480/489 | LOSS: 0.13780 (0.10986) | TIME: 0:00:57 |
[VALID F1] EPOCH: 1/3 | STEP: 488/489 | LOSS: 0.09675 (0.10928) | TIME: 0:00:58 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |       0.1299 |      0.10928 |  0.46778 | 0.503 | 0.456 | 0.407 | 0.464 | 0.474 | 0.502 | 0:19:53 |


[SAVED] EPOCH: 1 | MCRMSE: 0.467777281999588

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 2/3 | STEP: 0000/1467 | LOSS: 0.04869 (0.04869) | LR: 0.00001654 | TIME: 0:00:01 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0040/1467 | LOSS: 0.10742 (0.11505) | LR: 0.00001630 | TIME: 0:00:32 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0080/1467 | LOSS: 0.05416 (0.11303) | LR: 0.00001606 | TIME: 0:01:03 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0120/1467 | LOSS: 0.19983 (0.11162) | LR: 0.00001581 | TIME: 0:01:34 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0160/1467 | LOSS: 0.09032 (0.10977) | LR: 0.00001555 | TIME: 0:02:03 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0200/1467 | LOSS: 0.06500 (0.11274) | LR: 0.00001529 | TIME: 0:02:34 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0240/1467 | LOSS: 0.05625 (0.11115) | LR: 0.00001502 | TIME: 0:03:05 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0280/1467 | LOSS: 0.03704 (0.10914) | LR: 0.00001475 | TIME: 0:03:34 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0320/1467 | LOSS: 0.14617 (0.10968) | LR: 0.00001447 | TIME: 0:04:03 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0360/1467 | LOSS: 0.35947 (0.11070) | LR: 0.00001419 | TIME: 0:04:34 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0400/1467 | LOSS: 0.15385 (0.11066) | LR: 0.00001391 | TIME: 0:05:07 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0440/1467 | LOSS: 0.11415 (0.11017) | LR: 0.00001362 | TIME: 0:05:39 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0480/1467 | LOSS: 0.11000 (0.10895) | LR: 0.00001333 | TIME: 0:06:08 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0520/1467 | LOSS: 0.10240 (0.10890) | LR: 0.00001303 | TIME: 0:06:39 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0560/1467 | LOSS: 0.10475 (0.10866) | LR: 0.00001273 | TIME: 0:07:12 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0600/1467 | LOSS: 0.04250 (0.10857) | LR: 0.00001243 | TIME: 0:07:41 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0640/1467 | LOSS: 0.14322 (0.10771) | LR: 0.00001213 | TIME: 0:08:11 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0680/1467 | LOSS: 0.06814 (0.10750) | LR: 0.00001182 | TIME: 0:08:44 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0720/1467 | LOSS: 0.02609 (0.10642) | LR: 0.00001152 | TIME: 0:09:18 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0760/1467 | LOSS: 0.03573 (0.10694) | LR: 0.00001121 | TIME: 0:09:52 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0800/1467 | LOSS: 0.18951 (0.10645) | LR: 0.00001090 | TIME: 0:10:28 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0840/1467 | LOSS: 0.11505 (0.10628) | LR: 0.00001059 | TIME: 0:11:01 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0880/1467 | LOSS: 0.05708 (0.10541) | LR: 0.00001028 | TIME: 0:11:35 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0920/1467 | LOSS: 0.10910 (0.10503) | LR: 0.00000996 | TIME: 0:12:07 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0960/1467 | LOSS: 0.07015 (0.10480) | LR: 0.00000965 | TIME: 0:12:40 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1000/1467 | LOSS: 0.12124 (0.10459) | LR: 0.00000934 | TIME: 0:13:13 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1040/1467 | LOSS: 0.08939 (0.10458) | LR: 0.00000903 | TIME: 0:13:47 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1080/1467 | LOSS: 0.03806 (0.10458) | LR: 0.00000872 | TIME: 0:14:19 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1120/1467 | LOSS: 0.06845 (0.10383) | LR: 0.00000841 | TIME: 0:14:53 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1160/1467 | LOSS: 0.05449 (0.10328) | LR: 0.00000811 | TIME: 0:15:26 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1200/1467 | LOSS: 0.10474 (0.10354) | LR: 0.00000780 | TIME: 0:15:57 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1240/1467 | LOSS: 0.15085 (0.10341) | LR: 0.00000750 | TIME: 0:16:29 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1280/1467 | LOSS: 0.08582 (0.10316) | LR: 0.00000720 | TIME: 0:17:02 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1320/1467 | LOSS: 0.06777 (0.10358) | LR: 0.00000690 | TIME: 0:17:34 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1360/1467 | LOSS: 0.21175 (0.10424) | LR: 0.00000661 | TIME: 0:18:06 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1400/1467 | LOSS: 0.05241 (0.10369) | LR: 0.00000632 | TIME: 0:18:40 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1440/1467 | LOSS: 0.09096 (0.10338) | LR: 0.00000603 | TIME: 0:19:12 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1466/1467 | LOSS: 0.05074 (0.10311) | LR: 0.00000584 | TIME: 0:19:33 |

VALID_LOOP
[VALID F1] EPOCH: 2/3 | STEP: 000/489 | LOSS: 0.07866 (0.07866) | TIME: 0:00:01 |
[VALID F1] EPOCH: 2/3 | STEP: 040/489 | LOSS: 0.12952 (0.10134) | TIME: 0:00:07 |
[VALID F1] EPOCH: 2/3 | STEP: 080/489 | LOSS: 0.05836 (0.10633) | TIME: 0:00:14 |
[VALID F1] EPOCH: 2/3 | STEP: 120/489 | LOSS: 0.06456 (0.10366) | TIME: 0:00:20 |
[VALID F1] EPOCH: 2/3 | STEP: 160/489 | LOSS: 0.15656 (0.10291) | TIME: 0:00:27 |
[VALID F1] EPOCH: 2/3 | STEP: 200/489 | LOSS: 0.04024 (0.10360) | TIME: 0:00:34 |
[VALID F1] EPOCH: 2/3 | STEP: 240/489 | LOSS: 0.03064 (0.10373) | TIME: 0:00:40 |
[VALID F1] EPOCH: 2/3 | STEP: 280/489 | LOSS: 0.04563 (0.10348) | TIME: 0:00:47 |
[VALID F1] EPOCH: 2/3 | STEP: 320/489 | LOSS: 0.11767 (0.10415) | TIME: 0:00:53 |
[VALID F1] EPOCH: 2/3 | STEP: 360/489 | LOSS: 0.09601 (0.10407) | TIME: 0:01:00 |
[VALID F1] EPOCH: 2/3 | STEP: 400/489 | LOSS: 0.04861 (0.10410) | TIME: 0:01:07 |
[VALID F1] EPOCH: 2/3 | STEP: 440/489 | LOSS: 0.06052 (0.10528) | TIME: 0:01:13 |
[VALID F1] EPOCH: 2/3 | STEP: 480/489 | LOSS: 0.16979 (0.10505) | TIME: 0:01:20 |
[VALID F1] EPOCH: 2/3 | STEP: 488/489 | LOSS: 0.06164 (0.10444) | TIME: 0:01:21 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10311 |      0.10444 |  0.45777 | 0.495 | 0.458 | 0.414 | 0.456 | 0.481 | 0.443 | 0:20:54 |


[SAVED] EPOCH: 2 | MCRMSE: 0.45777013897895813

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 3/3 | STEP: 0000/1467 | LOSS: 0.05880 (0.05880) | LR: 0.00000584 | TIME: 0:00:01 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0040/1467 | LOSS: 0.06723 (0.08794) | LR: 0.00000556 | TIME: 0:00:38 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0080/1467 | LOSS: 0.20412 (0.08189) | LR: 0.00000528 | TIME: 0:01:08 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0120/1467 | LOSS: 0.03757 (0.08017) | LR: 0.00000501 | TIME: 0:01:39 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0160/1467 | LOSS: 0.14482 (0.08132) | LR: 0.00000474 | TIME: 0:02:09 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0200/1467 | LOSS: 0.03907 (0.08069) | LR: 0.00000448 | TIME: 0:02:39 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0240/1467 | LOSS: 0.06780 (0.08115) | LR: 0.00000422 | TIME: 0:03:10 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0280/1467 | LOSS: 0.06326 (0.08199) | LR: 0.00000397 | TIME: 0:03:41 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0320/1467 | LOSS: 0.05972 (0.08251) | LR: 0.00000372 | TIME: 0:04:12 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0360/1467 | LOSS: 0.08612 (0.08199) | LR: 0.00000348 | TIME: 0:04:42 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0400/1467 | LOSS: 0.07329 (0.08170) | LR: 0.00000325 | TIME: 0:05:14 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0440/1467 | LOSS: 0.04370 (0.08044) | LR: 0.00000302 | TIME: 0:05:45 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0480/1467 | LOSS: 0.10309 (0.08038) | LR: 0.00000280 | TIME: 0:06:16 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0520/1467 | LOSS: 0.12183 (0.08052) | LR: 0.00000259 | TIME: 0:06:48 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0560/1467 | LOSS: 0.02381 (0.08073) | LR: 0.00000239 | TIME: 0:07:17 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0600/1467 | LOSS: 0.10641 (0.08096) | LR: 0.00000219 | TIME: 0:07:48 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0640/1467 | LOSS: 0.05871 (0.08069) | LR: 0.00000200 | TIME: 0:08:17 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0680/1467 | LOSS: 0.02544 (0.08100) | LR: 0.00000181 | TIME: 0:08:50 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0720/1467 | LOSS: 0.14847 (0.08155) | LR: 0.00000164 | TIME: 0:09:24 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0760/1467 | LOSS: 0.05126 (0.08137) | LR: 0.00000147 | TIME: 0:09:57 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0800/1467 | LOSS: 0.10403 (0.08078) | LR: 0.00000131 | TIME: 0:10:29 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0840/1467 | LOSS: 0.06877 (0.08074) | LR: 0.00000116 | TIME: 0:10:59 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0880/1467 | LOSS: 0.06552 (0.08071) | LR: 0.00000102 | TIME: 0:11:34 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0920/1467 | LOSS: 0.02934 (0.08063) | LR: 0.00000089 | TIME: 0:12:06 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0960/1467 | LOSS: 0.04023 (0.08028) | LR: 0.00000077 | TIME: 0:12:38 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1000/1467 | LOSS: 0.10894 (0.08009) | LR: 0.00000065 | TIME: 0:13:09 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1040/1467 | LOSS: 0.05172 (0.07975) | LR: 0.00000055 | TIME: 0:13:39 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1080/1467 | LOSS: 0.04809 (0.08004) | LR: 0.00000045 | TIME: 0:14:09 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1120/1467 | LOSS: 0.02277 (0.07983) | LR: 0.00000036 | TIME: 0:14:39 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1160/1467 | LOSS: 0.05783 (0.07991) | LR: 0.00000028 | TIME: 0:15:12 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1200/1467 | LOSS: 0.06413 (0.07971) | LR: 0.00000021 | TIME: 0:15:41 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1240/1467 | LOSS: 0.08342 (0.07954) | LR: 0.00000015 | TIME: 0:16:11 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1280/1467 | LOSS: 0.14903 (0.07948) | LR: 0.00000010 | TIME: 0:16:42 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1320/1467 | LOSS: 0.06066 (0.07967) | LR: 0.00000006 | TIME: 0:17:14 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1360/1467 | LOSS: 0.08305 (0.07966) | LR: 0.00000003 | TIME: 0:17:42 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1400/1467 | LOSS: 0.14830 (0.07940) | LR: 0.00000001 | TIME: 0:18:17 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1440/1467 | LOSS: 0.07745 (0.07956) | LR: 0.00000000 | TIME: 0:18:51 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1466/1467 | LOSS: 0.10550 (0.07964) | LR: 0.00000000 | TIME: 0:19:12 |

VALID_LOOP
[VALID F1] EPOCH: 3/3 | STEP: 000/489 | LOSS: 0.08068 (0.08068) | TIME: 0:00:01 |
[VALID F1] EPOCH: 3/3 | STEP: 040/489 | LOSS: 0.11811 (0.10022) | TIME: 0:00:07 |
[VALID F1] EPOCH: 3/3 | STEP: 080/489 | LOSS: 0.05310 (0.10513) | TIME: 0:00:14 |
[VALID F1] EPOCH: 3/3 | STEP: 120/489 | LOSS: 0.06049 (0.10280) | TIME: 0:00:21 |
[VALID F1] EPOCH: 3/3 | STEP: 160/489 | LOSS: 0.15006 (0.10198) | TIME: 0:00:27 |
[VALID F1] EPOCH: 3/3 | STEP: 200/489 | LOSS: 0.03724 (0.10245) | TIME: 0:00:34 |
[VALID F1] EPOCH: 3/3 | STEP: 240/489 | LOSS: 0.03527 (0.10229) | TIME: 0:00:40 |
[VALID F1] EPOCH: 3/3 | STEP: 280/489 | LOSS: 0.04863 (0.10225) | TIME: 0:00:47 |
[VALID F1] EPOCH: 3/3 | STEP: 320/489 | LOSS: 0.12978 (0.10295) | TIME: 0:00:54 |
[VALID F1] EPOCH: 3/3 | STEP: 360/489 | LOSS: 0.10397 (0.10289) | TIME: 0:01:00 |
[VALID F1] EPOCH: 3/3 | STEP: 400/489 | LOSS: 0.04992 (0.10290) | TIME: 0:01:07 |
[VALID F1] EPOCH: 3/3 | STEP: 440/489 | LOSS: 0.05703 (0.10403) | TIME: 0:01:13 |
[VALID F1] EPOCH: 3/3 | STEP: 480/489 | LOSS: 0.15637 (0.10393) | TIME: 0:01:20 |
[VALID F1] EPOCH: 3/3 | STEP: 488/489 | LOSS: 0.06426 (0.10327) | TIME: 0:01:21 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.07964 |      0.10327 |  0.45486 | 0.490 | 0.456 | 0.403 | 0.456 | 0.481 | 0.442 | 0:20:33 |


[SAVED] EPOCH: 3 | MCRMSE: 0.45486411452293396


----------------------------------- FOLD 1 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45486     0.49019   0.45639       0.40336        0.45617    0.48075        0.44234

################################### END OF FOlD 1 ###################################


Date: 2022-11-28 23:14:15.790614+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 4398}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 768

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 1/3 | STEP: 0000/1466 | LOSS: 0.14528 (0.14528) | LR: 0.00000005 | TIME: 0:00:01 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0040/1466 | LOSS: 0.03374 (0.09643) | LR: 0.00000224 | TIME: 0:00:31 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0080/1466 | LOSS: 0.24504 (0.10969) | LR: 0.00000443 | TIME: 0:01:02 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0120/1466 | LOSS: 0.17340 (0.11600) | LR: 0.00000661 | TIME: 0:01:33 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0160/1466 | LOSS: 0.17581 (0.11767) | LR: 0.00000880 | TIME: 0:02:04 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0200/1466 | LOSS: 0.06712 (0.11788) | LR: 0.00001098 | TIME: 0:02:33 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0240/1466 | LOSS: 0.05233 (0.11769) | LR: 0.00001317 | TIME: 0:03:05 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0280/1466 | LOSS: 0.09071 (0.11853) | LR: 0.00001536 | TIME: 0:03:36 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0320/1466 | LOSS: 0.11270 (0.12039) | LR: 0.00001754 | TIME: 0:04:09 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0360/1466 | LOSS: 0.09660 (0.12198) | LR: 0.00001973 | TIME: 0:04:42 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0400/1466 | LOSS: 0.15606 (0.12309) | LR: 0.00002000 | TIME: 0:05:15 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0440/1466 | LOSS: 0.07843 (0.12507) | LR: 0.00001998 | TIME: 0:05:47 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0480/1466 | LOSS: 0.16521 (0.12702) | LR: 0.00001996 | TIME: 0:06:21 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0520/1466 | LOSS: 0.10297 (0.12706) | LR: 0.00001993 | TIME: 0:06:53 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0560/1466 | LOSS: 0.09187 (0.12671) | LR: 0.00001988 | TIME: 0:07:23 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0600/1466 | LOSS: 0.19656 (0.12639) | LR: 0.00001983 | TIME: 0:07:54 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0640/1466 | LOSS: 0.20976 (0.12686) | LR: 0.00001977 | TIME: 0:08:23 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0680/1466 | LOSS: 0.13455 (0.12654) | LR: 0.00001970 | TIME: 0:08:54 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0720/1466 | LOSS: 0.16454 (0.12623) | LR: 0.00001962 | TIME: 0:09:23 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0760/1466 | LOSS: 0.15752 (0.12681) | LR: 0.00001953 | TIME: 0:09:54 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0800/1466 | LOSS: 0.12917 (0.12780) | LR: 0.00001943 | TIME: 0:10:26 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0840/1466 | LOSS: 0.11307 (0.12735) | LR: 0.00001932 | TIME: 0:10:59 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0880/1466 | LOSS: 0.09792 (0.12751) | LR: 0.00001921 | TIME: 0:11:30 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0920/1466 | LOSS: 0.33686 (0.12775) | LR: 0.00001908 | TIME: 0:11:59 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0960/1466 | LOSS: 0.37868 (0.12776) | LR: 0.00001894 | TIME: 0:12:31 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1000/1466 | LOSS: 0.13203 (0.12760) | LR: 0.00001880 | TIME: 0:13:02 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1040/1466 | LOSS: 0.20292 (0.12782) | LR: 0.00001865 | TIME: 0:13:34 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1080/1466 | LOSS: 0.26864 (0.12771) | LR: 0.00001849 | TIME: 0:14:05 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1120/1466 | LOSS: 0.07985 (0.12797) | LR: 0.00001832 | TIME: 0:14:35 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1160/1466 | LOSS: 0.05802 (0.12871) | LR: 0.00001814 | TIME: 0:15:08 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1200/1466 | LOSS: 0.12108 (0.12866) | LR: 0.00001796 | TIME: 0:15:37 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1240/1466 | LOSS: 0.16795 (0.12889) | LR: 0.00001776 | TIME: 0:16:07 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1280/1466 | LOSS: 0.09103 (0.12899) | LR: 0.00001756 | TIME: 0:16:38 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1320/1466 | LOSS: 0.12976 (0.12843) | LR: 0.00001736 | TIME: 0:17:08 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1360/1466 | LOSS: 0.06784 (0.12810) | LR: 0.00001714 | TIME: 0:17:37 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1400/1466 | LOSS: 0.12823 (0.12826) | LR: 0.00001692 | TIME: 0:18:07 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1440/1466 | LOSS: 0.09196 (0.12813) | LR: 0.00001669 | TIME: 0:18:37 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1465/1466 | LOSS: 0.19674 (0.12831) | LR: 0.00001655 | TIME: 0:18:55 |

VALID_LOOP
[VALID F2] EPOCH: 1/3 | STEP: 000/489 | LOSS: 0.04728 (0.04728) | TIME: 0:00:01 |
[VALID F2] EPOCH: 1/3 | STEP: 040/489 | LOSS: 0.23411 (0.12804) | TIME: 0:00:05 |
[VALID F2] EPOCH: 1/3 | STEP: 080/489 | LOSS: 0.04968 (0.12093) | TIME: 0:00:10 |
[VALID F2] EPOCH: 1/3 | STEP: 120/489 | LOSS: 0.12659 (0.12079) | TIME: 0:00:15 |
[VALID F2] EPOCH: 1/3 | STEP: 160/489 | LOSS: 0.20137 (0.12367) | TIME: 0:00:20 |
[VALID F2] EPOCH: 1/3 | STEP: 200/489 | LOSS: 0.05283 (0.12052) | TIME: 0:00:24 |
[VALID F2] EPOCH: 1/3 | STEP: 240/489 | LOSS: 0.06992 (0.12392) | TIME: 0:00:29 |
[VALID F2] EPOCH: 1/3 | STEP: 280/489 | LOSS: 0.04590 (0.12267) | TIME: 0:00:34 |
[VALID F2] EPOCH: 1/3 | STEP: 320/489 | LOSS: 0.10870 (0.12267) | TIME: 0:00:38 |
[VALID F2] EPOCH: 1/3 | STEP: 360/489 | LOSS: 0.09976 (0.12375) | TIME: 0:00:43 |
[VALID F2] EPOCH: 1/3 | STEP: 400/489 | LOSS: 0.11035 (0.12318) | TIME: 0:00:48 |
[VALID F2] EPOCH: 1/3 | STEP: 440/489 | LOSS: 0.13908 (0.12352) | TIME: 0:00:53 |
[VALID F2] EPOCH: 1/3 | STEP: 480/489 | LOSS: 0.15190 (0.12256) | TIME: 0:00:57 |
[VALID F2] EPOCH: 1/3 | STEP: 488/489 | LOSS: 0.20666 (0.12218) | TIME: 0:00:58 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |      0.12831 |      0.12218 |  0.49647 | 0.506 | 0.465 | 0.496 | 0.514 | 0.493 | 0.505 | 0:19:54 |


[SAVED] EPOCH: 1 | MCRMSE: 0.49646520614624023

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 2/3 | STEP: 0000/1466 | LOSS: 0.09106 (0.09106) | LR: 0.00001654 | TIME: 0:00:01 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0040/1466 | LOSS: 0.08435 (0.11363) | LR: 0.00001630 | TIME: 0:00:33 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0080/1466 | LOSS: 0.08611 (0.10438) | LR: 0.00001606 | TIME: 0:01:05 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0120/1466 | LOSS: 0.06154 (0.09972) | LR: 0.00001581 | TIME: 0:01:35 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0160/1466 | LOSS: 0.26015 (0.10852) | LR: 0.00001555 | TIME: 0:02:04 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0200/1466 | LOSS: 0.07202 (0.10804) | LR: 0.00001529 | TIME: 0:02:34 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0240/1466 | LOSS: 0.25864 (0.10619) | LR: 0.00001502 | TIME: 0:03:06 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0280/1466 | LOSS: 0.08480 (0.10671) | LR: 0.00001475 | TIME: 0:03:39 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0320/1466 | LOSS: 0.14268 (0.10585) | LR: 0.00001447 | TIME: 0:04:10 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0360/1466 | LOSS: 0.11251 (0.10532) | LR: 0.00001419 | TIME: 0:04:40 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0400/1466 | LOSS: 0.07617 (0.10585) | LR: 0.00001391 | TIME: 0:05:11 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0440/1466 | LOSS: 0.10271 (0.10666) | LR: 0.00001362 | TIME: 0:05:43 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0480/1466 | LOSS: 0.03434 (0.10768) | LR: 0.00001332 | TIME: 0:06:13 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0520/1466 | LOSS: 0.07737 (0.10752) | LR: 0.00001303 | TIME: 0:06:44 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0560/1466 | LOSS: 0.05015 (0.10602) | LR: 0.00001273 | TIME: 0:07:15 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0600/1466 | LOSS: 0.05029 (0.10630) | LR: 0.00001243 | TIME: 0:07:45 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0640/1466 | LOSS: 0.19744 (0.10593) | LR: 0.00001213 | TIME: 0:08:17 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0680/1466 | LOSS: 0.09167 (0.10575) | LR: 0.00001182 | TIME: 0:08:48 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0720/1466 | LOSS: 0.07290 (0.10502) | LR: 0.00001151 | TIME: 0:09:16 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0760/1466 | LOSS: 0.02428 (0.10424) | LR: 0.00001120 | TIME: 0:09:49 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0800/1466 | LOSS: 0.17995 (0.10417) | LR: 0.00001089 | TIME: 0:10:19 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0840/1466 | LOSS: 0.12908 (0.10447) | LR: 0.00001058 | TIME: 0:10:53 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0880/1466 | LOSS: 0.07644 (0.10428) | LR: 0.00001027 | TIME: 0:11:27 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0920/1466 | LOSS: 0.09322 (0.10531) | LR: 0.00000996 | TIME: 0:12:01 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0960/1466 | LOSS: 0.05689 (0.10563) | LR: 0.00000965 | TIME: 0:12:35 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1000/1466 | LOSS: 0.11246 (0.10583) | LR: 0.00000934 | TIME: 0:13:07 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1040/1466 | LOSS: 0.04403 (0.10598) | LR: 0.00000903 | TIME: 0:13:41 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1080/1466 | LOSS: 0.11513 (0.10604) | LR: 0.00000872 | TIME: 0:14:16 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1120/1466 | LOSS: 0.08261 (0.10564) | LR: 0.00000841 | TIME: 0:14:50 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1160/1466 | LOSS: 0.02920 (0.10550) | LR: 0.00000810 | TIME: 0:15:24 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1200/1466 | LOSS: 0.11864 (0.10551) | LR: 0.00000780 | TIME: 0:15:58 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1240/1466 | LOSS: 0.16779 (0.10530) | LR: 0.00000749 | TIME: 0:16:33 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1280/1466 | LOSS: 0.10050 (0.10500) | LR: 0.00000719 | TIME: 0:17:05 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1320/1466 | LOSS: 0.09434 (0.10493) | LR: 0.00000690 | TIME: 0:17:39 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1360/1466 | LOSS: 0.09646 (0.10512) | LR: 0.00000660 | TIME: 0:18:13 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1400/1466 | LOSS: 0.06165 (0.10519) | LR: 0.00000631 | TIME: 0:18:47 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1440/1466 | LOSS: 0.04593 (0.10481) | LR: 0.00000602 | TIME: 0:19:23 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1465/1466 | LOSS: 0.07296 (0.10469) | LR: 0.00000584 | TIME: 0:19:44 |

VALID_LOOP
[VALID F2] EPOCH: 2/3 | STEP: 000/489 | LOSS: 0.02340 (0.02340) | TIME: 0:00:01 |
[VALID F2] EPOCH: 2/3 | STEP: 040/489 | LOSS: 0.11302 (0.10219) | TIME: 0:00:06 |
[VALID F2] EPOCH: 2/3 | STEP: 080/489 | LOSS: 0.05949 (0.09552) | TIME: 0:00:11 |
[VALID F2] EPOCH: 2/3 | STEP: 120/489 | LOSS: 0.08864 (0.09459) | TIME: 0:00:16 |
[VALID F2] EPOCH: 2/3 | STEP: 160/489 | LOSS: 0.09166 (0.09753) | TIME: 0:00:22 |
[VALID F2] EPOCH: 2/3 | STEP: 200/489 | LOSS: 0.07490 (0.09561) | TIME: 0:00:27 |
[VALID F2] EPOCH: 2/3 | STEP: 240/489 | LOSS: 0.07051 (0.09876) | TIME: 0:00:32 |
[VALID F2] EPOCH: 2/3 | STEP: 280/489 | LOSS: 0.06389 (0.09768) | TIME: 0:00:37 |
[VALID F2] EPOCH: 2/3 | STEP: 320/489 | LOSS: 0.09141 (0.09769) | TIME: 0:00:42 |
[VALID F2] EPOCH: 2/3 | STEP: 360/489 | LOSS: 0.14299 (0.09820) | TIME: 0:00:47 |
[VALID F2] EPOCH: 2/3 | STEP: 400/489 | LOSS: 0.08014 (0.09833) | TIME: 0:00:52 |
[VALID F2] EPOCH: 2/3 | STEP: 440/489 | LOSS: 0.19272 (0.09897) | TIME: 0:00:57 |
[VALID F2] EPOCH: 2/3 | STEP: 480/489 | LOSS: 0.08801 (0.09866) | TIME: 0:01:02 |
[VALID F2] EPOCH: 2/3 | STEP: 488/489 | LOSS: 0.07089 (0.09828) | TIME: 0:01:03 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10469 |      0.09828 |  0.44388 | 0.467 | 0.449 | 0.410 | 0.452 | 0.460 | 0.426 | 0:20:48 |


[SAVED] EPOCH: 2 | MCRMSE: 0.44388046860694885

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 3/3 | STEP: 0000/1466 | LOSS: 0.10358 (0.10358) | LR: 0.00000584 | TIME: 0:00:02 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0040/1466 | LOSS: 0.09737 (0.07900) | LR: 0.00000556 | TIME: 0:00:33 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0080/1466 | LOSS: 0.10168 (0.08136) | LR: 0.00000528 | TIME: 0:01:06 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0120/1466 | LOSS: 0.03687 (0.08175) | LR: 0.00000501 | TIME: 0:01:38 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0160/1466 | LOSS: 0.05197 (0.08459) | LR: 0.00000474 | TIME: 0:02:10 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0200/1466 | LOSS: 0.06479 (0.08350) | LR: 0.00000448 | TIME: 0:02:43 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0240/1466 | LOSS: 0.12468 (0.08247) | LR: 0.00000422 | TIME: 0:03:12 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0280/1466 | LOSS: 0.03694 (0.08212) | LR: 0.00000397 | TIME: 0:03:42 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0320/1466 | LOSS: 0.04724 (0.08258) | LR: 0.00000372 | TIME: 0:04:14 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0360/1466 | LOSS: 0.08233 (0.08230) | LR: 0.00000348 | TIME: 0:04:45 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0400/1466 | LOSS: 0.06368 (0.08289) | LR: 0.00000325 | TIME: 0:05:15 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0440/1466 | LOSS: 0.11060 (0.08302) | LR: 0.00000302 | TIME: 0:05:46 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0480/1466 | LOSS: 0.11995 (0.08289) | LR: 0.00000280 | TIME: 0:06:19 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0520/1466 | LOSS: 0.04468 (0.08223) | LR: 0.00000259 | TIME: 0:06:49 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0560/1466 | LOSS: 0.04920 (0.08210) | LR: 0.00000238 | TIME: 0:07:18 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0600/1466 | LOSS: 0.07324 (0.08208) | LR: 0.00000219 | TIME: 0:07:48 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0640/1466 | LOSS: 0.06898 (0.08283) | LR: 0.00000200 | TIME: 0:08:21 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0680/1466 | LOSS: 0.04194 (0.08244) | LR: 0.00000181 | TIME: 0:08:53 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0720/1466 | LOSS: 0.06172 (0.08195) | LR: 0.00000164 | TIME: 0:09:23 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0760/1466 | LOSS: 0.07139 (0.08173) | LR: 0.00000147 | TIME: 0:09:55 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0800/1466 | LOSS: 0.07977 (0.08200) | LR: 0.00000131 | TIME: 0:10:25 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0840/1466 | LOSS: 0.11028 (0.08201) | LR: 0.00000116 | TIME: 0:10:57 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0880/1466 | LOSS: 0.03351 (0.08210) | LR: 0.00000102 | TIME: 0:11:29 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0920/1466 | LOSS: 0.06970 (0.08181) | LR: 0.00000089 | TIME: 0:11:59 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0960/1466 | LOSS: 0.06600 (0.08220) | LR: 0.00000076 | TIME: 0:12:30 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1000/1466 | LOSS: 0.09405 (0.08253) | LR: 0.00000065 | TIME: 0:13:01 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1040/1466 | LOSS: 0.04271 (0.08234) | LR: 0.00000054 | TIME: 0:13:33 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1080/1466 | LOSS: 0.06641 (0.08202) | LR: 0.00000045 | TIME: 0:14:04 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1120/1466 | LOSS: 0.04060 (0.08180) | LR: 0.00000036 | TIME: 0:14:36 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1160/1466 | LOSS: 0.05646 (0.08164) | LR: 0.00000028 | TIME: 0:15:06 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1200/1466 | LOSS: 0.05917 (0.08144) | LR: 0.00000021 | TIME: 0:15:36 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1240/1466 | LOSS: 0.05635 (0.08136) | LR: 0.00000015 | TIME: 0:16:08 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1280/1466 | LOSS: 0.15589 (0.08131) | LR: 0.00000010 | TIME: 0:16:39 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1320/1466 | LOSS: 0.07278 (0.08144) | LR: 0.00000006 | TIME: 0:17:11 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1360/1466 | LOSS: 0.08198 (0.08130) | LR: 0.00000003 | TIME: 0:17:42 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1400/1466 | LOSS: 0.16612 (0.08141) | LR: 0.00000001 | TIME: 0:18:14 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1440/1466 | LOSS: 0.04691 (0.08108) | LR: 0.00000000 | TIME: 0:18:45 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1465/1466 | LOSS: 0.07510 (0.08142) | LR: 0.00000000 | TIME: 0:19:06 |

VALID_LOOP
[VALID F2] EPOCH: 3/3 | STEP: 000/489 | LOSS: 0.03240 (0.03240) | TIME: 0:00:01 |
[VALID F2] EPOCH: 3/3 | STEP: 040/489 | LOSS: 0.12584 (0.10261) | TIME: 0:00:06 |
[VALID F2] EPOCH: 3/3 | STEP: 080/489 | LOSS: 0.05635 (0.09561) | TIME: 0:00:12 |
[VALID F2] EPOCH: 3/3 | STEP: 120/489 | LOSS: 0.07791 (0.09428) | TIME: 0:00:18 |
[VALID F2] EPOCH: 3/3 | STEP: 160/489 | LOSS: 0.10235 (0.09686) | TIME: 0:00:24 |
[VALID F2] EPOCH: 3/3 | STEP: 200/489 | LOSS: 0.07551 (0.09514) | TIME: 0:00:30 |
[VALID F2] EPOCH: 3/3 | STEP: 240/489 | LOSS: 0.05537 (0.09837) | TIME: 0:00:36 |
[VALID F2] EPOCH: 3/3 | STEP: 280/489 | LOSS: 0.04579 (0.09717) | TIME: 0:00:42 |
[VALID F2] EPOCH: 3/3 | STEP: 320/489 | LOSS: 0.10245 (0.09706) | TIME: 0:00:47 |
[VALID F2] EPOCH: 3/3 | STEP: 360/489 | LOSS: 0.14554 (0.09733) | TIME: 0:00:53 |
[VALID F2] EPOCH: 3/3 | STEP: 400/489 | LOSS: 0.07793 (0.09735) | TIME: 0:00:59 |
[VALID F2] EPOCH: 3/3 | STEP: 440/489 | LOSS: 0.18137 (0.09788) | TIME: 0:01:05 |
[VALID F2] EPOCH: 3/3 | STEP: 480/489 | LOSS: 0.09088 (0.09761) | TIME: 0:01:11 |
[VALID F2] EPOCH: 3/3 | STEP: 488/489 | LOSS: 0.09752 (0.09726) | TIME: 0:01:12 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08142 |      0.09726 |  0.44161 | 0.462 | 0.444 | 0.410 | 0.448 | 0.460 | 0.427 | 0:20:20 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4416123330593109


----------------------------------- FOLD 2 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.44161     0.46192   0.44374       0.40962        0.44753    0.45986          0.427

################################### END OF FOlD 2 ###################################


Date: 2022-11-29 00:15:45.818870+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 4398}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 768

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 1/3 | STEP: 0000/1466 | LOSS: 0.10368 (0.10368) | LR: 0.00000005 | TIME: 0:00:01 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0040/1466 | LOSS: 0.12775 (0.11762) | LR: 0.00000224 | TIME: 0:00:32 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0080/1466 | LOSS: 0.07367 (0.10828) | LR: 0.00000443 | TIME: 0:01:03 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0120/1466 | LOSS: 0.03461 (0.11094) | LR: 0.00000661 | TIME: 0:01:35 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0160/1466 | LOSS: 0.14916 (0.11482) | LR: 0.00000880 | TIME: 0:02:05 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0200/1466 | LOSS: 0.11806 (0.11571) | LR: 0.00001098 | TIME: 0:02:36 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0240/1466 | LOSS: 0.05879 (0.11349) | LR: 0.00001317 | TIME: 0:03:06 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0280/1466 | LOSS: 0.15868 (0.11773) | LR: 0.00001536 | TIME: 0:03:39 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0320/1466 | LOSS: 0.15310 (0.12074) | LR: 0.00001754 | TIME: 0:04:08 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0360/1466 | LOSS: 0.23179 (0.12679) | LR: 0.00001973 | TIME: 0:04:38 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0400/1466 | LOSS: 0.10552 (0.12889) | LR: 0.00002000 | TIME: 0:05:09 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0440/1466 | LOSS: 0.06459 (0.12891) | LR: 0.00001998 | TIME: 0:05:41 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0480/1466 | LOSS: 0.10770 (0.12931) | LR: 0.00001996 | TIME: 0:06:12 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0520/1466 | LOSS: 0.19796 (0.13091) | LR: 0.00001993 | TIME: 0:06:43 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0560/1466 | LOSS: 0.10608 (0.12993) | LR: 0.00001988 | TIME: 0:07:13 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0600/1466 | LOSS: 0.05082 (0.12936) | LR: 0.00001983 | TIME: 0:07:44 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0640/1466 | LOSS: 0.14060 (0.12831) | LR: 0.00001977 | TIME: 0:08:17 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0680/1466 | LOSS: 0.10122 (0.12784) | LR: 0.00001970 | TIME: 0:08:47 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0720/1466 | LOSS: 0.23750 (0.12762) | LR: 0.00001962 | TIME: 0:09:16 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0760/1466 | LOSS: 0.12518 (0.12735) | LR: 0.00001953 | TIME: 0:09:47 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0800/1466 | LOSS: 0.11618 (0.12672) | LR: 0.00001943 | TIME: 0:10:16 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0840/1466 | LOSS: 0.14123 (0.12770) | LR: 0.00001932 | TIME: 0:10:47 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0880/1466 | LOSS: 0.12921 (0.12819) | LR: 0.00001921 | TIME: 0:11:18 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0920/1466 | LOSS: 0.09693 (0.12789) | LR: 0.00001908 | TIME: 0:11:46 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0960/1466 | LOSS: 0.06675 (0.12871) | LR: 0.00001894 | TIME: 0:12:18 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1000/1466 | LOSS: 0.06374 (0.12850) | LR: 0.00001880 | TIME: 0:12:47 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1040/1466 | LOSS: 0.08247 (0.12897) | LR: 0.00001865 | TIME: 0:13:16 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1080/1466 | LOSS: 0.25684 (0.12922) | LR: 0.00001849 | TIME: 0:13:46 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1120/1466 | LOSS: 0.15428 (0.12871) | LR: 0.00001832 | TIME: 0:14:16 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1160/1466 | LOSS: 0.08464 (0.12898) | LR: 0.00001814 | TIME: 0:14:45 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1200/1466 | LOSS: 0.13242 (0.12880) | LR: 0.00001796 | TIME: 0:15:15 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1240/1466 | LOSS: 0.10796 (0.12842) | LR: 0.00001776 | TIME: 0:15:44 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1280/1466 | LOSS: 0.17286 (0.12866) | LR: 0.00001756 | TIME: 0:16:13 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1320/1466 | LOSS: 0.09131 (0.12829) | LR: 0.00001736 | TIME: 0:16:41 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1360/1466 | LOSS: 0.07595 (0.12802) | LR: 0.00001714 | TIME: 0:17:11 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1400/1466 | LOSS: 0.10942 (0.12798) | LR: 0.00001692 | TIME: 0:17:42 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1440/1466 | LOSS: 0.03934 (0.12801) | LR: 0.00001669 | TIME: 0:18:12 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1465/1466 | LOSS: 0.41220 (0.12826) | LR: 0.00001655 | TIME: 0:18:31 |

VALID_LOOP
[VALID F3] EPOCH: 1/3 | STEP: 000/489 | LOSS: 0.46140 (0.46140) | TIME: 0:00:01 |
[VALID F3] EPOCH: 1/3 | STEP: 040/489 | LOSS: 0.10132 (0.12953) | TIME: 0:00:05 |
[VALID F3] EPOCH: 1/3 | STEP: 080/489 | LOSS: 0.25907 (0.12821) | TIME: 0:00:10 |
[VALID F3] EPOCH: 1/3 | STEP: 120/489 | LOSS: 0.10891 (0.12390) | TIME: 0:00:14 |
[VALID F3] EPOCH: 1/3 | STEP: 160/489 | LOSS: 0.09808 (0.12469) | TIME: 0:00:19 |
[VALID F3] EPOCH: 1/3 | STEP: 200/489 | LOSS: 0.07998 (0.12311) | TIME: 0:00:24 |
[VALID F3] EPOCH: 1/3 | STEP: 240/489 | LOSS: 0.06149 (0.11929) | TIME: 0:00:28 |
[VALID F3] EPOCH: 1/3 | STEP: 280/489 | LOSS: 0.06637 (0.12062) | TIME: 0:00:33 |
[VALID F3] EPOCH: 1/3 | STEP: 320/489 | LOSS: 0.08664 (0.12093) | TIME: 0:00:37 |
[VALID F3] EPOCH: 1/3 | STEP: 360/489 | LOSS: 0.09854 (0.11921) | TIME: 0:00:42 |
[VALID F3] EPOCH: 1/3 | STEP: 400/489 | LOSS: 0.06790 (0.11964) | TIME: 0:00:47 |
[VALID F3] EPOCH: 1/3 | STEP: 440/489 | LOSS: 0.09642 (0.11891) | TIME: 0:00:51 |
[VALID F3] EPOCH: 1/3 | STEP: 480/489 | LOSS: 0.08605 (0.11751) | TIME: 0:00:56 |
[VALID F3] EPOCH: 1/3 | STEP: 488/489 | LOSS: 0.02853 (0.11749) | TIME: 0:00:57 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |      0.12826 |      0.11749 |  0.48572 | 0.497 | 0.538 | 0.501 | 0.457 | 0.481 | 0.440 | 0:19:28 |


[SAVED] EPOCH: 1 | MCRMSE: 0.48571690917015076

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 2/3 | STEP: 0000/1466 | LOSS: 0.07554 (0.07554) | LR: 0.00001654 | TIME: 0:00:01 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0040/1466 | LOSS: 0.08083 (0.11198) | LR: 0.00001630 | TIME: 0:00:33 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0080/1466 | LOSS: 0.05772 (0.11120) | LR: 0.00001606 | TIME: 0:01:03 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0120/1466 | LOSS: 0.20382 (0.10953) | LR: 0.00001581 | TIME: 0:01:33 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0160/1466 | LOSS: 0.06310 (0.11123) | LR: 0.00001555 | TIME: 0:02:01 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0200/1466 | LOSS: 0.11830 (0.11136) | LR: 0.00001529 | TIME: 0:02:32 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0240/1466 | LOSS: 0.01911 (0.10894) | LR: 0.00001502 | TIME: 0:03:00 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0280/1466 | LOSS: 0.18451 (0.11180) | LR: 0.00001475 | TIME: 0:03:29 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0320/1466 | LOSS: 0.14086 (0.11173) | LR: 0.00001447 | TIME: 0:03:58 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0360/1466 | LOSS: 0.06106 (0.11007) | LR: 0.00001419 | TIME: 0:04:29 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0400/1466 | LOSS: 0.07782 (0.11082) | LR: 0.00001391 | TIME: 0:05:00 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0440/1466 | LOSS: 0.21795 (0.11046) | LR: 0.00001362 | TIME: 0:05:29 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0480/1466 | LOSS: 0.14965 (0.11014) | LR: 0.00001332 | TIME: 0:05:59 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0520/1466 | LOSS: 0.09734 (0.11059) | LR: 0.00001303 | TIME: 0:06:30 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0560/1466 | LOSS: 0.08979 (0.10980) | LR: 0.00001273 | TIME: 0:07:04 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0600/1466 | LOSS: 0.09966 (0.10919) | LR: 0.00001243 | TIME: 0:07:33 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0640/1466 | LOSS: 0.15864 (0.10894) | LR: 0.00001213 | TIME: 0:08:06 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0680/1466 | LOSS: 0.05401 (0.10960) | LR: 0.00001182 | TIME: 0:08:39 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0720/1466 | LOSS: 0.09049 (0.10930) | LR: 0.00001151 | TIME: 0:09:08 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0760/1466 | LOSS: 0.07256 (0.10930) | LR: 0.00001120 | TIME: 0:09:39 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0800/1466 | LOSS: 0.06009 (0.10931) | LR: 0.00001089 | TIME: 0:10:08 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0840/1466 | LOSS: 0.06418 (0.10934) | LR: 0.00001058 | TIME: 0:10:39 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0880/1466 | LOSS: 0.07187 (0.10872) | LR: 0.00001027 | TIME: 0:11:09 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0920/1466 | LOSS: 0.05417 (0.10796) | LR: 0.00000996 | TIME: 0:11:39 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0960/1466 | LOSS: 0.17955 (0.10731) | LR: 0.00000965 | TIME: 0:12:09 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1000/1466 | LOSS: 0.04375 (0.10653) | LR: 0.00000934 | TIME: 0:12:40 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1040/1466 | LOSS: 0.15592 (0.10611) | LR: 0.00000903 | TIME: 0:13:08 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1080/1466 | LOSS: 0.14779 (0.10592) | LR: 0.00000872 | TIME: 0:13:41 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1120/1466 | LOSS: 0.04800 (0.10531) | LR: 0.00000841 | TIME: 0:14:12 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1160/1466 | LOSS: 0.07386 (0.10512) | LR: 0.00000810 | TIME: 0:14:43 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1200/1466 | LOSS: 0.06537 (0.10473) | LR: 0.00000780 | TIME: 0:15:14 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1240/1466 | LOSS: 0.10740 (0.10477) | LR: 0.00000749 | TIME: 0:15:43 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1280/1466 | LOSS: 0.07874 (0.10439) | LR: 0.00000719 | TIME: 0:16:12 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1320/1466 | LOSS: 0.15844 (0.10464) | LR: 0.00000690 | TIME: 0:16:42 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1360/1466 | LOSS: 0.04346 (0.10465) | LR: 0.00000660 | TIME: 0:17:11 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1400/1466 | LOSS: 0.08250 (0.10450) | LR: 0.00000631 | TIME: 0:17:43 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1440/1466 | LOSS: 0.15432 (0.10433) | LR: 0.00000602 | TIME: 0:18:15 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1465/1466 | LOSS: 0.11872 (0.10394) | LR: 0.00000584 | TIME: 0:18:32 |

VALID_LOOP
[VALID F3] EPOCH: 2/3 | STEP: 000/489 | LOSS: 0.28634 (0.28634) | TIME: 0:00:01 |
[VALID F3] EPOCH: 2/3 | STEP: 040/489 | LOSS: 0.06242 (0.10054) | TIME: 0:00:06 |
[VALID F3] EPOCH: 2/3 | STEP: 080/489 | LOSS: 0.15585 (0.10287) | TIME: 0:00:11 |
[VALID F3] EPOCH: 2/3 | STEP: 120/489 | LOSS: 0.13136 (0.10101) | TIME: 0:00:16 |
[VALID F3] EPOCH: 2/3 | STEP: 160/489 | LOSS: 0.05190 (0.10175) | TIME: 0:00:21 |
[VALID F3] EPOCH: 2/3 | STEP: 200/489 | LOSS: 0.13086 (0.09988) | TIME: 0:00:26 |
[VALID F3] EPOCH: 2/3 | STEP: 240/489 | LOSS: 0.10571 (0.09743) | TIME: 0:00:31 |
[VALID F3] EPOCH: 2/3 | STEP: 280/489 | LOSS: 0.05383 (0.10059) | TIME: 0:00:36 |
[VALID F3] EPOCH: 2/3 | STEP: 320/489 | LOSS: 0.13150 (0.10185) | TIME: 0:00:41 |
[VALID F3] EPOCH: 2/3 | STEP: 360/489 | LOSS: 0.09086 (0.10078) | TIME: 0:00:47 |
[VALID F3] EPOCH: 2/3 | STEP: 400/489 | LOSS: 0.09522 (0.10126) | TIME: 0:00:52 |
[VALID F3] EPOCH: 2/3 | STEP: 440/489 | LOSS: 0.06484 (0.10207) | TIME: 0:00:57 |
[VALID F3] EPOCH: 2/3 | STEP: 480/489 | LOSS: 0.05553 (0.10178) | TIME: 0:01:02 |
[VALID F3] EPOCH: 2/3 | STEP: 488/489 | LOSS: 0.04169 (0.10171) | TIME: 0:01:03 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10394 |      0.10171 |  0.45164 | 0.474 | 0.448 | 0.418 | 0.457 | 0.472 | 0.440 | 0:19:35 |


[SAVED] EPOCH: 2 | MCRMSE: 0.45164263248443604

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 3/3 | STEP: 0000/1466 | LOSS: 0.14491 (0.14491) | LR: 0.00000584 | TIME: 0:00:01 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0040/1466 | LOSS: 0.09151 (0.09177) | LR: 0.00000556 | TIME: 0:00:31 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0080/1466 | LOSS: 0.05728 (0.08667) | LR: 0.00000528 | TIME: 0:01:00 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0120/1466 | LOSS: 0.04507 (0.08656) | LR: 0.00000501 | TIME: 0:01:30 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0160/1466 | LOSS: 0.04998 (0.08497) | LR: 0.00000474 | TIME: 0:02:00 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0200/1466 | LOSS: 0.09333 (0.08486) | LR: 0.00000448 | TIME: 0:02:30 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0240/1466 | LOSS: 0.05238 (0.08462) | LR: 0.00000422 | TIME: 0:03:01 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0280/1466 | LOSS: 0.06406 (0.08182) | LR: 0.00000397 | TIME: 0:03:31 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0320/1466 | LOSS: 0.10639 (0.08305) | LR: 0.00000372 | TIME: 0:04:01 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0360/1466 | LOSS: 0.06169 (0.08215) | LR: 0.00000348 | TIME: 0:04:32 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0400/1466 | LOSS: 0.05860 (0.08274) | LR: 0.00000325 | TIME: 0:05:03 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0440/1466 | LOSS: 0.04726 (0.08335) | LR: 0.00000302 | TIME: 0:05:35 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0480/1466 | LOSS: 0.16886 (0.08358) | LR: 0.00000280 | TIME: 0:06:03 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0520/1466 | LOSS: 0.02000 (0.08285) | LR: 0.00000259 | TIME: 0:06:36 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0560/1466 | LOSS: 0.07219 (0.08266) | LR: 0.00000238 | TIME: 0:07:06 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0600/1466 | LOSS: 0.04102 (0.08234) | LR: 0.00000219 | TIME: 0:07:38 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0640/1466 | LOSS: 0.14322 (0.08219) | LR: 0.00000200 | TIME: 0:08:07 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0680/1466 | LOSS: 0.05683 (0.08185) | LR: 0.00000181 | TIME: 0:08:41 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0720/1466 | LOSS: 0.08379 (0.08162) | LR: 0.00000164 | TIME: 0:09:11 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0760/1466 | LOSS: 0.06033 (0.08134) | LR: 0.00000147 | TIME: 0:09:40 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0800/1466 | LOSS: 0.05670 (0.08158) | LR: 0.00000131 | TIME: 0:10:11 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0840/1466 | LOSS: 0.06358 (0.08170) | LR: 0.00000116 | TIME: 0:10:42 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0880/1466 | LOSS: 0.05550 (0.08186) | LR: 0.00000102 | TIME: 0:11:12 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0920/1466 | LOSS: 0.06957 (0.08117) | LR: 0.00000089 | TIME: 0:11:42 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0960/1466 | LOSS: 0.05366 (0.08127) | LR: 0.00000076 | TIME: 0:12:13 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1000/1466 | LOSS: 0.06102 (0.08156) | LR: 0.00000065 | TIME: 0:12:45 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1040/1466 | LOSS: 0.08164 (0.08163) | LR: 0.00000054 | TIME: 0:13:17 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1080/1466 | LOSS: 0.02294 (0.08140) | LR: 0.00000045 | TIME: 0:13:47 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1120/1466 | LOSS: 0.03020 (0.08109) | LR: 0.00000036 | TIME: 0:14:17 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1160/1466 | LOSS: 0.07206 (0.08091) | LR: 0.00000028 | TIME: 0:14:48 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1200/1466 | LOSS: 0.03649 (0.08074) | LR: 0.00000021 | TIME: 0:15:18 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1240/1466 | LOSS: 0.04667 (0.08034) | LR: 0.00000015 | TIME: 0:15:48 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1280/1466 | LOSS: 0.03742 (0.08041) | LR: 0.00000010 | TIME: 0:16:19 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1320/1466 | LOSS: 0.11753 (0.08019) | LR: 0.00000006 | TIME: 0:16:51 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1360/1466 | LOSS: 0.04051 (0.08041) | LR: 0.00000003 | TIME: 0:17:21 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1400/1466 | LOSS: 0.05264 (0.08023) | LR: 0.00000001 | TIME: 0:17:52 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1440/1466 | LOSS: 0.12012 (0.08013) | LR: 0.00000000 | TIME: 0:18:23 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1465/1466 | LOSS: 0.12546 (0.08009) | LR: 0.00000000 | TIME: 0:18:42 |

VALID_LOOP
[VALID F3] EPOCH: 3/3 | STEP: 000/489 | LOSS: 0.31391 (0.31391) | TIME: 0:00:01 |
[VALID F3] EPOCH: 3/3 | STEP: 040/489 | LOSS: 0.06138 (0.09973) | TIME: 0:00:06 |
[VALID F3] EPOCH: 3/3 | STEP: 080/489 | LOSS: 0.16068 (0.10287) | TIME: 0:00:11 |
[VALID F3] EPOCH: 3/3 | STEP: 120/489 | LOSS: 0.10292 (0.10083) | TIME: 0:00:16 |
[VALID F3] EPOCH: 3/3 | STEP: 160/489 | LOSS: 0.05833 (0.10194) | TIME: 0:00:21 |
[VALID F3] EPOCH: 3/3 | STEP: 200/489 | LOSS: 0.13421 (0.10014) | TIME: 0:00:26 |
[VALID F3] EPOCH: 3/3 | STEP: 240/489 | LOSS: 0.09899 (0.09716) | TIME: 0:00:31 |
[VALID F3] EPOCH: 3/3 | STEP: 280/489 | LOSS: 0.05131 (0.09984) | TIME: 0:00:36 |
[VALID F3] EPOCH: 3/3 | STEP: 320/489 | LOSS: 0.12047 (0.10102) | TIME: 0:00:42 |
[VALID F3] EPOCH: 3/3 | STEP: 360/489 | LOSS: 0.08437 (0.09985) | TIME: 0:00:47 |
[VALID F3] EPOCH: 3/3 | STEP: 400/489 | LOSS: 0.08470 (0.10025) | TIME: 0:00:52 |
[VALID F3] EPOCH: 3/3 | STEP: 440/489 | LOSS: 0.05941 (0.10070) | TIME: 0:00:57 |
[VALID F3] EPOCH: 3/3 | STEP: 480/489 | LOSS: 0.05343 (0.10018) | TIME: 0:01:02 |
[VALID F3] EPOCH: 3/3 | STEP: 488/489 | LOSS: 0.03627 (0.10011) | TIME: 0:01:03 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08009 |      0.10011 |  0.44804 | 0.470 | 0.443 | 0.418 | 0.449 | 0.473 | 0.435 | 0:19:45 |


[SAVED] EPOCH: 3 | MCRMSE: 0.44804105162620544


----------------------------------- FOLD 3 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.44804     0.46978   0.44315       0.41806        0.44929    0.47326         0.4347

################################### END OF FOlD 3 ###################################


