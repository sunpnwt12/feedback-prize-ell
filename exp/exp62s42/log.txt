Date: 2022-11-25 13:12:27.782435+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 4398}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 768

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 1/3 | STEP: 0000/1466 | LOSS: 0.11405 (0.11405) | LR: 0.00000005 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0040/1466 | LOSS: 0.12333 (0.13551) | LR: 0.00000224 | TIME: 0:01:12 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0080/1466 | LOSS: 0.09837 (0.12816) | LR: 0.00000443 | TIME: 0:02:36 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0120/1466 | LOSS: 0.09001 (0.12850) | LR: 0.00000661 | TIME: 0:03:55 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0160/1466 | LOSS: 0.07778 (0.12711) | LR: 0.00000880 | TIME: 0:05:17 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0200/1466 | LOSS: 0.10484 (0.12793) | LR: 0.00001098 | TIME: 0:06:43 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0240/1466 | LOSS: 0.12626 (0.12589) | LR: 0.00001317 | TIME: 0:08:04 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0280/1466 | LOSS: 0.15672 (0.12479) | LR: 0.00001536 | TIME: 0:09:23 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0320/1466 | LOSS: 0.10082 (0.12930) | LR: 0.00001754 | TIME: 0:10:43 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0360/1466 | LOSS: 0.04876 (0.12826) | LR: 0.00001973 | TIME: 0:12:03 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0400/1466 | LOSS: 0.13593 (0.13109) | LR: 0.00002000 | TIME: 0:13:20 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0440/1466 | LOSS: 0.11685 (0.13473) | LR: 0.00001998 | TIME: 0:14:43 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0480/1466 | LOSS: 0.24268 (0.13407) | LR: 0.00001996 | TIME: 0:16:09 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0520/1466 | LOSS: 0.15117 (0.13294) | LR: 0.00001993 | TIME: 0:17:29 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0560/1466 | LOSS: 0.13454 (0.13261) | LR: 0.00001988 | TIME: 0:18:53 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0600/1466 | LOSS: 0.13861 (0.13465) | LR: 0.00001983 | TIME: 0:20:16 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0640/1466 | LOSS: 0.06833 (0.13471) | LR: 0.00001977 | TIME: 0:21:37 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0680/1466 | LOSS: 0.07311 (0.13522) | LR: 0.00001970 | TIME: 0:22:55 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0720/1466 | LOSS: 0.05171 (0.13526) | LR: 0.00001962 | TIME: 0:24:16 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0760/1466 | LOSS: 0.04932 (0.13455) | LR: 0.00001953 | TIME: 0:25:34 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0800/1466 | LOSS: 0.07561 (0.13476) | LR: 0.00001943 | TIME: 0:26:53 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0840/1466 | LOSS: 0.02227 (0.13466) | LR: 0.00001932 | TIME: 0:28:09 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0880/1466 | LOSS: 0.11800 (0.13389) | LR: 0.00001921 | TIME: 0:29:25 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0920/1466 | LOSS: 0.18521 (0.13311) | LR: 0.00001908 | TIME: 0:30:42 |
[TRAIN F0] EPOCH: 1/3 | STEP: 0960/1466 | LOSS: 0.03697 (0.13242) | LR: 0.00001894 | TIME: 0:32:01 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1000/1466 | LOSS: 0.12808 (0.13204) | LR: 0.00001880 | TIME: 0:33:15 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1040/1466 | LOSS: 0.16014 (0.13240) | LR: 0.00001865 | TIME: 0:34:33 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1080/1466 | LOSS: 0.07476 (0.13202) | LR: 0.00001849 | TIME: 0:35:52 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1120/1466 | LOSS: 0.15268 (0.13256) | LR: 0.00001832 | TIME: 0:37:14 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1160/1466 | LOSS: 0.17143 (0.13202) | LR: 0.00001814 | TIME: 0:38:34 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1200/1466 | LOSS: 0.11670 (0.13191) | LR: 0.00001796 | TIME: 0:39:54 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1240/1466 | LOSS: 0.13488 (0.13173) | LR: 0.00001776 | TIME: 0:41:13 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1280/1466 | LOSS: 0.13473 (0.13191) | LR: 0.00001756 | TIME: 0:42:30 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1320/1466 | LOSS: 0.13885 (0.13197) | LR: 0.00001736 | TIME: 0:43:53 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1360/1466 | LOSS: 0.13062 (0.13165) | LR: 0.00001714 | TIME: 0:45:12 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1400/1466 | LOSS: 0.10721 (0.13122) | LR: 0.00001692 | TIME: 0:46:28 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1440/1466 | LOSS: 0.13712 (0.13108) | LR: 0.00001669 | TIME: 0:47:42 |
[TRAIN F0] EPOCH: 1/3 | STEP: 1465/1466 | LOSS: 0.10284 (0.13069) | LR: 0.00001655 | TIME: 0:48:31 |

VALID_LOOP
[VALID F0] EPOCH: 1/3 | STEP: 000/489 | LOSS: 0.08568 (0.08568) | TIME: 0:00:01 |
[VALID F0] EPOCH: 1/3 | STEP: 040/489 | LOSS: 0.08286 (0.11725) | TIME: 0:00:25 |
[VALID F0] EPOCH: 1/3 | STEP: 080/489 | LOSS: 0.13413 (0.10680) | TIME: 0:00:49 |
[VALID F0] EPOCH: 1/3 | STEP: 120/489 | LOSS: 0.04529 (0.10521) | TIME: 0:01:13 |
[VALID F0] EPOCH: 1/3 | STEP: 160/489 | LOSS: 0.14463 (0.10797) | TIME: 0:01:37 |
[VALID F0] EPOCH: 1/3 | STEP: 200/489 | LOSS: 0.09356 (0.10506) | TIME: 0:02:02 |
[VALID F0] EPOCH: 1/3 | STEP: 240/489 | LOSS: 0.08296 (0.10503) | TIME: 0:02:26 |
[VALID F0] EPOCH: 1/3 | STEP: 280/489 | LOSS: 0.09624 (0.10732) | TIME: 0:02:50 |
[VALID F0] EPOCH: 1/3 | STEP: 320/489 | LOSS: 0.07966 (0.10613) | TIME: 0:03:14 |
[VALID F0] EPOCH: 1/3 | STEP: 360/489 | LOSS: 0.07694 (0.10621) | TIME: 0:03:38 |
[VALID F0] EPOCH: 1/3 | STEP: 400/489 | LOSS: 0.14719 (0.10703) | TIME: 0:04:02 |
[VALID F0] EPOCH: 1/3 | STEP: 440/489 | LOSS: 0.10479 (0.10808) | TIME: 0:04:26 |
[VALID F0] EPOCH: 1/3 | STEP: 480/489 | LOSS: 0.11954 (0.10755) | TIME: 0:04:50 |
[VALID F0] EPOCH: 1/3 | STEP: 488/489 | LOSS: 0.14729 (0.10744) | TIME: 0:04:55 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |      0.13069 |      0.10744 |  0.46374 | 0.505 | 0.447 | 0.431 | 0.461 | 0.504 | 0.434 | 0:53:26 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4637419879436493

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 2/3 | STEP: 0000/1466 | LOSS: 0.11101 (0.11101) | LR: 0.00001654 | TIME: 0:00:02 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0040/1466 | LOSS: 0.31922 (0.10671) | LR: 0.00001630 | TIME: 0:01:20 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0080/1466 | LOSS: 0.12658 (0.10272) | LR: 0.00001606 | TIME: 0:02:36 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0120/1466 | LOSS: 0.14672 (0.10987) | LR: 0.00001581 | TIME: 0:03:54 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0160/1466 | LOSS: 0.09293 (0.11039) | LR: 0.00001555 | TIME: 0:05:13 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0200/1466 | LOSS: 0.11835 (0.11175) | LR: 0.00001529 | TIME: 0:06:36 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0240/1466 | LOSS: 0.14755 (0.11140) | LR: 0.00001502 | TIME: 0:07:51 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0280/1466 | LOSS: 0.12782 (0.11240) | LR: 0.00001475 | TIME: 0:09:08 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0320/1466 | LOSS: 0.21050 (0.11437) | LR: 0.00001447 | TIME: 0:10:22 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0360/1466 | LOSS: 0.23017 (0.11574) | LR: 0.00001419 | TIME: 0:11:45 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0400/1466 | LOSS: 0.12199 (0.11461) | LR: 0.00001391 | TIME: 0:13:03 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0440/1466 | LOSS: 0.30046 (0.11149) | LR: 0.00001362 | TIME: 0:14:14 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0480/1466 | LOSS: 0.03144 (0.11056) | LR: 0.00001332 | TIME: 0:15:38 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0520/1466 | LOSS: 0.08103 (0.10946) | LR: 0.00001303 | TIME: 0:16:58 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0560/1466 | LOSS: 0.11191 (0.10833) | LR: 0.00001273 | TIME: 0:18:15 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0600/1466 | LOSS: 0.09506 (0.10805) | LR: 0.00001243 | TIME: 0:19:42 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0640/1466 | LOSS: 0.05277 (0.10742) | LR: 0.00001213 | TIME: 0:21:06 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0680/1466 | LOSS: 0.09397 (0.10749) | LR: 0.00001182 | TIME: 0:22:32 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0720/1466 | LOSS: 0.05761 (0.10642) | LR: 0.00001151 | TIME: 0:23:48 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0760/1466 | LOSS: 0.08892 (0.10729) | LR: 0.00001120 | TIME: 0:25:10 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0800/1466 | LOSS: 0.02467 (0.10718) | LR: 0.00001089 | TIME: 0:26:31 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0840/1466 | LOSS: 0.07189 (0.10706) | LR: 0.00001058 | TIME: 0:27:56 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0880/1466 | LOSS: 0.07796 (0.10667) | LR: 0.00001027 | TIME: 0:29:18 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0920/1466 | LOSS: 0.10554 (0.10623) | LR: 0.00000996 | TIME: 0:30:39 |
[TRAIN F0] EPOCH: 2/3 | STEP: 0960/1466 | LOSS: 0.08778 (0.10578) | LR: 0.00000965 | TIME: 0:32:00 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1000/1466 | LOSS: 0.11105 (0.10533) | LR: 0.00000934 | TIME: 0:33:18 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1040/1466 | LOSS: 0.05207 (0.10537) | LR: 0.00000903 | TIME: 0:34:40 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1080/1466 | LOSS: 0.22657 (0.10561) | LR: 0.00000872 | TIME: 0:36:03 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1120/1466 | LOSS: 0.09589 (0.10592) | LR: 0.00000841 | TIME: 0:37:18 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1160/1466 | LOSS: 0.05491 (0.10573) | LR: 0.00000810 | TIME: 0:38:32 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1200/1466 | LOSS: 0.11975 (0.10536) | LR: 0.00000780 | TIME: 0:39:49 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1240/1466 | LOSS: 0.07569 (0.10571) | LR: 0.00000749 | TIME: 0:41:12 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1280/1466 | LOSS: 0.09402 (0.10577) | LR: 0.00000719 | TIME: 0:42:35 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1320/1466 | LOSS: 0.06283 (0.10550) | LR: 0.00000690 | TIME: 0:43:54 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1360/1466 | LOSS: 0.06533 (0.10522) | LR: 0.00000660 | TIME: 0:45:16 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1400/1466 | LOSS: 0.16228 (0.10510) | LR: 0.00000631 | TIME: 0:46:40 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1440/1466 | LOSS: 0.09569 (0.10500) | LR: 0.00000602 | TIME: 0:48:00 |
[TRAIN F0] EPOCH: 2/3 | STEP: 1465/1466 | LOSS: 0.05783 (0.10495) | LR: 0.00000584 | TIME: 0:48:54 |

VALID_LOOP
[VALID F0] EPOCH: 2/3 | STEP: 000/489 | LOSS: 0.07690 (0.07690) | TIME: 0:00:01 |
[VALID F0] EPOCH: 2/3 | STEP: 040/489 | LOSS: 0.06362 (0.10518) | TIME: 0:00:26 |
[VALID F0] EPOCH: 2/3 | STEP: 080/489 | LOSS: 0.10730 (0.09490) | TIME: 0:00:50 |
[VALID F0] EPOCH: 2/3 | STEP: 120/489 | LOSS: 0.05675 (0.09626) | TIME: 0:01:14 |
[VALID F0] EPOCH: 2/3 | STEP: 160/489 | LOSS: 0.12171 (0.10086) | TIME: 0:01:39 |
[VALID F0] EPOCH: 2/3 | STEP: 200/489 | LOSS: 0.11367 (0.09915) | TIME: 0:02:03 |
[VALID F0] EPOCH: 2/3 | STEP: 240/489 | LOSS: 0.08439 (0.09922) | TIME: 0:02:27 |
[VALID F0] EPOCH: 2/3 | STEP: 280/489 | LOSS: 0.08930 (0.10166) | TIME: 0:02:51 |
[VALID F0] EPOCH: 2/3 | STEP: 320/489 | LOSS: 0.07207 (0.10012) | TIME: 0:03:16 |
[VALID F0] EPOCH: 2/3 | STEP: 360/489 | LOSS: 0.07505 (0.09993) | TIME: 0:03:40 |
[VALID F0] EPOCH: 2/3 | STEP: 400/489 | LOSS: 0.14091 (0.10120) | TIME: 0:04:05 |
[VALID F0] EPOCH: 2/3 | STEP: 440/489 | LOSS: 0.11786 (0.10182) | TIME: 0:04:29 |
[VALID F0] EPOCH: 2/3 | STEP: 480/489 | LOSS: 0.09024 (0.10160) | TIME: 0:04:53 |
[VALID F0] EPOCH: 2/3 | STEP: 488/489 | LOSS: 0.11540 (0.10143) | TIME: 0:04:58 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10495 |      0.10143 |  0.45064 | 0.485 | 0.440 | 0.413 | 0.449 | 0.477 | 0.440 | 0:53:52 |


[SAVED] EPOCH: 2 | MCRMSE: 0.45064494013786316

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 3/3 | STEP: 0000/1466 | LOSS: 0.22690 (0.22690) | LR: 0.00000584 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0040/1466 | LOSS: 0.03281 (0.07528) | LR: 0.00000556 | TIME: 0:01:19 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0080/1466 | LOSS: 0.05332 (0.07485) | LR: 0.00000528 | TIME: 0:02:42 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0120/1466 | LOSS: 0.08491 (0.07703) | LR: 0.00000501 | TIME: 0:04:06 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0160/1466 | LOSS: 0.13746 (0.07768) | LR: 0.00000474 | TIME: 0:05:18 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0200/1466 | LOSS: 0.12010 (0.08103) | LR: 0.00000448 | TIME: 0:06:36 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0240/1466 | LOSS: 0.08576 (0.08186) | LR: 0.00000422 | TIME: 0:08:00 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0280/1466 | LOSS: 0.06505 (0.08257) | LR: 0.00000397 | TIME: 0:09:19 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0320/1466 | LOSS: 0.12348 (0.08147) | LR: 0.00000372 | TIME: 0:10:39 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0360/1466 | LOSS: 0.08705 (0.08338) | LR: 0.00000348 | TIME: 0:12:01 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0400/1466 | LOSS: 0.10881 (0.08370) | LR: 0.00000325 | TIME: 0:13:20 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0440/1466 | LOSS: 0.05661 (0.08266) | LR: 0.00000302 | TIME: 0:14:41 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0480/1466 | LOSS: 0.04645 (0.08262) | LR: 0.00000280 | TIME: 0:16:01 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0520/1466 | LOSS: 0.04146 (0.08180) | LR: 0.00000259 | TIME: 0:17:25 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0560/1466 | LOSS: 0.06111 (0.08261) | LR: 0.00000238 | TIME: 0:18:45 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0600/1466 | LOSS: 0.10119 (0.08357) | LR: 0.00000219 | TIME: 0:20:12 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0640/1466 | LOSS: 0.04072 (0.08311) | LR: 0.00000200 | TIME: 0:21:25 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0680/1466 | LOSS: 0.13636 (0.08265) | LR: 0.00000181 | TIME: 0:22:40 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0720/1466 | LOSS: 0.01875 (0.08328) | LR: 0.00000164 | TIME: 0:24:05 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0760/1466 | LOSS: 0.03444 (0.08301) | LR: 0.00000147 | TIME: 0:25:21 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0800/1466 | LOSS: 0.05585 (0.08309) | LR: 0.00000131 | TIME: 0:26:42 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0840/1466 | LOSS: 0.10037 (0.08327) | LR: 0.00000116 | TIME: 0:28:06 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0880/1466 | LOSS: 0.03080 (0.08340) | LR: 0.00000102 | TIME: 0:29:23 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0920/1466 | LOSS: 0.12740 (0.08358) | LR: 0.00000089 | TIME: 0:30:48 |
[TRAIN F0] EPOCH: 3/3 | STEP: 0960/1466 | LOSS: 0.06737 (0.08316) | LR: 0.00000076 | TIME: 0:32:02 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1000/1466 | LOSS: 0.13410 (0.08334) | LR: 0.00000065 | TIME: 0:33:24 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1040/1466 | LOSS: 0.06719 (0.08294) | LR: 0.00000054 | TIME: 0:34:46 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1080/1466 | LOSS: 0.05395 (0.08355) | LR: 0.00000045 | TIME: 0:36:12 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1120/1466 | LOSS: 0.16898 (0.08358) | LR: 0.00000036 | TIME: 0:37:32 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1160/1466 | LOSS: 0.02652 (0.08369) | LR: 0.00000028 | TIME: 0:38:58 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1200/1466 | LOSS: 0.07325 (0.08373) | LR: 0.00000021 | TIME: 0:40:11 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1240/1466 | LOSS: 0.06766 (0.08329) | LR: 0.00000015 | TIME: 0:41:34 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1280/1466 | LOSS: 0.03868 (0.08318) | LR: 0.00000010 | TIME: 0:43:04 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1320/1466 | LOSS: 0.05625 (0.08329) | LR: 0.00000006 | TIME: 0:44:26 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1360/1466 | LOSS: 0.10552 (0.08345) | LR: 0.00000003 | TIME: 0:45:44 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1400/1466 | LOSS: 0.06096 (0.08354) | LR: 0.00000001 | TIME: 0:47:08 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1440/1466 | LOSS: 0.02930 (0.08323) | LR: 0.00000000 | TIME: 0:48:28 |
[TRAIN F0] EPOCH: 3/3 | STEP: 1465/1466 | LOSS: 0.09490 (0.08341) | LR: 0.00000000 | TIME: 0:49:19 |

VALID_LOOP
[VALID F0] EPOCH: 3/3 | STEP: 000/489 | LOSS: 0.07560 (0.07560) | TIME: 0:00:01 |
[VALID F0] EPOCH: 3/3 | STEP: 040/489 | LOSS: 0.06100 (0.10299) | TIME: 0:00:26 |
[VALID F0] EPOCH: 3/3 | STEP: 080/489 | LOSS: 0.12646 (0.09515) | TIME: 0:00:52 |
[VALID F0] EPOCH: 3/3 | STEP: 120/489 | LOSS: 0.04416 (0.09463) | TIME: 0:01:17 |
[VALID F0] EPOCH: 3/3 | STEP: 160/489 | LOSS: 0.13612 (0.09794) | TIME: 0:01:42 |
[VALID F0] EPOCH: 3/3 | STEP: 200/489 | LOSS: 0.10765 (0.09601) | TIME: 0:02:08 |
[VALID F0] EPOCH: 3/3 | STEP: 240/489 | LOSS: 0.07845 (0.09625) | TIME: 0:02:34 |
[VALID F0] EPOCH: 3/3 | STEP: 280/489 | LOSS: 0.09240 (0.09873) | TIME: 0:02:59 |
[VALID F0] EPOCH: 3/3 | STEP: 320/489 | LOSS: 0.07639 (0.09755) | TIME: 0:03:25 |
[VALID F0] EPOCH: 3/3 | STEP: 360/489 | LOSS: 0.07489 (0.09750) | TIME: 0:03:51 |
[VALID F0] EPOCH: 3/3 | STEP: 400/489 | LOSS: 0.11899 (0.09835) | TIME: 0:04:17 |
[VALID F0] EPOCH: 3/3 | STEP: 440/489 | LOSS: 0.10002 (0.09924) | TIME: 0:04:42 |
[VALID F0] EPOCH: 3/3 | STEP: 480/489 | LOSS: 0.10005 (0.09882) | TIME: 0:05:08 |
[VALID F0] EPOCH: 3/3 | STEP: 488/489 | LOSS: 0.13801 (0.09872) | TIME: 0:05:13 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08341 |      0.09872 |  0.44457 | 0.475 | 0.439 | 0.408 | 0.449 | 0.463 | 0.433 | 0:54:33 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4445747137069702


----------------------------------- FOLD 0 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.44457     0.47491   0.43897       0.40835        0.44925    0.46302        0.43295

################################### END OF FOlD 0 ###################################


Date: 2022-11-25 15:55:25.114480+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 4401}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 768

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 1/3 | STEP: 0000/1467 | LOSS: 0.14538 (0.14538) | LR: 0.00000005 | TIME: 0:00:02 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0040/1467 | LOSS: 0.13323 (0.11838) | LR: 0.00000224 | TIME: 0:01:17 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0080/1467 | LOSS: 0.08212 (0.12520) | LR: 0.00000443 | TIME: 0:02:38 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0120/1467 | LOSS: 0.02593 (0.12446) | LR: 0.00000661 | TIME: 0:04:03 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0160/1467 | LOSS: 0.13445 (0.12168) | LR: 0.00000880 | TIME: 0:05:18 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0200/1467 | LOSS: 0.10812 (0.12262) | LR: 0.00001098 | TIME: 0:06:39 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0240/1467 | LOSS: 0.10678 (0.12567) | LR: 0.00001317 | TIME: 0:07:56 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0280/1467 | LOSS: 0.27024 (0.12926) | LR: 0.00001536 | TIME: 0:09:20 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0320/1467 | LOSS: 0.11507 (0.12985) | LR: 0.00001754 | TIME: 0:10:36 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0360/1467 | LOSS: 0.07991 (0.13069) | LR: 0.00001973 | TIME: 0:11:52 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0400/1467 | LOSS: 0.10269 (0.13188) | LR: 0.00002000 | TIME: 0:13:16 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0440/1467 | LOSS: 0.32420 (0.13420) | LR: 0.00001998 | TIME: 0:14:30 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0480/1467 | LOSS: 0.10342 (0.13452) | LR: 0.00001996 | TIME: 0:15:47 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0520/1467 | LOSS: 0.13750 (0.13700) | LR: 0.00001993 | TIME: 0:17:12 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0560/1467 | LOSS: 0.15098 (0.13689) | LR: 0.00001988 | TIME: 0:18:34 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0600/1467 | LOSS: 0.08134 (0.13538) | LR: 0.00001983 | TIME: 0:19:54 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0640/1467 | LOSS: 0.15997 (0.13566) | LR: 0.00001977 | TIME: 0:21:11 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0680/1467 | LOSS: 0.15332 (0.13563) | LR: 0.00001970 | TIME: 0:22:34 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0720/1467 | LOSS: 0.03850 (0.13494) | LR: 0.00001962 | TIME: 0:23:54 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0760/1467 | LOSS: 0.17937 (0.13465) | LR: 0.00001953 | TIME: 0:25:11 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0800/1467 | LOSS: 0.11355 (0.13326) | LR: 0.00001943 | TIME: 0:26:32 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0840/1467 | LOSS: 0.12809 (0.13338) | LR: 0.00001932 | TIME: 0:27:43 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0880/1467 | LOSS: 0.02867 (0.13360) | LR: 0.00001921 | TIME: 0:28:58 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0920/1467 | LOSS: 0.12224 (0.13269) | LR: 0.00001908 | TIME: 0:30:16 |
[TRAIN F1] EPOCH: 1/3 | STEP: 0960/1467 | LOSS: 0.08879 (0.13261) | LR: 0.00001895 | TIME: 0:31:45 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1000/1467 | LOSS: 0.08070 (0.13166) | LR: 0.00001880 | TIME: 0:33:07 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1040/1467 | LOSS: 0.20919 (0.13218) | LR: 0.00001865 | TIME: 0:34:22 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1080/1467 | LOSS: 0.13275 (0.13197) | LR: 0.00001849 | TIME: 0:35:39 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1120/1467 | LOSS: 0.14217 (0.13182) | LR: 0.00001832 | TIME: 0:36:59 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1160/1467 | LOSS: 0.10272 (0.13140) | LR: 0.00001814 | TIME: 0:38:22 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1200/1467 | LOSS: 0.09065 (0.13089) | LR: 0.00001796 | TIME: 0:39:37 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1240/1467 | LOSS: 0.20449 (0.13081) | LR: 0.00001777 | TIME: 0:40:59 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1280/1467 | LOSS: 0.11850 (0.13057) | LR: 0.00001757 | TIME: 0:42:18 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1320/1467 | LOSS: 0.10429 (0.13029) | LR: 0.00001736 | TIME: 0:43:40 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1360/1467 | LOSS: 0.26357 (0.13054) | LR: 0.00001715 | TIME: 0:44:58 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1400/1467 | LOSS: 0.08514 (0.13040) | LR: 0.00001693 | TIME: 0:46:17 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1440/1467 | LOSS: 0.17976 (0.12964) | LR: 0.00001670 | TIME: 0:47:36 |
[TRAIN F1] EPOCH: 1/3 | STEP: 1466/1467 | LOSS: 0.09460 (0.12904) | LR: 0.00001655 | TIME: 0:48:22 |

VALID_LOOP
[VALID F1] EPOCH: 1/3 | STEP: 000/489 | LOSS: 0.08261 (0.08261) | TIME: 0:00:01 |
[VALID F1] EPOCH: 1/3 | STEP: 040/489 | LOSS: 0.05436 (0.11589) | TIME: 0:00:26 |
[VALID F1] EPOCH: 1/3 | STEP: 080/489 | LOSS: 0.05741 (0.11489) | TIME: 0:00:50 |
[VALID F1] EPOCH: 1/3 | STEP: 120/489 | LOSS: 0.06739 (0.11401) | TIME: 0:01:14 |
[VALID F1] EPOCH: 1/3 | STEP: 160/489 | LOSS: 0.09143 (0.11699) | TIME: 0:01:39 |
[VALID F1] EPOCH: 1/3 | STEP: 200/489 | LOSS: 0.05000 (0.11428) | TIME: 0:02:03 |
[VALID F1] EPOCH: 1/3 | STEP: 240/489 | LOSS: 0.18998 (0.11632) | TIME: 0:02:27 |
[VALID F1] EPOCH: 1/3 | STEP: 280/489 | LOSS: 0.07316 (0.11554) | TIME: 0:02:52 |
[VALID F1] EPOCH: 1/3 | STEP: 320/489 | LOSS: 0.12714 (0.11840) | TIME: 0:03:16 |
[VALID F1] EPOCH: 1/3 | STEP: 360/489 | LOSS: 0.07169 (0.11800) | TIME: 0:03:41 |
[VALID F1] EPOCH: 1/3 | STEP: 400/489 | LOSS: 0.08120 (0.11703) | TIME: 0:04:05 |
[VALID F1] EPOCH: 1/3 | STEP: 440/489 | LOSS: 0.18646 (0.11885) | TIME: 0:04:29 |
[VALID F1] EPOCH: 1/3 | STEP: 480/489 | LOSS: 0.11067 (0.11914) | TIME: 0:04:54 |
[VALID F1] EPOCH: 1/3 | STEP: 488/489 | LOSS: 0.06963 (0.11894) | TIME: 0:04:58 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |      0.12904 |      0.11894 |  0.48902 | 0.511 | 0.509 | 0.482 | 0.459 | 0.514 | 0.460 | 0:53:20 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4890206754207611

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 2/3 | STEP: 0000/1467 | LOSS: 0.15170 (0.15170) | LR: 0.00001654 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0040/1467 | LOSS: 0.08196 (0.12109) | LR: 0.00001630 | TIME: 0:01:22 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0080/1467 | LOSS: 0.07399 (0.11209) | LR: 0.00001606 | TIME: 0:02:35 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0120/1467 | LOSS: 0.13958 (0.11234) | LR: 0.00001581 | TIME: 0:03:55 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0160/1467 | LOSS: 0.21380 (0.11411) | LR: 0.00001555 | TIME: 0:05:11 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0200/1467 | LOSS: 0.06415 (0.11241) | LR: 0.00001529 | TIME: 0:06:29 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0240/1467 | LOSS: 0.03109 (0.10990) | LR: 0.00001502 | TIME: 0:07:48 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0280/1467 | LOSS: 0.05719 (0.10951) | LR: 0.00001475 | TIME: 0:09:17 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0320/1467 | LOSS: 0.17728 (0.10793) | LR: 0.00001447 | TIME: 0:10:32 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0360/1467 | LOSS: 0.05012 (0.10753) | LR: 0.00001419 | TIME: 0:11:50 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0400/1467 | LOSS: 0.07607 (0.10667) | LR: 0.00001391 | TIME: 0:13:05 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0440/1467 | LOSS: 0.03556 (0.10720) | LR: 0.00001362 | TIME: 0:14:25 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0480/1467 | LOSS: 0.04907 (0.10617) | LR: 0.00001333 | TIME: 0:15:36 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0520/1467 | LOSS: 0.07984 (0.10521) | LR: 0.00001303 | TIME: 0:16:59 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0560/1467 | LOSS: 0.05131 (0.10584) | LR: 0.00001273 | TIME: 0:18:20 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0600/1467 | LOSS: 0.15371 (0.10700) | LR: 0.00001243 | TIME: 0:19:48 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0640/1467 | LOSS: 0.15028 (0.10795) | LR: 0.00001213 | TIME: 0:21:12 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0680/1467 | LOSS: 0.13146 (0.10755) | LR: 0.00001182 | TIME: 0:22:33 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0720/1467 | LOSS: 0.08167 (0.10728) | LR: 0.00001152 | TIME: 0:23:47 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0760/1467 | LOSS: 0.10573 (0.10724) | LR: 0.00001121 | TIME: 0:25:10 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0800/1467 | LOSS: 0.03972 (0.10667) | LR: 0.00001090 | TIME: 0:26:33 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0840/1467 | LOSS: 0.05509 (0.10643) | LR: 0.00001059 | TIME: 0:27:52 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0880/1467 | LOSS: 0.04411 (0.10692) | LR: 0.00001028 | TIME: 0:29:17 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0920/1467 | LOSS: 0.12612 (0.10652) | LR: 0.00000996 | TIME: 0:30:32 |
[TRAIN F1] EPOCH: 2/3 | STEP: 0960/1467 | LOSS: 0.13815 (0.10620) | LR: 0.00000965 | TIME: 0:31:49 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1000/1467 | LOSS: 0.08674 (0.10573) | LR: 0.00000934 | TIME: 0:33:17 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1040/1467 | LOSS: 0.25020 (0.10549) | LR: 0.00000903 | TIME: 0:34:35 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1080/1467 | LOSS: 0.09400 (0.10539) | LR: 0.00000872 | TIME: 0:35:58 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1120/1467 | LOSS: 0.05509 (0.10520) | LR: 0.00000841 | TIME: 0:37:14 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1160/1467 | LOSS: 0.09543 (0.10529) | LR: 0.00000811 | TIME: 0:38:31 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1200/1467 | LOSS: 0.08376 (0.10511) | LR: 0.00000780 | TIME: 0:39:52 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1240/1467 | LOSS: 0.07963 (0.10507) | LR: 0.00000750 | TIME: 0:41:10 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1280/1467 | LOSS: 0.06626 (0.10503) | LR: 0.00000720 | TIME: 0:42:29 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1320/1467 | LOSS: 0.04764 (0.10442) | LR: 0.00000690 | TIME: 0:43:48 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1360/1467 | LOSS: 0.07685 (0.10403) | LR: 0.00000661 | TIME: 0:45:00 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1400/1467 | LOSS: 0.10060 (0.10340) | LR: 0.00000632 | TIME: 0:46:17 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1440/1467 | LOSS: 0.11901 (0.10336) | LR: 0.00000603 | TIME: 0:47:36 |
[TRAIN F1] EPOCH: 2/3 | STEP: 1466/1467 | LOSS: 0.09458 (0.10345) | LR: 0.00000584 | TIME: 0:48:26 |

VALID_LOOP
[VALID F1] EPOCH: 2/3 | STEP: 000/489 | LOSS: 0.08147 (0.08147) | TIME: 0:00:01 |
[VALID F1] EPOCH: 2/3 | STEP: 040/489 | LOSS: 0.07589 (0.10142) | TIME: 0:00:26 |
[VALID F1] EPOCH: 2/3 | STEP: 080/489 | LOSS: 0.04318 (0.10312) | TIME: 0:00:50 |
[VALID F1] EPOCH: 2/3 | STEP: 120/489 | LOSS: 0.07998 (0.10406) | TIME: 0:01:15 |
[VALID F1] EPOCH: 2/3 | STEP: 160/489 | LOSS: 0.06266 (0.10848) | TIME: 0:01:39 |
[VALID F1] EPOCH: 2/3 | STEP: 200/489 | LOSS: 0.05634 (0.10463) | TIME: 0:02:04 |
[VALID F1] EPOCH: 2/3 | STEP: 240/489 | LOSS: 0.12675 (0.10557) | TIME: 0:02:29 |
[VALID F1] EPOCH: 2/3 | STEP: 280/489 | LOSS: 0.07682 (0.10475) | TIME: 0:02:53 |
[VALID F1] EPOCH: 2/3 | STEP: 320/489 | LOSS: 0.08524 (0.10598) | TIME: 0:03:18 |
[VALID F1] EPOCH: 2/3 | STEP: 360/489 | LOSS: 0.07974 (0.10463) | TIME: 0:03:42 |
[VALID F1] EPOCH: 2/3 | STEP: 400/489 | LOSS: 0.09907 (0.10368) | TIME: 0:04:07 |
[VALID F1] EPOCH: 2/3 | STEP: 440/489 | LOSS: 0.13899 (0.10490) | TIME: 0:04:32 |
[VALID F1] EPOCH: 2/3 | STEP: 480/489 | LOSS: 0.09633 (0.10545) | TIME: 0:04:56 |
[VALID F1] EPOCH: 2/3 | STEP: 488/489 | LOSS: 0.05173 (0.10519) | TIME: 0:05:01 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10345 |      0.10519 |  0.45979 | 0.482 | 0.459 | 0.427 | 0.466 | 0.472 | 0.452 | 0:53:28 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4597877562046051

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 3/3 | STEP: 0000/1467 | LOSS: 0.02657 (0.02657) | LR: 0.00000584 | TIME: 0:00:02 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0040/1467 | LOSS: 0.05327 (0.06539) | LR: 0.00000556 | TIME: 0:01:28 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0080/1467 | LOSS: 0.08078 (0.07841) | LR: 0.00000528 | TIME: 0:02:38 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0120/1467 | LOSS: 0.11942 (0.08173) | LR: 0.00000501 | TIME: 0:04:01 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0160/1467 | LOSS: 0.13079 (0.08529) | LR: 0.00000474 | TIME: 0:05:16 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0200/1467 | LOSS: 0.04918 (0.08252) | LR: 0.00000448 | TIME: 0:06:32 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0240/1467 | LOSS: 0.07750 (0.08201) | LR: 0.00000422 | TIME: 0:07:49 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0280/1467 | LOSS: 0.11690 (0.08306) | LR: 0.00000397 | TIME: 0:09:03 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0320/1467 | LOSS: 0.14174 (0.08340) | LR: 0.00000372 | TIME: 0:10:30 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0360/1467 | LOSS: 0.02927 (0.08208) | LR: 0.00000348 | TIME: 0:11:52 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0400/1467 | LOSS: 0.07252 (0.08240) | LR: 0.00000325 | TIME: 0:13:10 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0440/1467 | LOSS: 0.05163 (0.08157) | LR: 0.00000302 | TIME: 0:14:28 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0480/1467 | LOSS: 0.08120 (0.08161) | LR: 0.00000280 | TIME: 0:15:48 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0520/1467 | LOSS: 0.09388 (0.08172) | LR: 0.00000259 | TIME: 0:17:10 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0560/1467 | LOSS: 0.09178 (0.08208) | LR: 0.00000239 | TIME: 0:18:33 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0600/1467 | LOSS: 0.09620 (0.08212) | LR: 0.00000219 | TIME: 0:19:48 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0640/1467 | LOSS: 0.01815 (0.08248) | LR: 0.00000200 | TIME: 0:21:14 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0680/1467 | LOSS: 0.04712 (0.08221) | LR: 0.00000181 | TIME: 0:22:28 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0720/1467 | LOSS: 0.05378 (0.08265) | LR: 0.00000164 | TIME: 0:23:47 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0760/1467 | LOSS: 0.07952 (0.08262) | LR: 0.00000147 | TIME: 0:25:06 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0800/1467 | LOSS: 0.08741 (0.08273) | LR: 0.00000131 | TIME: 0:26:25 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0840/1467 | LOSS: 0.06841 (0.08305) | LR: 0.00000116 | TIME: 0:27:44 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0880/1467 | LOSS: 0.12178 (0.08296) | LR: 0.00000102 | TIME: 0:29:10 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0920/1467 | LOSS: 0.05803 (0.08334) | LR: 0.00000089 | TIME: 0:30:29 |
[TRAIN F1] EPOCH: 3/3 | STEP: 0960/1467 | LOSS: 0.17978 (0.08320) | LR: 0.00000077 | TIME: 0:31:48 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1000/1467 | LOSS: 0.07093 (0.08316) | LR: 0.00000065 | TIME: 0:33:06 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1040/1467 | LOSS: 0.07091 (0.08294) | LR: 0.00000055 | TIME: 0:34:22 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1080/1467 | LOSS: 0.05079 (0.08286) | LR: 0.00000045 | TIME: 0:35:45 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1120/1467 | LOSS: 0.07587 (0.08297) | LR: 0.00000036 | TIME: 0:37:06 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1160/1467 | LOSS: 0.09542 (0.08240) | LR: 0.00000028 | TIME: 0:38:17 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1200/1467 | LOSS: 0.07991 (0.08228) | LR: 0.00000021 | TIME: 0:39:38 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1240/1467 | LOSS: 0.06071 (0.08223) | LR: 0.00000015 | TIME: 0:41:00 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1280/1467 | LOSS: 0.15616 (0.08248) | LR: 0.00000010 | TIME: 0:42:25 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1320/1467 | LOSS: 0.09978 (0.08233) | LR: 0.00000006 | TIME: 0:43:42 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1360/1467 | LOSS: 0.07243 (0.08247) | LR: 0.00000003 | TIME: 0:45:03 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1400/1467 | LOSS: 0.13274 (0.08253) | LR: 0.00000001 | TIME: 0:46:22 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1440/1467 | LOSS: 0.02879 (0.08263) | LR: 0.00000000 | TIME: 0:47:42 |
[TRAIN F1] EPOCH: 3/3 | STEP: 1466/1467 | LOSS: 0.07399 (0.08252) | LR: 0.00000000 | TIME: 0:48:31 |

VALID_LOOP
[VALID F1] EPOCH: 3/3 | STEP: 000/489 | LOSS: 0.06734 (0.06734) | TIME: 0:00:01 |
[VALID F1] EPOCH: 3/3 | STEP: 040/489 | LOSS: 0.08193 (0.09713) | TIME: 0:00:26 |
[VALID F1] EPOCH: 3/3 | STEP: 080/489 | LOSS: 0.04009 (0.09986) | TIME: 0:00:50 |
[VALID F1] EPOCH: 3/3 | STEP: 120/489 | LOSS: 0.09312 (0.10106) | TIME: 0:01:14 |
[VALID F1] EPOCH: 3/3 | STEP: 160/489 | LOSS: 0.06684 (0.10531) | TIME: 0:01:39 |
[VALID F1] EPOCH: 3/3 | STEP: 200/489 | LOSS: 0.06755 (0.10204) | TIME: 0:02:03 |
[VALID F1] EPOCH: 3/3 | STEP: 240/489 | LOSS: 0.13496 (0.10336) | TIME: 0:02:27 |
[VALID F1] EPOCH: 3/3 | STEP: 280/489 | LOSS: 0.08148 (0.10251) | TIME: 0:02:51 |
[VALID F1] EPOCH: 3/3 | STEP: 320/489 | LOSS: 0.07280 (0.10358) | TIME: 0:03:16 |
[VALID F1] EPOCH: 3/3 | STEP: 360/489 | LOSS: 0.07253 (0.10214) | TIME: 0:03:40 |
[VALID F1] EPOCH: 3/3 | STEP: 400/489 | LOSS: 0.09550 (0.10096) | TIME: 0:04:04 |
[VALID F1] EPOCH: 3/3 | STEP: 440/489 | LOSS: 0.13465 (0.10189) | TIME: 0:04:29 |
[VALID F1] EPOCH: 3/3 | STEP: 480/489 | LOSS: 0.08077 (0.10231) | TIME: 0:04:53 |
[VALID F1] EPOCH: 3/3 | STEP: 488/489 | LOSS: 0.04562 (0.10199) | TIME: 0:04:58 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08252 |      0.10199 |  0.45244 | 0.480 | 0.441 | 0.423 | 0.456 | 0.469 | 0.447 | 0:53:29 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4524412155151367


----------------------------------- FOLD 1 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45244     0.47978   0.44069       0.42293         0.4555    0.46904        0.44671

################################### END OF FOlD 1 ###################################


Date: 2022-11-25 18:36:43.042582+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 4398}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 768

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 1/3 | STEP: 0000/1466 | LOSS: 0.15405 (0.15405) | LR: 0.00000005 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0040/1466 | LOSS: 0.09181 (0.12804) | LR: 0.00000224 | TIME: 0:01:28 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0080/1466 | LOSS: 0.07082 (0.11536) | LR: 0.00000443 | TIME: 0:02:53 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0120/1466 | LOSS: 0.05012 (0.11985) | LR: 0.00000661 | TIME: 0:04:11 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0160/1466 | LOSS: 0.08670 (0.12004) | LR: 0.00000880 | TIME: 0:05:30 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0200/1466 | LOSS: 0.11075 (0.12105) | LR: 0.00001098 | TIME: 0:06:43 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0240/1466 | LOSS: 0.18353 (0.12344) | LR: 0.00001317 | TIME: 0:08:00 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0280/1466 | LOSS: 0.08309 (0.12317) | LR: 0.00001536 | TIME: 0:09:24 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0320/1466 | LOSS: 0.20877 (0.12474) | LR: 0.00001754 | TIME: 0:10:46 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0360/1466 | LOSS: 0.08290 (0.12548) | LR: 0.00001973 | TIME: 0:12:06 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0400/1466 | LOSS: 0.20021 (0.12722) | LR: 0.00002000 | TIME: 0:13:30 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0440/1466 | LOSS: 0.12489 (0.12661) | LR: 0.00001998 | TIME: 0:14:50 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0480/1466 | LOSS: 0.13047 (0.12948) | LR: 0.00001996 | TIME: 0:16:16 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0520/1466 | LOSS: 0.09628 (0.12932) | LR: 0.00001993 | TIME: 0:17:39 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0560/1466 | LOSS: 0.12894 (0.12988) | LR: 0.00001988 | TIME: 0:18:59 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0600/1466 | LOSS: 0.10382 (0.12984) | LR: 0.00001983 | TIME: 0:20:20 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0640/1466 | LOSS: 0.11551 (0.12999) | LR: 0.00001977 | TIME: 0:21:45 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0680/1466 | LOSS: 0.04505 (0.12980) | LR: 0.00001970 | TIME: 0:23:03 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0720/1466 | LOSS: 0.07224 (0.13041) | LR: 0.00001962 | TIME: 0:24:25 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0760/1466 | LOSS: 0.11310 (0.13014) | LR: 0.00001953 | TIME: 0:25:41 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0800/1466 | LOSS: 0.11891 (0.13016) | LR: 0.00001943 | TIME: 0:26:58 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0840/1466 | LOSS: 0.07652 (0.13030) | LR: 0.00001932 | TIME: 0:28:11 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0880/1466 | LOSS: 0.08609 (0.12982) | LR: 0.00001921 | TIME: 0:29:29 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0920/1466 | LOSS: 0.17994 (0.12904) | LR: 0.00001908 | TIME: 0:30:53 |
[TRAIN F2] EPOCH: 1/3 | STEP: 0960/1466 | LOSS: 0.10591 (0.12814) | LR: 0.00001894 | TIME: 0:32:09 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1000/1466 | LOSS: 0.13006 (0.12826) | LR: 0.00001880 | TIME: 0:33:28 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1040/1466 | LOSS: 0.17158 (0.12847) | LR: 0.00001865 | TIME: 0:34:46 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1080/1466 | LOSS: 0.13404 (0.12830) | LR: 0.00001849 | TIME: 0:36:07 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1120/1466 | LOSS: 0.05230 (0.12860) | LR: 0.00001832 | TIME: 0:37:23 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1160/1466 | LOSS: 0.26289 (0.12889) | LR: 0.00001814 | TIME: 0:38:48 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1200/1466 | LOSS: 0.11034 (0.12932) | LR: 0.00001796 | TIME: 0:40:11 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1240/1466 | LOSS: 0.15143 (0.12936) | LR: 0.00001776 | TIME: 0:41:30 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1280/1466 | LOSS: 0.25658 (0.12887) | LR: 0.00001756 | TIME: 0:42:55 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1320/1466 | LOSS: 0.25058 (0.12903) | LR: 0.00001736 | TIME: 0:44:15 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1360/1466 | LOSS: 0.06009 (0.12880) | LR: 0.00001714 | TIME: 0:45:37 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1400/1466 | LOSS: 0.10373 (0.12881) | LR: 0.00001692 | TIME: 0:47:02 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1440/1466 | LOSS: 0.09422 (0.12869) | LR: 0.00001669 | TIME: 0:48:25 |
[TRAIN F2] EPOCH: 1/3 | STEP: 1465/1466 | LOSS: 0.07618 (0.12876) | LR: 0.00001655 | TIME: 0:49:25 |

VALID_LOOP
[VALID F2] EPOCH: 1/3 | STEP: 000/489 | LOSS: 0.14274 (0.14274) | TIME: 0:00:01 |
[VALID F2] EPOCH: 1/3 | STEP: 040/489 | LOSS: 0.08465 (0.12102) | TIME: 0:00:26 |
[VALID F2] EPOCH: 1/3 | STEP: 080/489 | LOSS: 0.08619 (0.13785) | TIME: 0:00:51 |
[VALID F2] EPOCH: 1/3 | STEP: 120/489 | LOSS: 0.06228 (0.13430) | TIME: 0:01:16 |
[VALID F2] EPOCH: 1/3 | STEP: 160/489 | LOSS: 0.07992 (0.13363) | TIME: 0:01:41 |
[VALID F2] EPOCH: 1/3 | STEP: 200/489 | LOSS: 0.07771 (0.13186) | TIME: 0:02:06 |
[VALID F2] EPOCH: 1/3 | STEP: 240/489 | LOSS: 0.36726 (0.13143) | TIME: 0:02:31 |
[VALID F2] EPOCH: 1/3 | STEP: 280/489 | LOSS: 0.32384 (0.13197) | TIME: 0:02:55 |
[VALID F2] EPOCH: 1/3 | STEP: 320/489 | LOSS: 0.09279 (0.12853) | TIME: 0:03:20 |
[VALID F2] EPOCH: 1/3 | STEP: 360/489 | LOSS: 0.13713 (0.12575) | TIME: 0:03:45 |
[VALID F2] EPOCH: 1/3 | STEP: 400/489 | LOSS: 0.12026 (0.12681) | TIME: 0:04:10 |
[VALID F2] EPOCH: 1/3 | STEP: 440/489 | LOSS: 0.07370 (0.12749) | TIME: 0:04:35 |
[VALID F2] EPOCH: 1/3 | STEP: 480/489 | LOSS: 0.20029 (0.12846) | TIME: 0:05:00 |
[VALID F2] EPOCH: 1/3 | STEP: 488/489 | LOSS: 0.07243 (0.12792) | TIME: 0:05:05 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |      0.12876 |      0.12792 |  0.50748 | 0.543 | 0.504 | 0.560 | 0.499 | 0.479 | 0.460 | 0:54:31 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5074835419654846

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 2/3 | STEP: 0000/1466 | LOSS: 0.04314 (0.04314) | LR: 0.00001654 | TIME: 0:00:04 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0040/1466 | LOSS: 0.12359 (0.11064) | LR: 0.00001630 | TIME: 0:01:31 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0080/1466 | LOSS: 0.10851 (0.11096) | LR: 0.00001606 | TIME: 0:02:49 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0120/1466 | LOSS: 0.30310 (0.11513) | LR: 0.00001581 | TIME: 0:04:18 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0160/1466 | LOSS: 0.10452 (0.11419) | LR: 0.00001555 | TIME: 0:05:38 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0200/1466 | LOSS: 0.06393 (0.11362) | LR: 0.00001529 | TIME: 0:07:01 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0240/1466 | LOSS: 0.09612 (0.11416) | LR: 0.00001502 | TIME: 0:08:18 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0280/1466 | LOSS: 0.12252 (0.11206) | LR: 0.00001475 | TIME: 0:09:41 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0320/1466 | LOSS: 0.09468 (0.11037) | LR: 0.00001447 | TIME: 0:10:58 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0360/1466 | LOSS: 0.13642 (0.11045) | LR: 0.00001419 | TIME: 0:12:21 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0400/1466 | LOSS: 0.08366 (0.11082) | LR: 0.00001391 | TIME: 0:13:43 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0440/1466 | LOSS: 0.07552 (0.11020) | LR: 0.00001362 | TIME: 0:15:00 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0480/1466 | LOSS: 0.06397 (0.11059) | LR: 0.00001332 | TIME: 0:16:19 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0520/1466 | LOSS: 0.11102 (0.10997) | LR: 0.00001303 | TIME: 0:17:41 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0560/1466 | LOSS: 0.17812 (0.11060) | LR: 0.00001273 | TIME: 0:19:08 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0600/1466 | LOSS: 0.13677 (0.11063) | LR: 0.00001243 | TIME: 0:20:33 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0640/1466 | LOSS: 0.32920 (0.11010) | LR: 0.00001213 | TIME: 0:22:02 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0680/1466 | LOSS: 0.07401 (0.10947) | LR: 0.00001182 | TIME: 0:23:24 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0720/1466 | LOSS: 0.07130 (0.10962) | LR: 0.00001151 | TIME: 0:24:50 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0760/1466 | LOSS: 0.06394 (0.10886) | LR: 0.00001120 | TIME: 0:26:10 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0800/1466 | LOSS: 0.15747 (0.10825) | LR: 0.00001089 | TIME: 0:27:28 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0840/1466 | LOSS: 0.05828 (0.10799) | LR: 0.00001058 | TIME: 0:28:53 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0880/1466 | LOSS: 0.11517 (0.10774) | LR: 0.00001027 | TIME: 0:30:10 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0920/1466 | LOSS: 0.08859 (0.10700) | LR: 0.00000996 | TIME: 0:31:32 |
[TRAIN F2] EPOCH: 2/3 | STEP: 0960/1466 | LOSS: 0.14262 (0.10691) | LR: 0.00000965 | TIME: 0:32:56 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1000/1466 | LOSS: 0.19829 (0.10652) | LR: 0.00000934 | TIME: 0:34:14 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1040/1466 | LOSS: 0.12432 (0.10611) | LR: 0.00000903 | TIME: 0:35:32 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1080/1466 | LOSS: 0.07284 (0.10550) | LR: 0.00000872 | TIME: 0:36:51 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1120/1466 | LOSS: 0.08643 (0.10510) | LR: 0.00000841 | TIME: 0:38:06 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1160/1466 | LOSS: 0.22001 (0.10500) | LR: 0.00000810 | TIME: 0:39:28 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1200/1466 | LOSS: 0.04265 (0.10471) | LR: 0.00000780 | TIME: 0:40:51 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1240/1466 | LOSS: 0.08583 (0.10444) | LR: 0.00000749 | TIME: 0:42:13 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1280/1466 | LOSS: 0.09795 (0.10453) | LR: 0.00000719 | TIME: 0:43:35 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1320/1466 | LOSS: 0.12340 (0.10432) | LR: 0.00000690 | TIME: 0:44:52 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1360/1466 | LOSS: 0.10973 (0.10390) | LR: 0.00000660 | TIME: 0:46:17 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1400/1466 | LOSS: 0.13607 (0.10432) | LR: 0.00000631 | TIME: 0:47:36 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1440/1466 | LOSS: 0.07730 (0.10439) | LR: 0.00000602 | TIME: 0:48:52 |
[TRAIN F2] EPOCH: 2/3 | STEP: 1465/1466 | LOSS: 0.06944 (0.10415) | LR: 0.00000584 | TIME: 0:49:43 |

VALID_LOOP
[VALID F2] EPOCH: 2/3 | STEP: 000/489 | LOSS: 0.22543 (0.22543) | TIME: 0:00:01 |
[VALID F2] EPOCH: 2/3 | STEP: 040/489 | LOSS: 0.09478 (0.10650) | TIME: 0:00:26 |
[VALID F2] EPOCH: 2/3 | STEP: 080/489 | LOSS: 0.12897 (0.11892) | TIME: 0:00:51 |
[VALID F2] EPOCH: 2/3 | STEP: 120/489 | LOSS: 0.04487 (0.11293) | TIME: 0:01:15 |
[VALID F2] EPOCH: 2/3 | STEP: 160/489 | LOSS: 0.05520 (0.11127) | TIME: 0:01:40 |
[VALID F2] EPOCH: 2/3 | STEP: 200/489 | LOSS: 0.12688 (0.11101) | TIME: 0:02:05 |
[VALID F2] EPOCH: 2/3 | STEP: 240/489 | LOSS: 0.36191 (0.11245) | TIME: 0:02:30 |
[VALID F2] EPOCH: 2/3 | STEP: 280/489 | LOSS: 0.17015 (0.11276) | TIME: 0:02:54 |
[VALID F2] EPOCH: 2/3 | STEP: 320/489 | LOSS: 0.07850 (0.11064) | TIME: 0:03:19 |
[VALID F2] EPOCH: 2/3 | STEP: 360/489 | LOSS: 0.15849 (0.11058) | TIME: 0:03:44 |
[VALID F2] EPOCH: 2/3 | STEP: 400/489 | LOSS: 0.10027 (0.11186) | TIME: 0:04:09 |
[VALID F2] EPOCH: 2/3 | STEP: 440/489 | LOSS: 0.05066 (0.11217) | TIME: 0:04:33 |
[VALID F2] EPOCH: 2/3 | STEP: 480/489 | LOSS: 0.17945 (0.11190) | TIME: 0:04:58 |
[VALID F2] EPOCH: 2/3 | STEP: 488/489 | LOSS: 0.14675 (0.11154) | TIME: 0:05:03 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10415 |      0.11154 |  0.47257 | 0.477 | 0.450 | 0.425 | 0.474 | 0.552 | 0.457 | 0:54:46 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4725728929042816

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 3/3 | STEP: 0000/1466 | LOSS: 0.19810 (0.19810) | LR: 0.00000584 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0040/1466 | LOSS: 0.12139 (0.08780) | LR: 0.00000556 | TIME: 0:01:26 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0080/1466 | LOSS: 0.09612 (0.08616) | LR: 0.00000528 | TIME: 0:02:43 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0120/1466 | LOSS: 0.04539 (0.08391) | LR: 0.00000501 | TIME: 0:04:01 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0160/1466 | LOSS: 0.25308 (0.08349) | LR: 0.00000474 | TIME: 0:05:22 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0200/1466 | LOSS: 0.04837 (0.08276) | LR: 0.00000448 | TIME: 0:06:46 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0240/1466 | LOSS: 0.11404 (0.08419) | LR: 0.00000422 | TIME: 0:08:05 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0280/1466 | LOSS: 0.10893 (0.08296) | LR: 0.00000397 | TIME: 0:09:27 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0320/1466 | LOSS: 0.10769 (0.08121) | LR: 0.00000372 | TIME: 0:10:47 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0360/1466 | LOSS: 0.10855 (0.08177) | LR: 0.00000348 | TIME: 0:12:12 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0400/1466 | LOSS: 0.01961 (0.08169) | LR: 0.00000325 | TIME: 0:13:38 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0440/1466 | LOSS: 0.06147 (0.08136) | LR: 0.00000302 | TIME: 0:15:00 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0480/1466 | LOSS: 0.06605 (0.08163) | LR: 0.00000280 | TIME: 0:16:21 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0520/1466 | LOSS: 0.13472 (0.08161) | LR: 0.00000259 | TIME: 0:17:40 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0560/1466 | LOSS: 0.04387 (0.08151) | LR: 0.00000238 | TIME: 0:19:03 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0600/1466 | LOSS: 0.12108 (0.08150) | LR: 0.00000219 | TIME: 0:20:36 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0640/1466 | LOSS: 0.08202 (0.08085) | LR: 0.00000200 | TIME: 0:21:50 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0680/1466 | LOSS: 0.06160 (0.08088) | LR: 0.00000181 | TIME: 0:23:10 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0720/1466 | LOSS: 0.02296 (0.08104) | LR: 0.00000164 | TIME: 0:24:33 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0760/1466 | LOSS: 0.09353 (0.08097) | LR: 0.00000147 | TIME: 0:25:55 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0800/1466 | LOSS: 0.10807 (0.08060) | LR: 0.00000131 | TIME: 0:27:08 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0840/1466 | LOSS: 0.09161 (0.08120) | LR: 0.00000116 | TIME: 0:28:32 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0880/1466 | LOSS: 0.05385 (0.08181) | LR: 0.00000102 | TIME: 0:29:55 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0920/1466 | LOSS: 0.09658 (0.08187) | LR: 0.00000089 | TIME: 0:31:15 |
[TRAIN F2] EPOCH: 3/3 | STEP: 0960/1466 | LOSS: 0.11734 (0.08160) | LR: 0.00000076 | TIME: 0:32:37 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1000/1466 | LOSS: 0.08992 (0.08155) | LR: 0.00000065 | TIME: 0:33:54 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1040/1466 | LOSS: 0.09269 (0.08185) | LR: 0.00000054 | TIME: 0:35:20 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1080/1466 | LOSS: 0.07361 (0.08212) | LR: 0.00000045 | TIME: 0:36:44 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1120/1466 | LOSS: 0.07586 (0.08230) | LR: 0.00000036 | TIME: 0:38:03 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1160/1466 | LOSS: 0.05999 (0.08221) | LR: 0.00000028 | TIME: 0:39:28 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1200/1466 | LOSS: 0.02920 (0.08224) | LR: 0.00000021 | TIME: 0:40:54 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1240/1466 | LOSS: 0.03623 (0.08206) | LR: 0.00000015 | TIME: 0:42:07 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1280/1466 | LOSS: 0.02563 (0.08201) | LR: 0.00000010 | TIME: 0:43:32 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1320/1466 | LOSS: 0.13559 (0.08196) | LR: 0.00000006 | TIME: 0:44:55 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1360/1466 | LOSS: 0.04156 (0.08202) | LR: 0.00000003 | TIME: 0:46:20 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1400/1466 | LOSS: 0.10998 (0.08185) | LR: 0.00000001 | TIME: 0:47:40 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1440/1466 | LOSS: 0.03796 (0.08183) | LR: 0.00000000 | TIME: 0:49:02 |
[TRAIN F2] EPOCH: 3/3 | STEP: 1465/1466 | LOSS: 0.04134 (0.08172) | LR: 0.00000000 | TIME: 0:49:51 |

VALID_LOOP
[VALID F2] EPOCH: 3/3 | STEP: 000/489 | LOSS: 0.19147 (0.19147) | TIME: 0:00:01 |
[VALID F2] EPOCH: 3/3 | STEP: 040/489 | LOSS: 0.09053 (0.09964) | TIME: 0:00:26 |
[VALID F2] EPOCH: 3/3 | STEP: 080/489 | LOSS: 0.08274 (0.11056) | TIME: 0:00:51 |
[VALID F2] EPOCH: 3/3 | STEP: 120/489 | LOSS: 0.06069 (0.10637) | TIME: 0:01:16 |
[VALID F2] EPOCH: 3/3 | STEP: 160/489 | LOSS: 0.04201 (0.10464) | TIME: 0:01:41 |
[VALID F2] EPOCH: 3/3 | STEP: 200/489 | LOSS: 0.10445 (0.10417) | TIME: 0:02:06 |
[VALID F2] EPOCH: 3/3 | STEP: 240/489 | LOSS: 0.39505 (0.10506) | TIME: 0:02:31 |
[VALID F2] EPOCH: 3/3 | STEP: 280/489 | LOSS: 0.19288 (0.10512) | TIME: 0:02:56 |
[VALID F2] EPOCH: 3/3 | STEP: 320/489 | LOSS: 0.05425 (0.10243) | TIME: 0:03:20 |
[VALID F2] EPOCH: 3/3 | STEP: 360/489 | LOSS: 0.18818 (0.10162) | TIME: 0:03:45 |
[VALID F2] EPOCH: 3/3 | STEP: 400/489 | LOSS: 0.10820 (0.10297) | TIME: 0:04:10 |
[VALID F2] EPOCH: 3/3 | STEP: 440/489 | LOSS: 0.04102 (0.10339) | TIME: 0:04:35 |
[VALID F2] EPOCH: 3/3 | STEP: 480/489 | LOSS: 0.14960 (0.10361) | TIME: 0:05:01 |
[VALID F2] EPOCH: 3/3 | STEP: 488/489 | LOSS: 0.09780 (0.10320) | TIME: 0:05:06 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08172 |       0.1032 |  0.45539 | 0.476 | 0.447 | 0.415 | 0.466 | 0.474 | 0.454 | 0:54:57 |


[SAVED] EPOCH: 3 | MCRMSE: 0.45539045333862305


----------------------------------- FOLD 2 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45539     0.47601    0.4469       0.41465        0.46636    0.47395        0.45447

################################### END OF FOlD 2 ###################################


Date: 2022-11-25 22:25:33.101773+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-large
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 1e-05, LowerLayer: 2e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 366, 'num_training_steps': 4398}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 2 (Gradient_accumulate: 1)
max_len: 768

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 1/3 | STEP: 0000/1466 | LOSS: 0.07540 (0.07540) | LR: 0.00000005 | TIME: 0:00:02 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0040/1466 | LOSS: 0.15506 (0.10617) | LR: 0.00000224 | TIME: 0:01:19 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0080/1466 | LOSS: 0.04138 (0.10967) | LR: 0.00000443 | TIME: 0:02:37 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0120/1466 | LOSS: 0.24996 (0.11206) | LR: 0.00000661 | TIME: 0:03:55 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0160/1466 | LOSS: 0.10980 (0.10981) | LR: 0.00000880 | TIME: 0:05:19 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0200/1466 | LOSS: 0.06554 (0.11321) | LR: 0.00001098 | TIME: 0:06:38 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0240/1466 | LOSS: 0.13987 (0.11555) | LR: 0.00001317 | TIME: 0:07:58 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0280/1466 | LOSS: 0.12791 (0.11773) | LR: 0.00001536 | TIME: 0:09:19 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0320/1466 | LOSS: 0.10827 (0.11777) | LR: 0.00001754 | TIME: 0:10:37 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0360/1466 | LOSS: 0.15303 (0.11922) | LR: 0.00001973 | TIME: 0:11:53 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0400/1466 | LOSS: 0.17158 (0.11969) | LR: 0.00002000 | TIME: 0:13:14 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0440/1466 | LOSS: 0.25536 (0.12099) | LR: 0.00001998 | TIME: 0:14:36 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0480/1466 | LOSS: 0.18623 (0.12263) | LR: 0.00001996 | TIME: 0:16:04 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0520/1466 | LOSS: 0.23331 (0.12374) | LR: 0.00001993 | TIME: 0:17:34 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0560/1466 | LOSS: 0.03881 (0.12627) | LR: 0.00001988 | TIME: 0:18:53 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0600/1466 | LOSS: 0.14029 (0.12734) | LR: 0.00001983 | TIME: 0:20:18 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0640/1466 | LOSS: 0.09410 (0.12938) | LR: 0.00001977 | TIME: 0:21:32 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0680/1466 | LOSS: 0.10256 (0.13109) | LR: 0.00001970 | TIME: 0:22:57 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0720/1466 | LOSS: 0.16199 (0.13127) | LR: 0.00001962 | TIME: 0:24:21 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0760/1466 | LOSS: 0.03978 (0.13066) | LR: 0.00001953 | TIME: 0:25:35 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0800/1466 | LOSS: 0.25802 (0.13034) | LR: 0.00001943 | TIME: 0:26:52 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0840/1466 | LOSS: 0.16149 (0.13102) | LR: 0.00001932 | TIME: 0:28:11 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0880/1466 | LOSS: 0.06962 (0.12994) | LR: 0.00001921 | TIME: 0:29:37 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0920/1466 | LOSS: 0.16047 (0.12932) | LR: 0.00001908 | TIME: 0:31:01 |
[TRAIN F3] EPOCH: 1/3 | STEP: 0960/1466 | LOSS: 0.10395 (0.12881) | LR: 0.00001894 | TIME: 0:32:15 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1000/1466 | LOSS: 0.28855 (0.12916) | LR: 0.00001880 | TIME: 0:33:36 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1040/1466 | LOSS: 0.13321 (0.12932) | LR: 0.00001865 | TIME: 0:34:54 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1080/1466 | LOSS: 0.25021 (0.12967) | LR: 0.00001849 | TIME: 0:36:18 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1120/1466 | LOSS: 0.06553 (0.12974) | LR: 0.00001832 | TIME: 0:37:34 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1160/1466 | LOSS: 0.16667 (0.12916) | LR: 0.00001814 | TIME: 0:38:55 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1200/1466 | LOSS: 0.09681 (0.12923) | LR: 0.00001796 | TIME: 0:40:15 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1240/1466 | LOSS: 0.11259 (0.12883) | LR: 0.00001776 | TIME: 0:41:33 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1280/1466 | LOSS: 0.11187 (0.12875) | LR: 0.00001756 | TIME: 0:42:52 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1320/1466 | LOSS: 0.10661 (0.12902) | LR: 0.00001736 | TIME: 0:44:08 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1360/1466 | LOSS: 0.15366 (0.12954) | LR: 0.00001714 | TIME: 0:45:29 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1400/1466 | LOSS: 0.13231 (0.12884) | LR: 0.00001692 | TIME: 0:46:51 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1440/1466 | LOSS: 0.13635 (0.12925) | LR: 0.00001669 | TIME: 0:48:17 |
[TRAIN F3] EPOCH: 1/3 | STEP: 1465/1466 | LOSS: 0.13372 (0.12933) | LR: 0.00001655 | TIME: 0:49:07 |

VALID_LOOP
[VALID F3] EPOCH: 1/3 | STEP: 000/489 | LOSS: 0.25891 (0.25891) | TIME: 0:00:01 |
[VALID F3] EPOCH: 1/3 | STEP: 040/489 | LOSS: 0.13114 (0.11093) | TIME: 0:00:25 |
[VALID F3] EPOCH: 1/3 | STEP: 080/489 | LOSS: 0.13237 (0.11329) | TIME: 0:00:50 |
[VALID F3] EPOCH: 1/3 | STEP: 120/489 | LOSS: 0.11056 (0.10925) | TIME: 0:01:14 |
[VALID F3] EPOCH: 1/3 | STEP: 160/489 | LOSS: 0.13375 (0.11107) | TIME: 0:01:38 |
[VALID F3] EPOCH: 1/3 | STEP: 200/489 | LOSS: 0.03100 (0.11064) | TIME: 0:02:02 |
[VALID F3] EPOCH: 1/3 | STEP: 240/489 | LOSS: 0.05978 (0.11064) | TIME: 0:02:26 |
[VALID F3] EPOCH: 1/3 | STEP: 280/489 | LOSS: 0.05625 (0.11142) | TIME: 0:02:51 |
[VALID F3] EPOCH: 1/3 | STEP: 320/489 | LOSS: 0.12396 (0.11276) | TIME: 0:03:15 |
[VALID F3] EPOCH: 1/3 | STEP: 360/489 | LOSS: 0.07425 (0.11279) | TIME: 0:03:39 |
[VALID F3] EPOCH: 1/3 | STEP: 400/489 | LOSS: 0.10397 (0.11208) | TIME: 0:04:03 |
[VALID F3] EPOCH: 1/3 | STEP: 440/489 | LOSS: 0.09802 (0.11226) | TIME: 0:04:27 |
[VALID F3] EPOCH: 1/3 | STEP: 480/489 | LOSS: 0.15007 (0.11209) | TIME: 0:04:51 |
[VALID F3] EPOCH: 1/3 | STEP: 488/489 | LOSS: 0.06835 (0.11167) | TIME: 0:04:56 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |      0.12933 |      0.11167 |  0.47378 | 0.487 | 0.480 | 0.438 | 0.449 | 0.513 | 0.476 | 0:54:04 |


[SAVED] EPOCH: 1 | MCRMSE: 0.47378483414649963

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 2/3 | STEP: 0000/1466 | LOSS: 0.07504 (0.07504) | LR: 0.00001654 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0040/1466 | LOSS: 0.18544 (0.11261) | LR: 0.00001630 | TIME: 0:01:25 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0080/1466 | LOSS: 0.12156 (0.11952) | LR: 0.00001606 | TIME: 0:02:46 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0120/1466 | LOSS: 0.11033 (0.12079) | LR: 0.00001581 | TIME: 0:04:03 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0160/1466 | LOSS: 0.10105 (0.11883) | LR: 0.00001555 | TIME: 0:05:27 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0200/1466 | LOSS: 0.12362 (0.11532) | LR: 0.00001529 | TIME: 0:06:43 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0240/1466 | LOSS: 0.06736 (0.11407) | LR: 0.00001502 | TIME: 0:07:57 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0280/1466 | LOSS: 0.08551 (0.11517) | LR: 0.00001475 | TIME: 0:09:16 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0320/1466 | LOSS: 0.05709 (0.11352) | LR: 0.00001447 | TIME: 0:10:36 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0360/1466 | LOSS: 0.25949 (0.11361) | LR: 0.00001419 | TIME: 0:12:00 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0400/1466 | LOSS: 0.09392 (0.11206) | LR: 0.00001391 | TIME: 0:13:13 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0440/1466 | LOSS: 0.15139 (0.11160) | LR: 0.00001362 | TIME: 0:14:30 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0480/1466 | LOSS: 0.12321 (0.11062) | LR: 0.00001332 | TIME: 0:15:49 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0520/1466 | LOSS: 0.16741 (0.11077) | LR: 0.00001303 | TIME: 0:17:12 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0560/1466 | LOSS: 0.12705 (0.10913) | LR: 0.00001273 | TIME: 0:18:38 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0600/1466 | LOSS: 0.11008 (0.10868) | LR: 0.00001243 | TIME: 0:20:07 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0640/1466 | LOSS: 0.06958 (0.10834) | LR: 0.00001213 | TIME: 0:21:29 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0680/1466 | LOSS: 0.03067 (0.10783) | LR: 0.00001182 | TIME: 0:22:51 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0720/1466 | LOSS: 0.04706 (0.10654) | LR: 0.00001151 | TIME: 0:24:10 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0760/1466 | LOSS: 0.07377 (0.10605) | LR: 0.00001120 | TIME: 0:25:35 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0800/1466 | LOSS: 0.06363 (0.10647) | LR: 0.00001089 | TIME: 0:27:00 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0840/1466 | LOSS: 0.05469 (0.10566) | LR: 0.00001058 | TIME: 0:28:22 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0880/1466 | LOSS: 0.11474 (0.10575) | LR: 0.00001027 | TIME: 0:29:44 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0920/1466 | LOSS: 0.08833 (0.10471) | LR: 0.00000996 | TIME: 0:31:04 |
[TRAIN F3] EPOCH: 2/3 | STEP: 0960/1466 | LOSS: 0.09397 (0.10425) | LR: 0.00000965 | TIME: 0:32:25 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1000/1466 | LOSS: 0.03161 (0.10406) | LR: 0.00000934 | TIME: 0:33:44 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1040/1466 | LOSS: 0.09388 (0.10398) | LR: 0.00000903 | TIME: 0:35:01 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1080/1466 | LOSS: 0.05215 (0.10431) | LR: 0.00000872 | TIME: 0:36:19 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1120/1466 | LOSS: 0.19027 (0.10444) | LR: 0.00000841 | TIME: 0:37:33 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1160/1466 | LOSS: 0.16678 (0.10432) | LR: 0.00000810 | TIME: 0:38:54 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1200/1466 | LOSS: 0.08816 (0.10501) | LR: 0.00000780 | TIME: 0:40:17 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1240/1466 | LOSS: 0.15982 (0.10491) | LR: 0.00000749 | TIME: 0:41:39 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1280/1466 | LOSS: 0.11566 (0.10432) | LR: 0.00000719 | TIME: 0:42:58 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1320/1466 | LOSS: 0.06847 (0.10427) | LR: 0.00000690 | TIME: 0:44:15 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1360/1466 | LOSS: 0.09467 (0.10398) | LR: 0.00000660 | TIME: 0:45:38 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1400/1466 | LOSS: 0.03198 (0.10377) | LR: 0.00000631 | TIME: 0:47:00 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1440/1466 | LOSS: 0.08662 (0.10378) | LR: 0.00000602 | TIME: 0:48:20 |
[TRAIN F3] EPOCH: 2/3 | STEP: 1465/1466 | LOSS: 0.10586 (0.10348) | LR: 0.00000584 | TIME: 0:49:14 |

VALID_LOOP
[VALID F3] EPOCH: 2/3 | STEP: 000/489 | LOSS: 0.16721 (0.16721) | TIME: 0:00:01 |
[VALID F3] EPOCH: 2/3 | STEP: 040/489 | LOSS: 0.09709 (0.09811) | TIME: 0:00:25 |
[VALID F3] EPOCH: 2/3 | STEP: 080/489 | LOSS: 0.06758 (0.10090) | TIME: 0:00:50 |
[VALID F3] EPOCH: 2/3 | STEP: 120/489 | LOSS: 0.15258 (0.10178) | TIME: 0:01:14 |
[VALID F3] EPOCH: 2/3 | STEP: 160/489 | LOSS: 0.15979 (0.10386) | TIME: 0:01:38 |
[VALID F3] EPOCH: 2/3 | STEP: 200/489 | LOSS: 0.03874 (0.10441) | TIME: 0:02:03 |
[VALID F3] EPOCH: 2/3 | STEP: 240/489 | LOSS: 0.09383 (0.10380) | TIME: 0:02:27 |
[VALID F3] EPOCH: 2/3 | STEP: 280/489 | LOSS: 0.04776 (0.10453) | TIME: 0:02:51 |
[VALID F3] EPOCH: 2/3 | STEP: 320/489 | LOSS: 0.24823 (0.10549) | TIME: 0:03:16 |
[VALID F3] EPOCH: 2/3 | STEP: 360/489 | LOSS: 0.07288 (0.10618) | TIME: 0:03:40 |
[VALID F3] EPOCH: 2/3 | STEP: 400/489 | LOSS: 0.07041 (0.10591) | TIME: 0:04:04 |
[VALID F3] EPOCH: 2/3 | STEP: 440/489 | LOSS: 0.10506 (0.10657) | TIME: 0:04:29 |
[VALID F3] EPOCH: 2/3 | STEP: 480/489 | LOSS: 0.11006 (0.10702) | TIME: 0:04:53 |
[VALID F3] EPOCH: 2/3 | STEP: 488/489 | LOSS: 0.05729 (0.10658) | TIME: 0:04:58 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10348 |      0.10658 |  0.46246 | 0.488 | 0.455 | 0.416 | 0.475 | 0.483 | 0.457 | 0:54:13 |


[SAVED] EPOCH: 2 | MCRMSE: 0.46245917677879333

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 3/3 | STEP: 0000/1466 | LOSS: 0.06167 (0.06167) | LR: 0.00000584 | TIME: 0:00:02 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0040/1466 | LOSS: 0.10539 (0.07965) | LR: 0.00000556 | TIME: 0:01:19 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0080/1466 | LOSS: 0.07760 (0.08463) | LR: 0.00000528 | TIME: 0:02:37 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0120/1466 | LOSS: 0.08113 (0.08332) | LR: 0.00000501 | TIME: 0:03:59 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0160/1466 | LOSS: 0.05383 (0.08787) | LR: 0.00000474 | TIME: 0:05:22 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0200/1466 | LOSS: 0.06135 (0.08651) | LR: 0.00000448 | TIME: 0:06:45 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0240/1466 | LOSS: 0.10965 (0.08641) | LR: 0.00000422 | TIME: 0:08:06 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0280/1466 | LOSS: 0.14780 (0.08524) | LR: 0.00000397 | TIME: 0:09:25 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0320/1466 | LOSS: 0.06981 (0.08606) | LR: 0.00000372 | TIME: 0:10:49 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0360/1466 | LOSS: 0.05731 (0.08551) | LR: 0.00000348 | TIME: 0:12:12 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0400/1466 | LOSS: 0.10783 (0.08567) | LR: 0.00000325 | TIME: 0:13:37 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0440/1466 | LOSS: 0.05263 (0.08483) | LR: 0.00000302 | TIME: 0:14:56 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0480/1466 | LOSS: 0.13784 (0.08438) | LR: 0.00000280 | TIME: 0:16:18 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0520/1466 | LOSS: 0.14608 (0.08490) | LR: 0.00000259 | TIME: 0:17:42 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0560/1466 | LOSS: 0.06321 (0.08500) | LR: 0.00000238 | TIME: 0:19:07 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0600/1466 | LOSS: 0.07867 (0.08485) | LR: 0.00000219 | TIME: 0:20:33 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0640/1466 | LOSS: 0.04788 (0.08413) | LR: 0.00000200 | TIME: 0:21:52 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0680/1466 | LOSS: 0.05277 (0.08401) | LR: 0.00000181 | TIME: 0:23:07 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0720/1466 | LOSS: 0.06897 (0.08377) | LR: 0.00000164 | TIME: 0:24:25 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0760/1466 | LOSS: 0.11068 (0.08351) | LR: 0.00000147 | TIME: 0:25:41 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0800/1466 | LOSS: 0.04325 (0.08339) | LR: 0.00000131 | TIME: 0:27:06 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0840/1466 | LOSS: 0.06733 (0.08276) | LR: 0.00000116 | TIME: 0:28:31 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0880/1466 | LOSS: 0.10844 (0.08230) | LR: 0.00000102 | TIME: 0:29:49 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0920/1466 | LOSS: 0.09265 (0.08198) | LR: 0.00000089 | TIME: 0:31:07 |
[TRAIN F3] EPOCH: 3/3 | STEP: 0960/1466 | LOSS: 0.06013 (0.08190) | LR: 0.00000076 | TIME: 0:32:28 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1000/1466 | LOSS: 0.03517 (0.08193) | LR: 0.00000065 | TIME: 0:33:47 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1040/1466 | LOSS: 0.06852 (0.08216) | LR: 0.00000054 | TIME: 0:35:10 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1080/1466 | LOSS: 0.07165 (0.08194) | LR: 0.00000045 | TIME: 0:36:36 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1120/1466 | LOSS: 0.02672 (0.08189) | LR: 0.00000036 | TIME: 0:37:55 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1160/1466 | LOSS: 0.05590 (0.08216) | LR: 0.00000028 | TIME: 0:39:11 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1200/1466 | LOSS: 0.09457 (0.08237) | LR: 0.00000021 | TIME: 0:40:27 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1240/1466 | LOSS: 0.07697 (0.08237) | LR: 0.00000015 | TIME: 0:41:49 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1280/1466 | LOSS: 0.08977 (0.08226) | LR: 0.00000010 | TIME: 0:43:15 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1320/1466 | LOSS: 0.07614 (0.08212) | LR: 0.00000006 | TIME: 0:44:33 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1360/1466 | LOSS: 0.10282 (0.08190) | LR: 0.00000003 | TIME: 0:45:56 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1400/1466 | LOSS: 0.10557 (0.08189) | LR: 0.00000001 | TIME: 0:47:17 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1440/1466 | LOSS: 0.02709 (0.08142) | LR: 0.00000000 | TIME: 0:48:36 |
[TRAIN F3] EPOCH: 3/3 | STEP: 1465/1466 | LOSS: 0.06935 (0.08143) | LR: 0.00000000 | TIME: 0:49:27 |

VALID_LOOP
[VALID F3] EPOCH: 3/3 | STEP: 000/489 | LOSS: 0.20120 (0.20120) | TIME: 0:00:01 |
[VALID F3] EPOCH: 3/3 | STEP: 040/489 | LOSS: 0.11167 (0.09663) | TIME: 0:00:26 |
[VALID F3] EPOCH: 3/3 | STEP: 080/489 | LOSS: 0.08712 (0.09890) | TIME: 0:00:51 |
[VALID F3] EPOCH: 3/3 | STEP: 120/489 | LOSS: 0.12162 (0.09744) | TIME: 0:01:16 |
[VALID F3] EPOCH: 3/3 | STEP: 160/489 | LOSS: 0.12084 (0.09943) | TIME: 0:01:40 |
[VALID F3] EPOCH: 3/3 | STEP: 200/489 | LOSS: 0.03392 (0.09937) | TIME: 0:02:05 |
[VALID F3] EPOCH: 3/3 | STEP: 240/489 | LOSS: 0.06836 (0.09919) | TIME: 0:02:30 |
[VALID F3] EPOCH: 3/3 | STEP: 280/489 | LOSS: 0.04226 (0.09955) | TIME: 0:02:55 |
[VALID F3] EPOCH: 3/3 | STEP: 320/489 | LOSS: 0.18098 (0.09996) | TIME: 0:03:20 |
[VALID F3] EPOCH: 3/3 | STEP: 360/489 | LOSS: 0.06743 (0.10061) | TIME: 0:03:44 |
[VALID F3] EPOCH: 3/3 | STEP: 400/489 | LOSS: 0.06648 (0.10033) | TIME: 0:04:09 |
[VALID F3] EPOCH: 3/3 | STEP: 440/489 | LOSS: 0.08405 (0.10080) | TIME: 0:04:34 |
[VALID F3] EPOCH: 3/3 | STEP: 480/489 | LOSS: 0.11102 (0.10105) | TIME: 0:04:59 |
[VALID F3] EPOCH: 3/3 | STEP: 488/489 | LOSS: 0.04461 (0.10058) | TIME: 0:05:04 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08143 |      0.10058 |  0.44897 | 0.487 | 0.450 | 0.411 | 0.440 | 0.471 | 0.435 | 0:54:32 |


[SAVED] EPOCH: 3 | MCRMSE: 0.44896554946899414


----------------------------------- FOLD 3 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.44897     0.48661    0.4501       0.41116        0.44036    0.47093        0.43463

################################### END OF FOlD 3 ###################################


