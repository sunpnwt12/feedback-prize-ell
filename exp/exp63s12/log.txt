Date: 2022-11-26 12:33:08.513914+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: roberta-large
Model_config: RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1098}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 512

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 1/3 | STEP: 000/366 | LOSS: 0.14706 (0.14706) | LR: 0.00000033 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 1/3 | STEP: 040/366 | LOSS: 0.09200 (0.12956) | LR: 0.00001352 | TIME: 0:01:35 |
[TRAIN F0] EPOCH: 1/3 | STEP: 080/366 | LOSS: 0.14292 (0.12918) | LR: 0.00002670 | TIME: 0:03:11 |
[TRAIN F0] EPOCH: 1/3 | STEP: 120/366 | LOSS: 0.16703 (0.13718) | LR: 0.00002993 | TIME: 0:04:47 |
[TRAIN F0] EPOCH: 1/3 | STEP: 160/366 | LOSS: 0.19574 (0.13917) | LR: 0.00002964 | TIME: 0:06:23 |
[TRAIN F0] EPOCH: 1/3 | STEP: 200/366 | LOSS: 0.15012 (0.13758) | LR: 0.00002913 | TIME: 0:07:59 |
[TRAIN F0] EPOCH: 1/3 | STEP: 240/366 | LOSS: 0.15605 (0.13722) | LR: 0.00002839 | TIME: 0:09:34 |
[TRAIN F0] EPOCH: 1/3 | STEP: 280/366 | LOSS: 0.16347 (0.13838) | LR: 0.00002744 | TIME: 0:11:10 |
[TRAIN F0] EPOCH: 1/3 | STEP: 320/366 | LOSS: 0.11342 (0.13740) | LR: 0.00002630 | TIME: 0:12:46 |
[TRAIN F0] EPOCH: 1/3 | STEP: 360/366 | LOSS: 0.11673 (0.13649) | LR: 0.00002499 | TIME: 0:14:22 |
[TRAIN F0] EPOCH: 1/3 | STEP: 365/366 | LOSS: 0.21407 (0.13660) | LR: 0.00002481 | TIME: 0:14:33 |

VALID_LOOP
[VALID F0] EPOCH: 1/3 | STEP: 000/123 | LOSS: 0.14509 (0.14509) | TIME: 0:00:01 |
[VALID F0] EPOCH: 1/3 | STEP: 040/123 | LOSS: 0.11799 (0.12766) | TIME: 0:00:33 |
[VALID F0] EPOCH: 1/3 | STEP: 080/123 | LOSS: 0.09865 (0.12661) | TIME: 0:01:04 |
[VALID F0] EPOCH: 1/3 | STEP: 120/123 | LOSS: 0.12975 (0.12665) | TIME: 0:01:36 |
[VALID F0] EPOCH: 1/3 | STEP: 122/123 | LOSS: 0.06981 (0.12667) | TIME: 0:01:37 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |       0.1366 |      0.12667 |  0.50288 | 0.615 | 0.484 | 0.422 | 0.527 | 0.490 | 0.481 | 0:16:11 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5028769969940186

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 2/3 | STEP: 000/366 | LOSS: 0.15149 (0.15149) | LR: 0.00002477 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 2/3 | STEP: 040/366 | LOSS: 0.09462 (0.11623) | LR: 0.00002328 | TIME: 0:01:39 |
[TRAIN F0] EPOCH: 2/3 | STEP: 080/366 | LOSS: 0.14180 (0.12202) | LR: 0.00002166 | TIME: 0:03:16 |
[TRAIN F0] EPOCH: 2/3 | STEP: 120/366 | LOSS: 0.07812 (0.11983) | LR: 0.00001994 | TIME: 0:04:53 |
[TRAIN F0] EPOCH: 2/3 | STEP: 160/366 | LOSS: 0.09216 (0.11991) | LR: 0.00001814 | TIME: 0:06:30 |
[TRAIN F0] EPOCH: 2/3 | STEP: 200/366 | LOSS: 0.08202 (0.11728) | LR: 0.00001629 | TIME: 0:08:05 |
[TRAIN F0] EPOCH: 2/3 | STEP: 240/366 | LOSS: 0.19553 (0.11528) | LR: 0.00001442 | TIME: 0:09:42 |
[TRAIN F0] EPOCH: 2/3 | STEP: 280/366 | LOSS: 0.10385 (0.11311) | LR: 0.00001255 | TIME: 0:11:19 |
[TRAIN F0] EPOCH: 2/3 | STEP: 320/366 | LOSS: 0.08571 (0.11170) | LR: 0.00001073 | TIME: 0:12:55 |
[TRAIN F0] EPOCH: 2/3 | STEP: 360/366 | LOSS: 0.07404 (0.11065) | LR: 0.00000897 | TIME: 0:14:30 |
[TRAIN F0] EPOCH: 2/3 | STEP: 365/366 | LOSS: 0.09184 (0.11043) | LR: 0.00000876 | TIME: 0:14:42 |

VALID_LOOP
[VALID F0] EPOCH: 2/3 | STEP: 000/123 | LOSS: 0.09866 (0.09866) | TIME: 0:00:01 |
[VALID F0] EPOCH: 2/3 | STEP: 040/123 | LOSS: 0.10153 (0.10855) | TIME: 0:00:33 |
[VALID F0] EPOCH: 2/3 | STEP: 080/123 | LOSS: 0.06203 (0.10619) | TIME: 0:01:04 |
[VALID F0] EPOCH: 2/3 | STEP: 120/123 | LOSS: 0.12335 (0.10852) | TIME: 0:01:36 |
[VALID F0] EPOCH: 2/3 | STEP: 122/123 | LOSS: 0.06166 (0.10845) | TIME: 0:01:37 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.11043 |      0.10845 |  0.46626 | 0.514 | 0.442 | 0.426 | 0.470 | 0.479 | 0.465 | 0:16:19 |


[SAVED] EPOCH: 2 | MCRMSE: 0.46625733375549316

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 3/3 | STEP: 000/366 | LOSS: 0.10718 (0.10718) | LR: 0.00000872 | TIME: 0:00:03 |
[TRAIN F0] EPOCH: 3/3 | STEP: 040/366 | LOSS: 0.10399 (0.08554) | LR: 0.00000707 | TIME: 0:01:40 |
[TRAIN F0] EPOCH: 3/3 | STEP: 080/366 | LOSS: 0.10653 (0.08921) | LR: 0.00000555 | TIME: 0:03:16 |
[TRAIN F0] EPOCH: 3/3 | STEP: 120/366 | LOSS: 0.06842 (0.08732) | LR: 0.00000417 | TIME: 0:04:51 |
[TRAIN F0] EPOCH: 3/3 | STEP: 160/366 | LOSS: 0.05380 (0.08570) | LR: 0.00000296 | TIME: 0:06:27 |
[TRAIN F0] EPOCH: 3/3 | STEP: 200/366 | LOSS: 0.10061 (0.08590) | LR: 0.00000194 | TIME: 0:08:03 |
[TRAIN F0] EPOCH: 3/3 | STEP: 240/366 | LOSS: 0.07882 (0.08503) | LR: 0.00000113 | TIME: 0:09:38 |
[TRAIN F0] EPOCH: 3/3 | STEP: 280/366 | LOSS: 0.11935 (0.08468) | LR: 0.00000052 | TIME: 0:11:16 |
[TRAIN F0] EPOCH: 3/3 | STEP: 320/366 | LOSS: 0.05886 (0.08491) | LR: 0.00000015 | TIME: 0:12:52 |
[TRAIN F0] EPOCH: 3/3 | STEP: 360/366 | LOSS: 0.09518 (0.08486) | LR: 0.00000000 | TIME: 0:14:29 |
[TRAIN F0] EPOCH: 3/3 | STEP: 365/366 | LOSS: 0.06849 (0.08466) | LR: 0.00000000 | TIME: 0:14:41 |

VALID_LOOP
[VALID F0] EPOCH: 3/3 | STEP: 000/123 | LOSS: 0.10069 (0.10069) | TIME: 0:00:01 |
[VALID F0] EPOCH: 3/3 | STEP: 040/123 | LOSS: 0.09875 (0.10526) | TIME: 0:00:33 |
[VALID F0] EPOCH: 3/3 | STEP: 080/123 | LOSS: 0.06120 (0.10427) | TIME: 0:01:04 |
[VALID F0] EPOCH: 3/3 | STEP: 120/123 | LOSS: 0.11849 (0.10670) | TIME: 0:01:36 |
[VALID F0] EPOCH: 3/3 | STEP: 122/123 | LOSS: 0.04935 (0.10664) | TIME: 0:01:37 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08466 |      0.10664 |  0.46236 | 0.501 | 0.440 | 0.425 | 0.470 | 0.478 | 0.461 | 0:16:18 |


[SAVED] EPOCH: 3 | MCRMSE: 0.46235665678977966


----------------------------------- FOLD 0 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.46236     0.50069   0.43971       0.42525        0.46965    0.47834         0.4605

################################### END OF FOlD 0 ###################################


Date: 2022-11-26 13:22:36.276398+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: roberta-large
Model_config: RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1098}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 512

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 1/3 | STEP: 000/366 | LOSS: 0.14509 (0.14509) | LR: 0.00000033 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 1/3 | STEP: 040/366 | LOSS: 0.09834 (0.12961) | LR: 0.00001352 | TIME: 0:01:39 |
[TRAIN F1] EPOCH: 1/3 | STEP: 080/366 | LOSS: 0.16984 (0.13440) | LR: 0.00002670 | TIME: 0:03:15 |
[TRAIN F1] EPOCH: 1/3 | STEP: 120/366 | LOSS: 0.08281 (0.13547) | LR: 0.00002993 | TIME: 0:04:52 |
[TRAIN F1] EPOCH: 1/3 | STEP: 160/366 | LOSS: 0.11523 (0.13537) | LR: 0.00002964 | TIME: 0:06:27 |
[TRAIN F1] EPOCH: 1/3 | STEP: 200/366 | LOSS: 0.13312 (0.13854) | LR: 0.00002913 | TIME: 0:08:03 |
[TRAIN F1] EPOCH: 1/3 | STEP: 240/366 | LOSS: 0.11972 (0.14108) | LR: 0.00002839 | TIME: 0:09:39 |
[TRAIN F1] EPOCH: 1/3 | STEP: 280/366 | LOSS: 0.10248 (0.14128) | LR: 0.00002744 | TIME: 0:11:15 |
[TRAIN F1] EPOCH: 1/3 | STEP: 320/366 | LOSS: 0.11712 (0.14071) | LR: 0.00002630 | TIME: 0:12:50 |
[TRAIN F1] EPOCH: 1/3 | STEP: 360/366 | LOSS: 0.13184 (0.13956) | LR: 0.00002499 | TIME: 0:14:26 |
[TRAIN F1] EPOCH: 1/3 | STEP: 365/366 | LOSS: 0.12944 (0.13970) | LR: 0.00002481 | TIME: 0:14:39 |

VALID_LOOP
[VALID F1] EPOCH: 1/3 | STEP: 000/123 | LOSS: 0.10005 (0.10005) | TIME: 0:00:01 |
[VALID F1] EPOCH: 1/3 | STEP: 040/123 | LOSS: 0.14014 (0.12619) | TIME: 0:00:33 |
[VALID F1] EPOCH: 1/3 | STEP: 080/123 | LOSS: 0.09938 (0.12547) | TIME: 0:01:04 |
[VALID F1] EPOCH: 1/3 | STEP: 120/123 | LOSS: 0.14289 (0.12726) | TIME: 0:01:36 |
[VALID F1] EPOCH: 1/3 | STEP: 122/123 | LOSS: 0.08575 (0.12705) | TIME: 0:01:36 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |       0.1397 |      0.12705 |  0.50227 | 0.495 | 0.502 | 0.434 | 0.462 | 0.652 | 0.469 | 0:16:16 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5022677183151245

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 2/3 | STEP: 000/366 | LOSS: 0.15799 (0.15799) | LR: 0.00002477 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 2/3 | STEP: 040/366 | LOSS: 0.14517 (0.13485) | LR: 0.00002328 | TIME: 0:01:39 |
[TRAIN F1] EPOCH: 2/3 | STEP: 080/366 | LOSS: 0.11319 (0.12679) | LR: 0.00002166 | TIME: 0:03:16 |
[TRAIN F1] EPOCH: 2/3 | STEP: 120/366 | LOSS: 0.12463 (0.12170) | LR: 0.00001994 | TIME: 0:04:52 |
[TRAIN F1] EPOCH: 2/3 | STEP: 160/366 | LOSS: 0.09942 (0.11753) | LR: 0.00001814 | TIME: 0:06:28 |
[TRAIN F1] EPOCH: 2/3 | STEP: 200/366 | LOSS: 0.09240 (0.11641) | LR: 0.00001629 | TIME: 0:08:04 |
[TRAIN F1] EPOCH: 2/3 | STEP: 240/366 | LOSS: 0.08677 (0.11403) | LR: 0.00001442 | TIME: 0:09:40 |
[TRAIN F1] EPOCH: 2/3 | STEP: 280/366 | LOSS: 0.07415 (0.11376) | LR: 0.00001255 | TIME: 0:11:15 |
[TRAIN F1] EPOCH: 2/3 | STEP: 320/366 | LOSS: 0.06239 (0.11250) | LR: 0.00001073 | TIME: 0:12:51 |
[TRAIN F1] EPOCH: 2/3 | STEP: 360/366 | LOSS: 0.12038 (0.11112) | LR: 0.00000897 | TIME: 0:14:27 |
[TRAIN F1] EPOCH: 2/3 | STEP: 365/366 | LOSS: 0.13633 (0.11114) | LR: 0.00000876 | TIME: 0:14:39 |

VALID_LOOP
[VALID F1] EPOCH: 2/3 | STEP: 000/123 | LOSS: 0.08932 (0.08932) | TIME: 0:00:01 |
[VALID F1] EPOCH: 2/3 | STEP: 040/123 | LOSS: 0.10960 (0.10593) | TIME: 0:00:33 |
[VALID F1] EPOCH: 2/3 | STEP: 080/123 | LOSS: 0.10608 (0.10727) | TIME: 0:01:04 |
[VALID F1] EPOCH: 2/3 | STEP: 120/123 | LOSS: 0.09958 (0.10809) | TIME: 0:01:36 |
[VALID F1] EPOCH: 2/3 | STEP: 122/123 | LOSS: 0.09610 (0.10785) | TIME: 0:01:37 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.11114 |      0.10785 |  0.46554 | 0.487 | 0.479 | 0.413 | 0.467 | 0.493 | 0.455 | 0:16:16 |


[SAVED] EPOCH: 2 | MCRMSE: 0.46554288268089294

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 3/3 | STEP: 000/366 | LOSS: 0.09747 (0.09747) | LR: 0.00000872 | TIME: 0:00:03 |
[TRAIN F1] EPOCH: 3/3 | STEP: 040/366 | LOSS: 0.05959 (0.09050) | LR: 0.00000707 | TIME: 0:01:39 |
[TRAIN F1] EPOCH: 3/3 | STEP: 080/366 | LOSS: 0.11679 (0.08780) | LR: 0.00000555 | TIME: 0:03:15 |
[TRAIN F1] EPOCH: 3/3 | STEP: 120/366 | LOSS: 0.17298 (0.09022) | LR: 0.00000417 | TIME: 0:04:50 |
[TRAIN F1] EPOCH: 3/3 | STEP: 160/366 | LOSS: 0.10986 (0.08928) | LR: 0.00000296 | TIME: 0:06:26 |
[TRAIN F1] EPOCH: 3/3 | STEP: 200/366 | LOSS: 0.08073 (0.08927) | LR: 0.00000194 | TIME: 0:08:02 |
[TRAIN F1] EPOCH: 3/3 | STEP: 240/366 | LOSS: 0.04153 (0.08911) | LR: 0.00000113 | TIME: 0:09:38 |
[TRAIN F1] EPOCH: 3/3 | STEP: 280/366 | LOSS: 0.11109 (0.08788) | LR: 0.00000052 | TIME: 0:11:13 |
[TRAIN F1] EPOCH: 3/3 | STEP: 320/366 | LOSS: 0.11990 (0.08741) | LR: 0.00000015 | TIME: 0:12:49 |
[TRAIN F1] EPOCH: 3/3 | STEP: 360/366 | LOSS: 0.10415 (0.08659) | LR: 0.00000000 | TIME: 0:14:24 |
[TRAIN F1] EPOCH: 3/3 | STEP: 365/366 | LOSS: 0.06469 (0.08671) | LR: 0.00000000 | TIME: 0:14:36 |

VALID_LOOP
[VALID F1] EPOCH: 3/3 | STEP: 000/123 | LOSS: 0.09760 (0.09760) | TIME: 0:00:01 |
[VALID F1] EPOCH: 3/3 | STEP: 040/123 | LOSS: 0.09781 (0.10497) | TIME: 0:00:33 |
[VALID F1] EPOCH: 3/3 | STEP: 080/123 | LOSS: 0.11983 (0.10608) | TIME: 0:01:04 |
[VALID F1] EPOCH: 3/3 | STEP: 120/123 | LOSS: 0.08067 (0.10585) | TIME: 0:01:36 |
[VALID F1] EPOCH: 3/3 | STEP: 122/123 | LOSS: 0.09831 (0.10559) | TIME: 0:01:37 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08671 |      0.10559 |  0.46071 | 0.490 | 0.460 | 0.416 | 0.459 | 0.487 | 0.451 | 0:16:13 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4607107937335968


----------------------------------- FOLD 1 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.46071     0.49013   0.46045       0.41633        0.45855    0.48746        0.45134

################################### END OF FOlD 1 ###################################


Date: 2022-11-26 14:12:01.244523+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: roberta-large
Model_config: RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1098}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 512

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 1/3 | STEP: 000/366 | LOSS: 0.10613 (0.10613) | LR: 0.00000033 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 1/3 | STEP: 040/366 | LOSS: 0.07957 (0.13167) | LR: 0.00001352 | TIME: 0:01:40 |
[TRAIN F2] EPOCH: 1/3 | STEP: 080/366 | LOSS: 0.12403 (0.13866) | LR: 0.00002670 | TIME: 0:03:15 |
[TRAIN F2] EPOCH: 1/3 | STEP: 120/366 | LOSS: 0.11562 (0.14011) | LR: 0.00002993 | TIME: 0:04:51 |
[TRAIN F2] EPOCH: 1/3 | STEP: 160/366 | LOSS: 0.11485 (0.14138) | LR: 0.00002964 | TIME: 0:06:28 |
[TRAIN F2] EPOCH: 1/3 | STEP: 200/366 | LOSS: 0.18156 (0.14185) | LR: 0.00002913 | TIME: 0:08:04 |
[TRAIN F2] EPOCH: 1/3 | STEP: 240/366 | LOSS: 0.14399 (0.14029) | LR: 0.00002839 | TIME: 0:09:39 |
[TRAIN F2] EPOCH: 1/3 | STEP: 280/366 | LOSS: 0.17675 (0.14103) | LR: 0.00002744 | TIME: 0:11:15 |
[TRAIN F2] EPOCH: 1/3 | STEP: 320/366 | LOSS: 0.12070 (0.14110) | LR: 0.00002630 | TIME: 0:12:50 |
[TRAIN F2] EPOCH: 1/3 | STEP: 360/366 | LOSS: 0.09675 (0.13977) | LR: 0.00002499 | TIME: 0:14:26 |
[TRAIN F2] EPOCH: 1/3 | STEP: 365/366 | LOSS: 0.07808 (0.13980) | LR: 0.00002481 | TIME: 0:14:38 |

VALID_LOOP
[VALID F2] EPOCH: 1/3 | STEP: 000/123 | LOSS: 0.06599 (0.06599) | TIME: 0:00:01 |
[VALID F2] EPOCH: 1/3 | STEP: 040/123 | LOSS: 0.10018 (0.10181) | TIME: 0:00:33 |
[VALID F2] EPOCH: 1/3 | STEP: 080/123 | LOSS: 0.10865 (0.10256) | TIME: 0:01:04 |
[VALID F2] EPOCH: 1/3 | STEP: 120/123 | LOSS: 0.09762 (0.10485) | TIME: 0:01:36 |
[VALID F2] EPOCH: 1/3 | STEP: 122/123 | LOSS: 0.09846 (0.10456) | TIME: 0:01:37 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |       0.1398 |      0.10456 |   0.4581 | 0.488 | 0.461 | 0.410 | 0.453 | 0.480 | 0.456 | 0:16:15 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4581029713153839

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 2/3 | STEP: 000/366 | LOSS: 0.12289 (0.12289) | LR: 0.00002477 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 2/3 | STEP: 040/366 | LOSS: 0.09509 (0.11984) | LR: 0.00002328 | TIME: 0:01:39 |
[TRAIN F2] EPOCH: 2/3 | STEP: 080/366 | LOSS: 0.08422 (0.11423) | LR: 0.00002166 | TIME: 0:03:15 |
[TRAIN F2] EPOCH: 2/3 | STEP: 120/366 | LOSS: 0.08369 (0.11257) | LR: 0.00001994 | TIME: 0:04:50 |
[TRAIN F2] EPOCH: 2/3 | STEP: 160/366 | LOSS: 0.12086 (0.11214) | LR: 0.00001814 | TIME: 0:06:26 |
[TRAIN F2] EPOCH: 2/3 | STEP: 200/366 | LOSS: 0.08946 (0.11246) | LR: 0.00001629 | TIME: 0:08:01 |
[TRAIN F2] EPOCH: 2/3 | STEP: 240/366 | LOSS: 0.10287 (0.11293) | LR: 0.00001442 | TIME: 0:09:37 |
[TRAIN F2] EPOCH: 2/3 | STEP: 280/366 | LOSS: 0.08913 (0.11294) | LR: 0.00001255 | TIME: 0:11:12 |
[TRAIN F2] EPOCH: 2/3 | STEP: 320/366 | LOSS: 0.12728 (0.11185) | LR: 0.00001073 | TIME: 0:12:49 |
[TRAIN F2] EPOCH: 2/3 | STEP: 360/366 | LOSS: 0.08805 (0.11189) | LR: 0.00000897 | TIME: 0:14:24 |
[TRAIN F2] EPOCH: 2/3 | STEP: 365/366 | LOSS: 0.09718 (0.11150) | LR: 0.00000876 | TIME: 0:14:36 |

VALID_LOOP
[VALID F2] EPOCH: 2/3 | STEP: 000/123 | LOSS: 0.07993 (0.07993) | TIME: 0:00:01 |
[VALID F2] EPOCH: 2/3 | STEP: 040/123 | LOSS: 0.11959 (0.10171) | TIME: 0:00:33 |
[VALID F2] EPOCH: 2/3 | STEP: 080/123 | LOSS: 0.10979 (0.10044) | TIME: 0:01:04 |
[VALID F2] EPOCH: 2/3 | STEP: 120/123 | LOSS: 0.10470 (0.10206) | TIME: 0:01:36 |
[VALID F2] EPOCH: 2/3 | STEP: 122/123 | LOSS: 0.12985 (0.10174) | TIME: 0:01:37 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |       0.1115 |      0.10174 |  0.45204 | 0.466 | 0.449 | 0.420 | 0.454 | 0.469 | 0.454 | 0:16:13 |


[SAVED] EPOCH: 2 | MCRMSE: 0.452038437128067

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 3/3 | STEP: 000/366 | LOSS: 0.11645 (0.11645) | LR: 0.00000872 | TIME: 0:00:03 |
[TRAIN F2] EPOCH: 3/3 | STEP: 040/366 | LOSS: 0.07473 (0.09069) | LR: 0.00000707 | TIME: 0:01:39 |
[TRAIN F2] EPOCH: 3/3 | STEP: 080/366 | LOSS: 0.07643 (0.09096) | LR: 0.00000555 | TIME: 0:03:16 |
[TRAIN F2] EPOCH: 3/3 | STEP: 120/366 | LOSS: 0.10739 (0.08943) | LR: 0.00000417 | TIME: 0:04:52 |
[TRAIN F2] EPOCH: 3/3 | STEP: 160/366 | LOSS: 0.08617 (0.08799) | LR: 0.00000296 | TIME: 0:06:28 |
[TRAIN F2] EPOCH: 3/3 | STEP: 200/366 | LOSS: 0.11835 (0.08840) | LR: 0.00000194 | TIME: 0:08:03 |
[TRAIN F2] EPOCH: 3/3 | STEP: 240/366 | LOSS: 0.07645 (0.08847) | LR: 0.00000113 | TIME: 0:09:39 |
[TRAIN F2] EPOCH: 3/3 | STEP: 280/366 | LOSS: 0.06779 (0.08800) | LR: 0.00000052 | TIME: 0:11:14 |
[TRAIN F2] EPOCH: 3/3 | STEP: 320/366 | LOSS: 0.04602 (0.08866) | LR: 0.00000015 | TIME: 0:12:50 |
[TRAIN F2] EPOCH: 3/3 | STEP: 360/366 | LOSS: 0.08485 (0.08883) | LR: 0.00000000 | TIME: 0:14:26 |
[TRAIN F2] EPOCH: 3/3 | STEP: 365/366 | LOSS: 0.08175 (0.08866) | LR: 0.00000000 | TIME: 0:14:37 |

VALID_LOOP
[VALID F2] EPOCH: 3/3 | STEP: 000/123 | LOSS: 0.07754 (0.07754) | TIME: 0:00:01 |
[VALID F2] EPOCH: 3/3 | STEP: 040/123 | LOSS: 0.10754 (0.09747) | TIME: 0:00:33 |
[VALID F2] EPOCH: 3/3 | STEP: 080/123 | LOSS: 0.10565 (0.09713) | TIME: 0:01:04 |
[VALID F2] EPOCH: 3/3 | STEP: 120/123 | LOSS: 0.10307 (0.09858) | TIME: 0:01:36 |
[VALID F2] EPOCH: 3/3 | STEP: 122/123 | LOSS: 0.11364 (0.09828) | TIME: 0:01:37 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08866 |      0.09828 |  0.44396 | 0.464 | 0.448 | 0.405 | 0.444 | 0.465 | 0.438 | 0:16:15 |


[SAVED] EPOCH: 3 | MCRMSE: 0.44395920634269714


----------------------------------- FOLD 2 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.44396     0.46374   0.44782       0.40471        0.44394    0.46519        0.43834

################################### END OF FOlD 2 ###################################


Date: 2022-11-26 15:01:25.336242+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: roberta-large
Model_config: RobertaConfig {
  "_name_or_path": "roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1098}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 512

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 1/3 | STEP: 000/366 | LOSS: 0.09122 (0.09122) | LR: 0.00000033 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 1/3 | STEP: 040/366 | LOSS: 0.12595 (0.12284) | LR: 0.00001352 | TIME: 0:01:39 |
[TRAIN F3] EPOCH: 1/3 | STEP: 080/366 | LOSS: 0.20118 (0.13091) | LR: 0.00002670 | TIME: 0:03:14 |
[TRAIN F3] EPOCH: 1/3 | STEP: 120/366 | LOSS: 0.15175 (0.14275) | LR: 0.00002993 | TIME: 0:04:50 |
[TRAIN F3] EPOCH: 1/3 | STEP: 160/366 | LOSS: 0.13386 (0.14249) | LR: 0.00002964 | TIME: 0:06:27 |
[TRAIN F3] EPOCH: 1/3 | STEP: 200/366 | LOSS: 0.12238 (0.14105) | LR: 0.00002913 | TIME: 0:08:02 |
[TRAIN F3] EPOCH: 1/3 | STEP: 240/366 | LOSS: 0.11488 (0.14026) | LR: 0.00002839 | TIME: 0:09:38 |
[TRAIN F3] EPOCH: 1/3 | STEP: 280/366 | LOSS: 0.12953 (0.14061) | LR: 0.00002744 | TIME: 0:11:13 |
[TRAIN F3] EPOCH: 1/3 | STEP: 320/366 | LOSS: 0.07091 (0.14044) | LR: 0.00002630 | TIME: 0:12:49 |
[TRAIN F3] EPOCH: 1/3 | STEP: 360/366 | LOSS: 0.10926 (0.13993) | LR: 0.00002499 | TIME: 0:14:25 |
[TRAIN F3] EPOCH: 1/3 | STEP: 365/366 | LOSS: 0.06667 (0.13947) | LR: 0.00002481 | TIME: 0:14:37 |

VALID_LOOP
[VALID F3] EPOCH: 1/3 | STEP: 000/123 | LOSS: 0.13244 (0.13244) | TIME: 0:00:01 |
[VALID F3] EPOCH: 1/3 | STEP: 040/123 | LOSS: 0.16971 (0.13389) | TIME: 0:00:33 |
[VALID F3] EPOCH: 1/3 | STEP: 080/123 | LOSS: 0.09295 (0.13483) | TIME: 0:01:04 |
[VALID F3] EPOCH: 1/3 | STEP: 120/123 | LOSS: 0.09119 (0.13759) | TIME: 0:01:36 |
[VALID F3] EPOCH: 1/3 | STEP: 122/123 | LOSS: 0.12123 (0.13777) | TIME: 0:01:37 |

--------------------
EPOCH: 1/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/3     |      0.13947 |      0.13777 |  0.52396 | 0.515 | 0.626 | 0.448 | 0.607 | 0.484 | 0.464 | 0:16:14 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5239636898040771

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 2/3 | STEP: 000/366 | LOSS: 0.14727 (0.14727) | LR: 0.00002477 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 2/3 | STEP: 040/366 | LOSS: 0.09304 (0.11481) | LR: 0.00002328 | TIME: 0:01:39 |
[TRAIN F3] EPOCH: 2/3 | STEP: 080/366 | LOSS: 0.09318 (0.11197) | LR: 0.00002166 | TIME: 0:03:15 |
[TRAIN F3] EPOCH: 2/3 | STEP: 120/366 | LOSS: 0.18287 (0.11531) | LR: 0.00001994 | TIME: 0:04:50 |
[TRAIN F3] EPOCH: 2/3 | STEP: 160/366 | LOSS: 0.08133 (0.11430) | LR: 0.00001814 | TIME: 0:06:27 |
[TRAIN F3] EPOCH: 2/3 | STEP: 200/366 | LOSS: 0.08798 (0.11274) | LR: 0.00001629 | TIME: 0:08:03 |
[TRAIN F3] EPOCH: 2/3 | STEP: 240/366 | LOSS: 0.09001 (0.11184) | LR: 0.00001442 | TIME: 0:09:39 |
[TRAIN F3] EPOCH: 2/3 | STEP: 280/366 | LOSS: 0.10041 (0.11228) | LR: 0.00001255 | TIME: 0:11:14 |
[TRAIN F3] EPOCH: 2/3 | STEP: 320/366 | LOSS: 0.09680 (0.11138) | LR: 0.00001073 | TIME: 0:12:50 |
[TRAIN F3] EPOCH: 2/3 | STEP: 360/366 | LOSS: 0.10190 (0.10977) | LR: 0.00000897 | TIME: 0:14:25 |
[TRAIN F3] EPOCH: 2/3 | STEP: 365/366 | LOSS: 0.08377 (0.10959) | LR: 0.00000876 | TIME: 0:14:37 |

VALID_LOOP
[VALID F3] EPOCH: 2/3 | STEP: 000/123 | LOSS: 0.14663 (0.14663) | TIME: 0:00:01 |
[VALID F3] EPOCH: 2/3 | STEP: 040/123 | LOSS: 0.12322 (0.10441) | TIME: 0:00:33 |
[VALID F3] EPOCH: 2/3 | STEP: 080/123 | LOSS: 0.08439 (0.10417) | TIME: 0:01:04 |
[VALID F3] EPOCH: 2/3 | STEP: 120/123 | LOSS: 0.09579 (0.10462) | TIME: 0:01:36 |
[VALID F3] EPOCH: 2/3 | STEP: 122/123 | LOSS: 0.05221 (0.10461) | TIME: 0:01:37 |

--------------------
EPOCH: 2/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/3     |      0.10959 |      0.10461 |  0.45845 | 0.482 | 0.444 | 0.439 | 0.457 | 0.476 | 0.453 | 0:16:14 |


[SAVED] EPOCH: 2 | MCRMSE: 0.45844629406929016

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 3/3 | STEP: 000/366 | LOSS: 0.09559 (0.09559) | LR: 0.00000872 | TIME: 0:00:03 |
[TRAIN F3] EPOCH: 3/3 | STEP: 040/366 | LOSS: 0.07091 (0.08777) | LR: 0.00000707 | TIME: 0:01:39 |
[TRAIN F3] EPOCH: 3/3 | STEP: 080/366 | LOSS: 0.08982 (0.08704) | LR: 0.00000555 | TIME: 0:03:14 |
[TRAIN F3] EPOCH: 3/3 | STEP: 120/366 | LOSS: 0.09413 (0.08718) | LR: 0.00000417 | TIME: 0:04:50 |
[TRAIN F3] EPOCH: 3/3 | STEP: 160/366 | LOSS: 0.04576 (0.08665) | LR: 0.00000296 | TIME: 0:06:25 |
[TRAIN F3] EPOCH: 3/3 | STEP: 200/366 | LOSS: 0.10470 (0.08696) | LR: 0.00000194 | TIME: 0:08:02 |
[TRAIN F3] EPOCH: 3/3 | STEP: 240/366 | LOSS: 0.12883 (0.08702) | LR: 0.00000113 | TIME: 0:09:37 |
[TRAIN F3] EPOCH: 3/3 | STEP: 280/366 | LOSS: 0.07618 (0.08753) | LR: 0.00000052 | TIME: 0:11:14 |
[TRAIN F3] EPOCH: 3/3 | STEP: 320/366 | LOSS: 0.09464 (0.08737) | LR: 0.00000015 | TIME: 0:12:51 |
[TRAIN F3] EPOCH: 3/3 | STEP: 360/366 | LOSS: 0.13720 (0.08697) | LR: 0.00000000 | TIME: 0:14:26 |
[TRAIN F3] EPOCH: 3/3 | STEP: 365/366 | LOSS: 0.09280 (0.08704) | LR: 0.00000000 | TIME: 0:14:38 |

VALID_LOOP
[VALID F3] EPOCH: 3/3 | STEP: 000/123 | LOSS: 0.13793 (0.13793) | TIME: 0:00:01 |
[VALID F3] EPOCH: 3/3 | STEP: 040/123 | LOSS: 0.12576 (0.10269) | TIME: 0:00:33 |
[VALID F3] EPOCH: 3/3 | STEP: 080/123 | LOSS: 0.08605 (0.10222) | TIME: 0:01:04 |
[VALID F3] EPOCH: 3/3 | STEP: 120/123 | LOSS: 0.09162 (0.10176) | TIME: 0:01:36 |
[VALID F3] EPOCH: 3/3 | STEP: 122/123 | LOSS: 0.04292 (0.10183) | TIME: 0:01:37 |

--------------------
EPOCH: 3/3 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/3     |      0.08704 |      0.10183 |  0.45202 | 0.477 | 0.443 | 0.420 | 0.452 | 0.478 | 0.441 | 0:16:16 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4520159065723419


----------------------------------- FOLD 3 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45202     0.47748   0.44281       0.42022        0.45248    0.47821         0.4409

################################### END OF FOlD 3 ###################################


