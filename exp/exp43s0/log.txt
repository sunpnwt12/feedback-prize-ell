Date: 2022-11-12 17:32:41.760757+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-base
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: dynamic_padding

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.46127 (2.46127) | LR: 0.00000033 | TIME: 0:00:09 |
[TRAIN F0] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.24264 (1.83214) | LR: 0.00001352 | TIME: 0:02:32 |
[TRAIN F0] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.15389 (1.04051) | LR: 0.00002670 | TIME: 0:04:50 |
[TRAIN F0] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.23477 (0.74794) | LR: 0.00002996 | TIME: 0:07:21 |
[TRAIN F0] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.13269 (0.59785) | LR: 0.00002981 | TIME: 0:10:08 |
[TRAIN F0] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.11398 (0.50395) | LR: 0.00002953 | TIME: 0:12:49 |
[TRAIN F0] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.09445 (0.43910) | LR: 0.00002913 | TIME: 0:15:01 |
[TRAIN F0] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.14047 (0.39655) | LR: 0.00002860 | TIME: 0:17:42 |
[TRAIN F0] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.12588 (0.36285) | LR: 0.00002797 | TIME: 0:20:02 |
[TRAIN F0] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.08583 (0.33664) | LR: 0.00002723 | TIME: 0:22:22 |
[TRAIN F0] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.13574 (0.33393) | LR: 0.00002713 | TIME: 0:22:46 |

VALID_LOOP
[VALID F0] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.10704 (0.10704) | TIME: 0:00:01 |
[VALID F0] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.10830 (0.12590) | TIME: 0:00:46 |
[VALID F0] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.15642 (0.12385) | TIME: 0:01:30 |
[VALID F0] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.15858 (0.12400) | TIME: 0:02:14 |
[VALID F0] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.09502 (0.12376) | TIME: 0:02:15 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.33393 |      0.12376 |    0.499 | 0.563 | 0.486 | 0.504 | 0.469 | 0.489 | 0.482 | 0:25:01 |


[SAVED] EPOCH: 1 | MCRMSE: 0.49899569153785706

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.08020 (0.08020) | LR: 0.00002711 | TIME: 0:00:04 |
[TRAIN F0] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.16485 (0.11717) | LR: 0.00002625 | TIME: 0:02:24 |
[TRAIN F0] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.08970 (0.11742) | LR: 0.00002529 | TIME: 0:04:44 |
[TRAIN F0] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.11938 (0.11520) | LR: 0.00002425 | TIME: 0:07:07 |
[TRAIN F0] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.09687 (0.11395) | LR: 0.00002313 | TIME: 0:09:36 |
[TRAIN F0] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.13748 (0.11367) | LR: 0.00002195 | TIME: 0:12:10 |
[TRAIN F0] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.11080 (0.11443) | LR: 0.00002070 | TIME: 0:14:36 |
[TRAIN F0] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.10162 (0.11398) | LR: 0.00001941 | TIME: 0:17:10 |
[TRAIN F0] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.11111 (0.11351) | LR: 0.00001808 | TIME: 0:19:30 |
[TRAIN F0] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.06714 (0.11330) | LR: 0.00001673 | TIME: 0:21:57 |
[TRAIN F0] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.08497 (0.11318) | LR: 0.00001656 | TIME: 0:22:17 |

VALID_LOOP
[VALID F0] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.09795 (0.09795) | TIME: 0:00:01 |
[VALID F0] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.09789 (0.11013) | TIME: 0:00:45 |
[VALID F0] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.11803 (0.10733) | TIME: 0:01:29 |
[VALID F0] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.16547 (0.10754) | TIME: 0:02:13 |
[VALID F0] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.06466 (0.10720) | TIME: 0:02:15 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11318 |       0.1072 |  0.46443 | 0.493 | 0.448 | 0.437 | 0.468 | 0.484 | 0.457 | 0:24:32 |


[SAVED] EPOCH: 2 | MCRMSE: 0.46442854404449463

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.09847 (0.09847) | LR: 0.00001652 | TIME: 0:00:07 |
[TRAIN F0] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.07588 (0.11196) | LR: 0.00001515 | TIME: 0:02:43 |
[TRAIN F0] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.06923 (0.10649) | LR: 0.00001378 | TIME: 0:05:07 |
[TRAIN F0] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.13852 (0.10482) | LR: 0.00001242 | TIME: 0:07:26 |
[TRAIN F0] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.08781 (0.10226) | LR: 0.00001108 | TIME: 0:09:55 |
[TRAIN F0] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.07347 (0.10236) | LR: 0.00000977 | TIME: 0:12:32 |
[TRAIN F0] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.05211 (0.10183) | LR: 0.00000851 | TIME: 0:15:07 |
[TRAIN F0] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.18352 (0.10141) | LR: 0.00000730 | TIME: 0:17:29 |
[TRAIN F0] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.11590 (0.10103) | LR: 0.00000616 | TIME: 0:19:55 |
[TRAIN F0] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.13609 (0.10002) | LR: 0.00000509 | TIME: 0:21:59 |
[TRAIN F0] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.09829 (0.10000) | LR: 0.00000496 | TIME: 0:22:17 |

VALID_LOOP
[VALID F0] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.10904 (0.10904) | TIME: 0:00:01 |
[VALID F0] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.11061 (0.11015) | TIME: 0:00:45 |
[VALID F0] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.10871 (0.10615) | TIME: 0:01:29 |
[VALID F0] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.13698 (0.10651) | TIME: 0:02:13 |
[VALID F0] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.05949 (0.10606) | TIME: 0:02:15 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |          0.1 |      0.10606 |  0.46148 | 0.498 | 0.440 | 0.438 | 0.458 | 0.473 | 0.461 | 0:24:32 |


[SAVED] EPOCH: 3 | MCRMSE: 0.46147868037223816

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.05577 (0.05577) | LR: 0.00000493 | TIME: 0:00:05 |
[TRAIN F0] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.10660 (0.09101) | LR: 0.00000396 | TIME: 0:02:39 |
[TRAIN F0] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.07829 (0.09143) | LR: 0.00000308 | TIME: 0:05:03 |
[TRAIN F0] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.11423 (0.09127) | LR: 0.00000230 | TIME: 0:07:21 |
[TRAIN F0] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.10688 (0.09230) | LR: 0.00000162 | TIME: 0:09:53 |
[TRAIN F0] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.05590 (0.09155) | LR: 0.00000106 | TIME: 0:12:49 |
[TRAIN F0] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.08899 (0.09128) | LR: 0.00000061 | TIME: 0:15:00 |
[TRAIN F0] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.10227 (0.09089) | LR: 0.00000028 | TIME: 0:17:13 |
[TRAIN F0] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.12209 (0.09099) | LR: 0.00000008 | TIME: 0:19:29 |
[TRAIN F0] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.05047 (0.09176) | LR: 0.00000000 | TIME: 0:22:06 |
[TRAIN F0] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.08031 (0.09164) | LR: 0.00000000 | TIME: 0:22:26 |

VALID_LOOP
[VALID F0] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.10339 (0.10339) | TIME: 0:00:01 |
[VALID F0] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.09376 (0.10541) | TIME: 0:00:45 |
[VALID F0] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.11177 (0.10320) | TIME: 0:01:29 |
[VALID F0] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.15530 (0.10390) | TIME: 0:02:14 |
[VALID F0] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.06400 (0.10354) | TIME: 0:02:15 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.09164 |      0.10354 |  0.45605 | 0.482 | 0.443 | 0.439 | 0.447 | 0.472 | 0.454 | 0:24:41 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4560467302799225


----------------------------------- FOLD 0 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45605     0.48155   0.44289       0.43867        0.44705    0.47244        0.45368

################################### END OF FOlD 0 ###################################


Date: 2022-11-12 19:11:46.453489+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-base
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: dynamic_padding

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.58044 (2.58044) | LR: 0.00000033 | TIME: 0:00:07 |
[TRAIN F1] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.39680 (1.87855) | LR: 0.00001352 | TIME: 0:02:35 |
[TRAIN F1] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.22715 (1.05343) | LR: 0.00002670 | TIME: 0:05:03 |
[TRAIN F1] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.10359 (0.75415) | LR: 0.00002996 | TIME: 0:07:31 |
[TRAIN F1] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.16425 (0.60252) | LR: 0.00002981 | TIME: 0:09:54 |
[TRAIN F1] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.18103 (0.50997) | LR: 0.00002953 | TIME: 0:12:26 |
[TRAIN F1] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.08296 (0.44681) | LR: 0.00002913 | TIME: 0:14:59 |
[TRAIN F1] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.09345 (0.40070) | LR: 0.00002860 | TIME: 0:17:09 |
[TRAIN F1] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.07094 (0.36653) | LR: 0.00002797 | TIME: 0:19:20 |
[TRAIN F1] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.12261 (0.33977) | LR: 0.00002723 | TIME: 0:21:46 |
[TRAIN F1] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.16432 (0.33704) | LR: 0.00002713 | TIME: 0:22:03 |

VALID_LOOP
[VALID F1] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.09952 (0.09952) | TIME: 0:00:01 |
[VALID F1] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.07576 (0.12452) | TIME: 0:00:46 |
[VALID F1] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.12547 (0.12456) | TIME: 0:01:30 |
[VALID F1] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.11363 (0.12344) | TIME: 0:02:15 |
[VALID F1] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.12570 (0.12314) | TIME: 0:02:16 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.33704 |      0.12314 |  0.49794 | 0.510 | 0.538 | 0.452 | 0.489 | 0.540 | 0.459 | 0:24:19 |


[SAVED] EPOCH: 1 | MCRMSE: 0.49793657660484314

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.12612 (0.12612) | LR: 0.00002711 | TIME: 0:00:05 |
[TRAIN F1] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.09103 (0.10497) | LR: 0.00002625 | TIME: 0:02:24 |
[TRAIN F1] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.11063 (0.10957) | LR: 0.00002529 | TIME: 0:04:57 |
[TRAIN F1] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.10209 (0.11353) | LR: 0.00002425 | TIME: 0:07:20 |
[TRAIN F1] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.14842 (0.11696) | LR: 0.00002313 | TIME: 0:09:51 |
[TRAIN F1] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.09096 (0.11488) | LR: 0.00002195 | TIME: 0:11:55 |
[TRAIN F1] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.08177 (0.11528) | LR: 0.00002070 | TIME: 0:14:40 |
[TRAIN F1] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.11850 (0.11599) | LR: 0.00001941 | TIME: 0:16:55 |
[TRAIN F1] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.07674 (0.11468) | LR: 0.00001808 | TIME: 0:19:29 |
[TRAIN F1] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.06610 (0.11401) | LR: 0.00001673 | TIME: 0:21:54 |
[TRAIN F1] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.11429 (0.11389) | LR: 0.00001656 | TIME: 0:22:10 |

VALID_LOOP
[VALID F1] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.07831 (0.07831) | TIME: 0:00:02 |
[VALID F1] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.08445 (0.11045) | TIME: 0:00:46 |
[VALID F1] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.10908 (0.11066) | TIME: 0:01:30 |
[VALID F1] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.08529 (0.11252) | TIME: 0:02:14 |
[VALID F1] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.10198 (0.11234) | TIME: 0:02:16 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11389 |      0.11234 |  0.47485 | 0.516 | 0.456 | 0.419 | 0.464 | 0.494 | 0.500 | 0:24:27 |


[SAVED] EPOCH: 2 | MCRMSE: 0.4748467206954956

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.11483 (0.11483) | LR: 0.00001652 | TIME: 0:00:02 |
[TRAIN F1] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.09960 (0.10553) | LR: 0.00001515 | TIME: 0:02:31 |
[TRAIN F1] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.11363 (0.10127) | LR: 0.00001378 | TIME: 0:05:10 |
[TRAIN F1] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.11322 (0.09925) | LR: 0.00001242 | TIME: 0:07:30 |
[TRAIN F1] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.08942 (0.09866) | LR: 0.00001108 | TIME: 0:09:53 |
[TRAIN F1] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.12169 (0.09966) | LR: 0.00000977 | TIME: 0:12:19 |
[TRAIN F1] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.11363 (0.09967) | LR: 0.00000851 | TIME: 0:14:47 |
[TRAIN F1] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.09301 (0.09980) | LR: 0.00000730 | TIME: 0:17:08 |
[TRAIN F1] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.07938 (0.09995) | LR: 0.00000616 | TIME: 0:19:31 |
[TRAIN F1] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.12270 (0.10078) | LR: 0.00000509 | TIME: 0:21:45 |
[TRAIN F1] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.07629 (0.10071) | LR: 0.00000496 | TIME: 0:22:01 |

VALID_LOOP
[VALID F1] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.07875 (0.07875) | TIME: 0:00:01 |
[VALID F1] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.08003 (0.10714) | TIME: 0:00:46 |
[VALID F1] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.10411 (0.10767) | TIME: 0:01:31 |
[VALID F1] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.09138 (0.10774) | TIME: 0:02:16 |
[VALID F1] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.09505 (0.10755) | TIME: 0:02:17 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.10071 |      0.10755 |  0.46463 | 0.499 | 0.465 | 0.412 | 0.476 | 0.475 | 0.462 | 0:24:19 |


[SAVED] EPOCH: 3 | MCRMSE: 0.46463051438331604

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.09281 (0.09281) | LR: 0.00000493 | TIME: 0:00:04 |
[TRAIN F1] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.10654 (0.09576) | LR: 0.00000396 | TIME: 0:02:24 |
[TRAIN F1] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.09276 (0.09660) | LR: 0.00000308 | TIME: 0:04:56 |
[TRAIN F1] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.08642 (0.09397) | LR: 0.00000230 | TIME: 0:07:24 |
[TRAIN F1] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.09560 (0.09398) | LR: 0.00000162 | TIME: 0:09:41 |
[TRAIN F1] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.10374 (0.09390) | LR: 0.00000106 | TIME: 0:11:58 |
[TRAIN F1] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.12654 (0.09357) | LR: 0.00000061 | TIME: 0:14:27 |
[TRAIN F1] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.07008 (0.09332) | LR: 0.00000028 | TIME: 0:17:01 |
[TRAIN F1] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.06163 (0.09366) | LR: 0.00000008 | TIME: 0:19:32 |
[TRAIN F1] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.07792 (0.09309) | LR: 0.00000000 | TIME: 0:21:52 |
[TRAIN F1] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.09727 (0.09295) | LR: 0.00000000 | TIME: 0:22:11 |

VALID_LOOP
[VALID F1] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.07292 (0.07292) | TIME: 0:00:01 |
[VALID F1] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.07151 (0.10296) | TIME: 0:00:46 |
[VALID F1] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.10389 (0.10303) | TIME: 0:01:31 |
[VALID F1] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.08159 (0.10393) | TIME: 0:02:16 |
[VALID F1] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.08721 (0.10378) | TIME: 0:02:17 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.09295 |      0.10378 |  0.45623 | 0.497 | 0.450 | 0.410 | 0.463 | 0.476 | 0.441 | 0:24:29 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4562300741672516


----------------------------------- FOLD 1 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45623     0.49706   0.45025       0.40996        0.46251     0.4763         0.4413

################################### END OF FOlD 1 ###################################


Date: 2022-11-12 20:49:39.186096+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-base
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: dynamic_padding

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.58601 (2.58601) | LR: 0.00000033 | TIME: 0:00:04 |
[TRAIN F2] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.23961 (1.84623) | LR: 0.00001352 | TIME: 0:02:31 |
[TRAIN F2] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.18609 (1.04934) | LR: 0.00002670 | TIME: 0:04:52 |
[TRAIN F2] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.18388 (0.75628) | LR: 0.00002996 | TIME: 0:07:18 |
[TRAIN F2] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.15271 (0.60347) | LR: 0.00002981 | TIME: 0:09:37 |
[TRAIN F2] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.13645 (0.51063) | LR: 0.00002953 | TIME: 0:12:06 |
[TRAIN F2] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.16838 (0.44832) | LR: 0.00002913 | TIME: 0:14:30 |
[TRAIN F2] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.11917 (0.40183) | LR: 0.00002860 | TIME: 0:16:47 |
[TRAIN F2] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.10614 (0.36760) | LR: 0.00002797 | TIME: 0:19:08 |
[TRAIN F2] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.11116 (0.34144) | LR: 0.00002723 | TIME: 0:21:45 |
[TRAIN F2] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.19226 (0.33827) | LR: 0.00002713 | TIME: 0:22:05 |

VALID_LOOP
[VALID F2] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.11575 (0.11575) | TIME: 0:00:02 |
[VALID F2] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.07025 (0.11096) | TIME: 0:00:48 |
[VALID F2] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.11115 (0.11365) | TIME: 0:01:34 |
[VALID F2] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.07196 (0.11195) | TIME: 0:02:20 |
[VALID F2] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.13300 (0.11179) | TIME: 0:02:22 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.33827 |      0.11179 |  0.47373 | 0.495 | 0.469 | 0.432 | 0.464 | 0.529 | 0.454 | 0:24:27 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4737284183502197

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.06838 (0.06838) | LR: 0.00002711 | TIME: 0:00:04 |
[TRAIN F2] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.09167 (0.11108) | LR: 0.00002625 | TIME: 0:02:35 |
[TRAIN F2] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.13971 (0.11305) | LR: 0.00002529 | TIME: 0:04:48 |
[TRAIN F2] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.12416 (0.11369) | LR: 0.00002425 | TIME: 0:07:18 |
[TRAIN F2] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.16953 (0.11479) | LR: 0.00002313 | TIME: 0:09:38 |
[TRAIN F2] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.12700 (0.11467) | LR: 0.00002195 | TIME: 0:12:07 |
[TRAIN F2] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.11267 (0.11592) | LR: 0.00002070 | TIME: 0:14:39 |
[TRAIN F2] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.13836 (0.11607) | LR: 0.00001941 | TIME: 0:16:55 |
[TRAIN F2] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.11386 (0.11490) | LR: 0.00001808 | TIME: 0:19:09 |
[TRAIN F2] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.15535 (0.11487) | LR: 0.00001673 | TIME: 0:21:27 |
[TRAIN F2] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.09846 (0.11475) | LR: 0.00001656 | TIME: 0:21:52 |

VALID_LOOP
[VALID F2] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.10202 (0.10202) | TIME: 0:00:02 |
[VALID F2] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.06503 (0.10552) | TIME: 0:00:48 |
[VALID F2] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.08642 (0.10674) | TIME: 0:01:34 |
[VALID F2] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.06098 (0.10599) | TIME: 0:02:20 |
[VALID F2] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.12597 (0.10580) | TIME: 0:02:22 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11475 |       0.1058 |  0.46068 | 0.493 | 0.453 | 0.418 | 0.457 | 0.480 | 0.463 | 0:24:13 |


[SAVED] EPOCH: 2 | MCRMSE: 0.46067753434181213

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.08587 (0.08587) | LR: 0.00001652 | TIME: 0:00:05 |
[TRAIN F2] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.13117 (0.10210) | LR: 0.00001515 | TIME: 0:02:26 |
[TRAIN F2] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.06867 (0.09864) | LR: 0.00001378 | TIME: 0:04:45 |
[TRAIN F2] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.12376 (0.10042) | LR: 0.00001242 | TIME: 0:07:14 |
[TRAIN F2] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.10645 (0.10082) | LR: 0.00001108 | TIME: 0:09:51 |
[TRAIN F2] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.07292 (0.10241) | LR: 0.00000977 | TIME: 0:12:15 |
[TRAIN F2] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.13465 (0.10323) | LR: 0.00000851 | TIME: 0:14:36 |
[TRAIN F2] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.05660 (0.10230) | LR: 0.00000730 | TIME: 0:16:53 |
[TRAIN F2] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.09130 (0.10179) | LR: 0.00000616 | TIME: 0:19:06 |
[TRAIN F2] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.07826 (0.10101) | LR: 0.00000509 | TIME: 0:21:37 |
[TRAIN F2] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.13325 (0.10096) | LR: 0.00000496 | TIME: 0:21:54 |

VALID_LOOP
[VALID F2] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.09961 (0.09961) | TIME: 0:00:02 |
[VALID F2] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.07359 (0.10442) | TIME: 0:00:48 |
[VALID F2] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.07552 (0.10487) | TIME: 0:01:35 |
[VALID F2] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.05963 (0.10403) | TIME: 0:02:21 |
[VALID F2] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.09221 (0.10378) | TIME: 0:02:23 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.10096 |      0.10378 |  0.45604 | 0.491 | 0.454 | 0.409 | 0.461 | 0.479 | 0.443 | 0:24:17 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4560425281524658

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.12610 (0.12610) | LR: 0.00000493 | TIME: 0:00:05 |
[TRAIN F2] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.07867 (0.09681) | LR: 0.00000396 | TIME: 0:02:19 |
[TRAIN F2] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.05948 (0.09169) | LR: 0.00000308 | TIME: 0:04:25 |
[TRAIN F2] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.10844 (0.09256) | LR: 0.00000230 | TIME: 0:06:29 |
[TRAIN F2] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.08279 (0.09361) | LR: 0.00000162 | TIME: 0:09:04 |
[TRAIN F2] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.11321 (0.09360) | LR: 0.00000106 | TIME: 0:11:25 |
[TRAIN F2] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.09410 (0.09290) | LR: 0.00000061 | TIME: 0:13:47 |
[TRAIN F2] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.09910 (0.09270) | LR: 0.00000028 | TIME: 0:16:32 |
[TRAIN F2] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.08923 (0.09238) | LR: 0.00000008 | TIME: 0:18:54 |
[TRAIN F2] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.05881 (0.09240) | LR: 0.00000000 | TIME: 0:21:23 |
[TRAIN F2] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.06196 (0.09229) | LR: 0.00000000 | TIME: 0:21:43 |

VALID_LOOP
[VALID F2] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.09136 (0.09136) | TIME: 0:00:02 |
[VALID F2] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.06331 (0.10116) | TIME: 0:00:49 |
[VALID F2] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.08078 (0.10212) | TIME: 0:01:35 |
[VALID F2] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.05834 (0.10163) | TIME: 0:02:22 |
[VALID F2] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.07629 (0.10139) | TIME: 0:02:24 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.09229 |      0.10139 |  0.45081 | 0.480 | 0.451 | 0.408 | 0.455 | 0.472 | 0.440 | 0:24:07 |


[SAVED] EPOCH: 4 | MCRMSE: 0.45081356167793274


----------------------------------- FOLD 2 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45081     0.47988   0.45122       0.40753        0.45464     0.4721        0.43951

################################### END OF FOlD 2 ###################################


Date: 2022-11-12 22:27:09.306509+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: microsoft/deberta-v3-base
Model_config: DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0.0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.20.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: dynamic_padding

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.53937 (2.53937) | LR: 0.00000033 | TIME: 0:00:02 |
[TRAIN F3] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.41580 (1.80990) | LR: 0.00001352 | TIME: 0:02:29 |
[TRAIN F3] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.09314 (1.03089) | LR: 0.00002670 | TIME: 0:05:01 |
[TRAIN F3] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.10630 (0.73786) | LR: 0.00002996 | TIME: 0:07:21 |
[TRAIN F3] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.15175 (0.58841) | LR: 0.00002981 | TIME: 0:09:38 |
[TRAIN F3] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.06967 (0.49863) | LR: 0.00002953 | TIME: 0:12:20 |
[TRAIN F3] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.13012 (0.43703) | LR: 0.00002913 | TIME: 0:14:58 |
[TRAIN F3] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.20565 (0.39328) | LR: 0.00002860 | TIME: 0:17:08 |
[TRAIN F3] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.12707 (0.36067) | LR: 0.00002797 | TIME: 0:19:41 |
[TRAIN F3] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.11526 (0.33344) | LR: 0.00002723 | TIME: 0:22:10 |
[TRAIN F3] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.12270 (0.33087) | LR: 0.00002713 | TIME: 0:22:31 |

VALID_LOOP
[VALID F3] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.12368 (0.12368) | TIME: 0:00:01 |
[VALID F3] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.13273 (0.11331) | TIME: 0:00:44 |
[VALID F3] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.12177 (0.10951) | TIME: 0:01:27 |
[VALID F3] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.08780 (0.11100) | TIME: 0:02:10 |
[VALID F3] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.04761 (0.11099) | TIME: 0:02:11 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.33087 |      0.11099 |  0.47203 | 0.534 | 0.445 | 0.443 | 0.468 | 0.485 | 0.456 | 0:24:42 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4720257818698883

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.11092 (0.11092) | LR: 0.00002711 | TIME: 0:00:05 |
[TRAIN F3] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.10457 (0.12134) | LR: 0.00002625 | TIME: 0:02:39 |
[TRAIN F3] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.17734 (0.12095) | LR: 0.00002529 | TIME: 0:04:52 |
[TRAIN F3] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.09085 (0.11995) | LR: 0.00002425 | TIME: 0:07:25 |
[TRAIN F3] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.12030 (0.11841) | LR: 0.00002313 | TIME: 0:09:47 |
[TRAIN F3] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.12509 (0.11876) | LR: 0.00002195 | TIME: 0:12:20 |
[TRAIN F3] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.10440 (0.11991) | LR: 0.00002070 | TIME: 0:14:32 |
[TRAIN F3] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.10736 (0.11779) | LR: 0.00001941 | TIME: 0:17:08 |
[TRAIN F3] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.12625 (0.11711) | LR: 0.00001808 | TIME: 0:19:36 |
[TRAIN F3] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.11270 (0.11571) | LR: 0.00001673 | TIME: 0:22:20 |
[TRAIN F3] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.10875 (0.11558) | LR: 0.00001656 | TIME: 0:22:41 |

VALID_LOOP
[VALID F3] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.12091 (0.12091) | TIME: 0:00:01 |
[VALID F3] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.09491 (0.10329) | TIME: 0:00:44 |
[VALID F3] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.08307 (0.10202) | TIME: 0:01:27 |
[VALID F3] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.08165 (0.10410) | TIME: 0:02:10 |
[VALID F3] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.04470 (0.10386) | TIME: 0:02:11 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.11558 |      0.10386 |  0.45659 | 0.488 | 0.452 | 0.415 | 0.463 | 0.471 | 0.451 | 0:24:53 |


[SAVED] EPOCH: 2 | MCRMSE: 0.45659124851226807

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.13778 (0.13778) | LR: 0.00001652 | TIME: 0:00:05 |
[TRAIN F3] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.09354 (0.10712) | LR: 0.00001515 | TIME: 0:02:39 |
[TRAIN F3] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.10608 (0.10624) | LR: 0.00001378 | TIME: 0:05:01 |
[TRAIN F3] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.12439 (0.10393) | LR: 0.00001242 | TIME: 0:07:21 |
[TRAIN F3] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.15175 (0.10358) | LR: 0.00001108 | TIME: 0:09:52 |
[TRAIN F3] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.08420 (0.10189) | LR: 0.00000977 | TIME: 0:12:25 |
[TRAIN F3] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.11082 (0.10158) | LR: 0.00000851 | TIME: 0:14:49 |
[TRAIN F3] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.11989 (0.10288) | LR: 0.00000730 | TIME: 0:17:13 |
[TRAIN F3] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.07751 (0.10164) | LR: 0.00000616 | TIME: 0:19:36 |
[TRAIN F3] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.12890 (0.10212) | LR: 0.00000509 | TIME: 0:22:03 |
[TRAIN F3] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.08568 (0.10219) | LR: 0.00000496 | TIME: 0:22:25 |

VALID_LOOP
[VALID F3] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.11970 (0.11970) | TIME: 0:00:01 |
[VALID F3] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.09663 (0.09935) | TIME: 0:00:44 |
[VALID F3] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.08520 (0.09804) | TIME: 0:01:27 |
[VALID F3] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.07999 (0.10051) | TIME: 0:02:10 |
[VALID F3] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.05646 (0.10048) | TIME: 0:02:11 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.10219 |      0.10048 |  0.44892 | 0.482 | 0.438 | 0.411 | 0.450 | 0.467 | 0.446 | 0:24:37 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4489244222640991

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.12524 (0.12524) | LR: 0.00000493 | TIME: 0:00:04 |
[TRAIN F3] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.09897 (0.09978) | LR: 0.00000396 | TIME: 0:02:34 |
[TRAIN F3] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.06136 (0.09666) | LR: 0.00000308 | TIME: 0:04:57 |
[TRAIN F3] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.11405 (0.09489) | LR: 0.00000230 | TIME: 0:07:28 |
[TRAIN F3] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.10839 (0.09433) | LR: 0.00000162 | TIME: 0:10:07 |
[TRAIN F3] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.08826 (0.09292) | LR: 0.00000106 | TIME: 0:12:34 |
[TRAIN F3] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.12120 (0.09316) | LR: 0.00000061 | TIME: 0:14:59 |
[TRAIN F3] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.08279 (0.09354) | LR: 0.00000028 | TIME: 0:17:15 |
[TRAIN F3] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.11643 (0.09404) | LR: 0.00000008 | TIME: 0:19:46 |
[TRAIN F3] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.09534 (0.09484) | LR: 0.00000000 | TIME: 0:22:21 |
[TRAIN F3] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.08008 (0.09472) | LR: 0.00000000 | TIME: 0:22:36 |

VALID_LOOP
[VALID F3] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.11646 (0.11646) | TIME: 0:00:01 |
[VALID F3] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.10101 (0.09948) | TIME: 0:00:44 |
[VALID F3] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.09085 (0.09807) | TIME: 0:01:27 |
[VALID F3] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.08159 (0.10035) | TIME: 0:02:10 |
[VALID F3] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.05655 (0.10031) | TIME: 0:02:11 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.09472 |      0.10031 |  0.44855 | 0.480 | 0.439 | 0.411 | 0.452 | 0.466 | 0.443 | 0:24:47 |


[SAVED] EPOCH: 4 | MCRMSE: 0.44854554533958435


----------------------------------- FOLD 3 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.44855     0.48047    0.4391       0.41093        0.45171    0.46634        0.44273

################################### END OF FOlD 3 ###################################


