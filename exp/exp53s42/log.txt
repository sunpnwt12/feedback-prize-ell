Date: 2022-11-19 16:01:54.070032+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: allenai/longformer-base-4096
Model_config: LongformerConfig {
  "_name_or_path": "allenai/longformer-base-4096",
  "attention_mode": "longformer",
  "attention_probs_dropout_prob": 0.0,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "ignore_attention_mask": false,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.65949 (2.65949) | LR: 0.00000033 | TIME: 0:00:05 |
[TRAIN F0] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.59362 (1.65042) | LR: 0.00001352 | TIME: 0:02:26 |
[TRAIN F0] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.20754 (0.96852) | LR: 0.00002670 | TIME: 0:04:45 |
[TRAIN F0] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.16780 (0.71335) | LR: 0.00002996 | TIME: 0:07:06 |
[TRAIN F0] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.15063 (0.57811) | LR: 0.00002981 | TIME: 0:09:25 |
[TRAIN F0] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.06594 (0.49206) | LR: 0.00002953 | TIME: 0:11:45 |
[TRAIN F0] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.14069 (0.43349) | LR: 0.00002913 | TIME: 0:14:06 |
[TRAIN F0] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.16363 (0.39261) | LR: 0.00002860 | TIME: 0:16:25 |
[TRAIN F0] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.08310 (0.36081) | LR: 0.00002797 | TIME: 0:18:46 |
[TRAIN F0] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.25966 (0.33520) | LR: 0.00002723 | TIME: 0:21:07 |
[TRAIN F0] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.10012 (0.33225) | LR: 0.00002713 | TIME: 0:21:25 |

VALID_LOOP
[VALID F0] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.06793 (0.06793) | TIME: 0:00:01 |
[VALID F0] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.06055 (0.10435) | TIME: 0:00:16 |
[VALID F0] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.08599 (0.10645) | TIME: 0:00:32 |
[VALID F0] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.10641 (0.10900) | TIME: 0:00:47 |
[VALID F0] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.08771 (0.10877) | TIME: 0:00:48 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.33225 |      0.10877 |  0.46706 | 0.507 | 0.459 | 0.422 | 0.468 | 0.488 | 0.458 | 0:22:13 |


[SAVED] EPOCH: 1 | MCRMSE: 0.4670581519603729

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.12892 (0.12892) | LR: 0.00002711 | TIME: 0:00:04 |
[TRAIN F0] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.07734 (0.12114) | LR: 0.00002625 | TIME: 0:02:25 |
[TRAIN F0] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.11391 (0.12388) | LR: 0.00002529 | TIME: 0:04:44 |
[TRAIN F0] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.11676 (0.12540) | LR: 0.00002425 | TIME: 0:07:05 |
[TRAIN F0] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.13653 (0.12591) | LR: 0.00002313 | TIME: 0:09:26 |
[TRAIN F0] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.14567 (0.12529) | LR: 0.00002195 | TIME: 0:11:47 |
[TRAIN F0] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.10826 (0.12319) | LR: 0.00002070 | TIME: 0:14:06 |
[TRAIN F0] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.11930 (0.12241) | LR: 0.00001941 | TIME: 0:16:27 |
[TRAIN F0] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.13647 (0.12238) | LR: 0.00001808 | TIME: 0:18:47 |
[TRAIN F0] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.09464 (0.12185) | LR: 0.00001673 | TIME: 0:21:06 |
[TRAIN F0] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.12082 (0.12182) | LR: 0.00001656 | TIME: 0:21:24 |

VALID_LOOP
[VALID F0] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.07179 (0.07179) | TIME: 0:00:01 |
[VALID F0] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.07392 (0.10800) | TIME: 0:00:16 |
[VALID F0] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.10987 (0.11093) | TIME: 0:00:32 |
[VALID F0] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.10489 (0.11359) | TIME: 0:00:47 |
[VALID F0] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.13063 (0.11362) | TIME: 0:00:48 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.12182 |      0.11362 |  0.47718 | 0.512 | 0.463 | 0.433 | 0.468 | 0.478 | 0.509 | 0:22:12 |

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.11434 (0.11434) | LR: 0.00001652 | TIME: 0:00:04 |
[TRAIN F0] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.08318 (0.11772) | LR: 0.00001515 | TIME: 0:02:25 |
[TRAIN F0] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.12622 (0.11364) | LR: 0.00001378 | TIME: 0:04:46 |
[TRAIN F0] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.14115 (0.11368) | LR: 0.00001242 | TIME: 0:07:07 |
[TRAIN F0] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.08778 (0.11138) | LR: 0.00001108 | TIME: 0:09:26 |
[TRAIN F0] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.08516 (0.10870) | LR: 0.00000977 | TIME: 0:11:47 |
[TRAIN F0] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.08911 (0.10889) | LR: 0.00000851 | TIME: 0:14:07 |
[TRAIN F0] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.07913 (0.10846) | LR: 0.00000730 | TIME: 0:16:28 |
[TRAIN F0] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.11792 (0.10747) | LR: 0.00000616 | TIME: 0:18:47 |
[TRAIN F0] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.10478 (0.10673) | LR: 0.00000509 | TIME: 0:21:08 |
[TRAIN F0] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.08211 (0.10657) | LR: 0.00000496 | TIME: 0:21:26 |

VALID_LOOP
[VALID F0] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.06621 (0.06621) | TIME: 0:00:01 |
[VALID F0] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.07044 (0.10236) | TIME: 0:00:16 |
[VALID F0] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.09017 (0.10290) | TIME: 0:00:32 |
[VALID F0] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.09715 (0.10470) | TIME: 0:00:47 |
[VALID F0] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.07230 (0.10442) | TIME: 0:00:48 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.10657 |      0.10442 |  0.45737 | 0.496 | 0.448 | 0.416 | 0.456 | 0.485 | 0.443 | 0:22:14 |


[SAVED] EPOCH: 3 | MCRMSE: 0.45737019181251526

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F0] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.10849 (0.10849) | LR: 0.00000493 | TIME: 0:00:04 |
[TRAIN F0] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.05938 (0.09950) | LR: 0.00000396 | TIME: 0:02:23 |
[TRAIN F0] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.05127 (0.09498) | LR: 0.00000308 | TIME: 0:04:44 |
[TRAIN F0] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.08044 (0.09561) | LR: 0.00000230 | TIME: 0:07:05 |
[TRAIN F0] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.10257 (0.09546) | LR: 0.00000162 | TIME: 0:09:26 |
[TRAIN F0] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.07801 (0.09627) | LR: 0.00000106 | TIME: 0:11:47 |
[TRAIN F0] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.06583 (0.09590) | LR: 0.00000061 | TIME: 0:14:08 |
[TRAIN F0] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.10808 (0.09606) | LR: 0.00000028 | TIME: 0:16:29 |
[TRAIN F0] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.10990 (0.09578) | LR: 0.00000008 | TIME: 0:18:48 |
[TRAIN F0] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.12495 (0.09621) | LR: 0.00000000 | TIME: 0:21:10 |
[TRAIN F0] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.09842 (0.09616) | LR: 0.00000000 | TIME: 0:21:27 |

VALID_LOOP
[VALID F0] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.05978 (0.05978) | TIME: 0:00:01 |
[VALID F0] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.06998 (0.09998) | TIME: 0:00:16 |
[VALID F0] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.09509 (0.10130) | TIME: 0:00:32 |
[VALID F0] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.09341 (0.10293) | TIME: 0:00:47 |
[VALID F0] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.08107 (0.10269) | TIME: 0:00:48 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.09616 |      0.10269 |  0.45357 | 0.488 | 0.448 | 0.413 | 0.457 | 0.476 | 0.439 | 0:22:15 |


[SAVED] EPOCH: 4 | MCRMSE: 0.45356690883636475


----------------------------------- FOLD 0 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45357     0.48804   0.44841       0.41314        0.45707    0.47585        0.43889

################################### END OF FOlD 0 ###################################


Date: 2022-11-19 17:30:57.020231+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: allenai/longformer-base-4096
Model_config: LongformerConfig {
  "_name_or_path": "allenai/longformer-base-4096",
  "attention_mode": "longformer",
  "attention_probs_dropout_prob": 0.0,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "ignore_attention_mask": false,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.75868 (2.75868) | LR: 0.00000033 | TIME: 0:00:04 |
[TRAIN F1] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.26677 (1.62941) | LR: 0.00001352 | TIME: 0:02:25 |
[TRAIN F1] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.16309 (0.94949) | LR: 0.00002670 | TIME: 0:04:46 |
[TRAIN F1] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.14175 (0.69685) | LR: 0.00002996 | TIME: 0:07:07 |
[TRAIN F1] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.10533 (0.55963) | LR: 0.00002981 | TIME: 0:09:26 |
[TRAIN F1] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.10713 (0.47565) | LR: 0.00002953 | TIME: 0:11:47 |
[TRAIN F1] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.14554 (0.42046) | LR: 0.00002913 | TIME: 0:14:07 |
[TRAIN F1] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.13162 (0.38207) | LR: 0.00002860 | TIME: 0:16:26 |
[TRAIN F1] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.14828 (0.35212) | LR: 0.00002797 | TIME: 0:18:45 |
[TRAIN F1] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.10634 (0.32854) | LR: 0.00002723 | TIME: 0:21:04 |
[TRAIN F1] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.10453 (0.32556) | LR: 0.00002713 | TIME: 0:21:20 |

VALID_LOOP
[VALID F1] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.13020 (0.13020) | TIME: 0:00:01 |
[VALID F1] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.08581 (0.12985) | TIME: 0:00:16 |
[VALID F1] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.11325 (0.13370) | TIME: 0:00:32 |
[VALID F1] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.11366 (0.13541) | TIME: 0:00:47 |
[VALID F1] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.05795 (0.13530) | TIME: 0:00:48 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.32556 |       0.1353 |  0.52262 | 0.569 | 0.483 | 0.518 | 0.513 | 0.572 | 0.480 | 0:22:08 |


[SAVED] EPOCH: 1 | MCRMSE: 0.522622287273407

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.19216 (0.19216) | LR: 0.00002711 | TIME: 0:00:04 |
[TRAIN F1] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.08078 (0.12300) | LR: 0.00002625 | TIME: 0:02:25 |
[TRAIN F1] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.15107 (0.12464) | LR: 0.00002529 | TIME: 0:04:46 |
[TRAIN F1] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.14347 (0.12107) | LR: 0.00002425 | TIME: 0:07:05 |
[TRAIN F1] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.07837 (0.12273) | LR: 0.00002313 | TIME: 0:09:26 |
[TRAIN F1] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.07046 (0.12425) | LR: 0.00002195 | TIME: 0:11:47 |
[TRAIN F1] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.11327 (0.12254) | LR: 0.00002070 | TIME: 0:14:07 |
[TRAIN F1] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.07285 (0.12274) | LR: 0.00001941 | TIME: 0:16:28 |
[TRAIN F1] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.12165 (0.12167) | LR: 0.00001808 | TIME: 0:18:49 |
[TRAIN F1] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.10673 (0.12048) | LR: 0.00001673 | TIME: 0:21:10 |
[TRAIN F1] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.11291 (0.12041) | LR: 0.00001656 | TIME: 0:21:27 |

VALID_LOOP
[VALID F1] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.10635 (0.10635) | TIME: 0:00:01 |
[VALID F1] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.07486 (0.11467) | TIME: 0:00:16 |
[VALID F1] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.11258 (0.11506) | TIME: 0:00:32 |
[VALID F1] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.09175 (0.11463) | TIME: 0:00:47 |
[VALID F1] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.03900 (0.11478) | TIME: 0:00:48 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.12041 |      0.11478 |   0.4804 | 0.514 | 0.461 | 0.430 | 0.478 | 0.503 | 0.497 | 0:22:15 |


[SAVED] EPOCH: 2 | MCRMSE: 0.48040059208869934

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.07452 (0.07452) | LR: 0.00001652 | TIME: 0:00:04 |
[TRAIN F1] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.14212 (0.10570) | LR: 0.00001515 | TIME: 0:02:25 |
[TRAIN F1] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.11329 (0.10658) | LR: 0.00001378 | TIME: 0:04:46 |
[TRAIN F1] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.13021 (0.10722) | LR: 0.00001242 | TIME: 0:07:05 |
[TRAIN F1] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.15300 (0.10798) | LR: 0.00001108 | TIME: 0:09:24 |
[TRAIN F1] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.19784 (0.10695) | LR: 0.00000977 | TIME: 0:11:44 |
[TRAIN F1] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.09735 (0.10596) | LR: 0.00000851 | TIME: 0:14:05 |
[TRAIN F1] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.14328 (0.10599) | LR: 0.00000730 | TIME: 0:16:26 |
[TRAIN F1] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.05225 (0.10454) | LR: 0.00000616 | TIME: 0:18:45 |
[TRAIN F1] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.10161 (0.10391) | LR: 0.00000509 | TIME: 0:21:06 |
[TRAIN F1] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.13795 (0.10404) | LR: 0.00000496 | TIME: 0:21:24 |

VALID_LOOP
[VALID F1] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.08843 (0.08843) | TIME: 0:00:01 |
[VALID F1] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.08499 (0.10842) | TIME: 0:00:16 |
[VALID F1] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.10592 (0.10905) | TIME: 0:00:32 |
[VALID F1] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.08743 (0.10839) | TIME: 0:00:47 |
[VALID F1] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.04641 (0.10849) | TIME: 0:00:48 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.10404 |      0.10849 |   0.4668 | 0.497 | 0.449 | 0.427 | 0.466 | 0.498 | 0.464 | 0:22:12 |


[SAVED] EPOCH: 3 | MCRMSE: 0.466801255941391

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F1] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.06135 (0.06135) | LR: 0.00000493 | TIME: 0:00:04 |
[TRAIN F1] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.08091 (0.09574) | LR: 0.00000396 | TIME: 0:02:25 |
[TRAIN F1] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.07753 (0.09852) | LR: 0.00000308 | TIME: 0:04:46 |
[TRAIN F1] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.08516 (0.09875) | LR: 0.00000230 | TIME: 0:07:07 |
[TRAIN F1] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.08407 (0.09767) | LR: 0.00000162 | TIME: 0:09:28 |
[TRAIN F1] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.08045 (0.09709) | LR: 0.00000106 | TIME: 0:11:49 |
[TRAIN F1] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.06742 (0.09632) | LR: 0.00000061 | TIME: 0:14:10 |
[TRAIN F1] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.14857 (0.09592) | LR: 0.00000028 | TIME: 0:16:31 |
[TRAIN F1] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.09743 (0.09528) | LR: 0.00000008 | TIME: 0:18:52 |
[TRAIN F1] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.05908 (0.09487) | LR: 0.00000000 | TIME: 0:21:13 |
[TRAIN F1] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.06734 (0.09492) | LR: 0.00000000 | TIME: 0:21:31 |

VALID_LOOP
[VALID F1] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.08636 (0.08636) | TIME: 0:00:01 |
[VALID F1] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.08356 (0.10752) | TIME: 0:00:16 |
[VALID F1] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.09943 (0.10773) | TIME: 0:00:32 |
[VALID F1] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.08102 (0.10721) | TIME: 0:00:47 |
[VALID F1] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.03927 (0.10729) | TIME: 0:00:48 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.09492 |      0.10729 |  0.46418 | 0.494 | 0.450 | 0.426 | 0.464 | 0.490 | 0.461 | 0:22:19 |


[SAVED] EPOCH: 4 | MCRMSE: 0.46418285369873047


----------------------------------- FOLD 1 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.46418     0.49416   0.44987       0.42644        0.46367    0.48981        0.46115

################################### END OF FOlD 1 ###################################


Date: 2022-11-19 19:00:01.830026+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: allenai/longformer-base-4096
Model_config: LongformerConfig {
  "_name_or_path": "allenai/longformer-base-4096",
  "attention_mode": "longformer",
  "attention_probs_dropout_prob": 0.0,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "ignore_attention_mask": false,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.31104 (2.31104) | LR: 0.00000033 | TIME: 0:00:04 |
[TRAIN F2] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.32152 (1.63253) | LR: 0.00001352 | TIME: 0:02:25 |
[TRAIN F2] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.14260 (0.95721) | LR: 0.00002670 | TIME: 0:04:46 |
[TRAIN F2] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.14212 (0.69981) | LR: 0.00002996 | TIME: 0:07:07 |
[TRAIN F2] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.19059 (0.56333) | LR: 0.00002981 | TIME: 0:09:28 |
[TRAIN F2] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.15709 (0.48148) | LR: 0.00002953 | TIME: 0:11:47 |
[TRAIN F2] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.10475 (0.42531) | LR: 0.00002913 | TIME: 0:14:08 |
[TRAIN F2] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.13283 (0.38377) | LR: 0.00002860 | TIME: 0:16:28 |
[TRAIN F2] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.11638 (0.35172) | LR: 0.00002797 | TIME: 0:18:47 |
[TRAIN F2] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.15776 (0.32740) | LR: 0.00002723 | TIME: 0:21:08 |
[TRAIN F2] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.08394 (0.32465) | LR: 0.00002713 | TIME: 0:21:26 |

VALID_LOOP
[VALID F2] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.16771 (0.16771) | TIME: 0:00:01 |
[VALID F2] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.09453 (0.11912) | TIME: 0:00:16 |
[VALID F2] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.10767 (0.11956) | TIME: 0:00:32 |
[VALID F2] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.11183 (0.11794) | TIME: 0:00:47 |
[VALID F2] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.16483 (0.11768) | TIME: 0:00:48 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.32465 |      0.11768 |  0.48722 | 0.512 | 0.465 | 0.440 | 0.486 | 0.514 | 0.507 | 0:22:14 |


[SAVED] EPOCH: 1 | MCRMSE: 0.487222284078598

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.10800 (0.10800) | LR: 0.00002711 | TIME: 0:00:04 |
[TRAIN F2] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.08394 (0.11970) | LR: 0.00002625 | TIME: 0:02:25 |
[TRAIN F2] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.14208 (0.12372) | LR: 0.00002529 | TIME: 0:04:41 |
[TRAIN F2] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.13235 (0.12342) | LR: 0.00002425 | TIME: 0:07:00 |
[TRAIN F2] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.12855 (0.12205) | LR: 0.00002313 | TIME: 0:09:21 |
[TRAIN F2] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.08994 (0.12180) | LR: 0.00002195 | TIME: 0:11:38 |
[TRAIN F2] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.12176 (0.12232) | LR: 0.00002070 | TIME: 0:13:58 |
[TRAIN F2] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.11063 (0.12176) | LR: 0.00001941 | TIME: 0:16:19 |
[TRAIN F2] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.11728 (0.12172) | LR: 0.00001808 | TIME: 0:18:40 |
[TRAIN F2] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.11783 (0.12061) | LR: 0.00001673 | TIME: 0:21:01 |
[TRAIN F2] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.13593 (0.12043) | LR: 0.00001656 | TIME: 0:21:18 |

VALID_LOOP
[VALID F2] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.15428 (0.15428) | TIME: 0:00:01 |
[VALID F2] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.09707 (0.11854) | TIME: 0:00:16 |
[VALID F2] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.10102 (0.11436) | TIME: 0:00:32 |
[VALID F2] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.10252 (0.11341) | TIME: 0:00:47 |
[VALID F2] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.13738 (0.11301) | TIME: 0:00:48 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |      0.12043 |      0.11301 |  0.47727 | 0.502 | 0.467 | 0.438 | 0.494 | 0.490 | 0.472 | 0:22:06 |


[SAVED] EPOCH: 2 | MCRMSE: 0.47726738452911377

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.08351 (0.08351) | LR: 0.00001652 | TIME: 0:00:04 |
[TRAIN F2] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.10188 (0.10605) | LR: 0.00001515 | TIME: 0:02:23 |
[TRAIN F2] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.09195 (0.10525) | LR: 0.00001378 | TIME: 0:04:42 |
[TRAIN F2] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.14475 (0.10756) | LR: 0.00001242 | TIME: 0:07:03 |
[TRAIN F2] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.15182 (0.10431) | LR: 0.00001108 | TIME: 0:09:24 |
[TRAIN F2] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.09356 (0.10291) | LR: 0.00000977 | TIME: 0:11:44 |
[TRAIN F2] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.10912 (0.10253) | LR: 0.00000851 | TIME: 0:14:05 |
[TRAIN F2] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.13581 (0.10353) | LR: 0.00000730 | TIME: 0:16:24 |
[TRAIN F2] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.10862 (0.10293) | LR: 0.00000616 | TIME: 0:18:45 |
[TRAIN F2] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.10798 (0.10279) | LR: 0.00000509 | TIME: 0:21:06 |
[TRAIN F2] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.08073 (0.10254) | LR: 0.00000496 | TIME: 0:21:24 |

VALID_LOOP
[VALID F2] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.15040 (0.15040) | TIME: 0:00:01 |
[VALID F2] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.09667 (0.11331) | TIME: 0:00:16 |
[VALID F2] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.09457 (0.11057) | TIME: 0:00:32 |
[VALID F2] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.10326 (0.11013) | TIME: 0:00:47 |
[VALID F2] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.12257 (0.10977) | TIME: 0:00:48 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.10254 |      0.10977 |  0.46978 | 0.501 | 0.460 | 0.427 | 0.479 | 0.489 | 0.464 | 0:22:12 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4697820842266083

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F2] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.08970 (0.08970) | LR: 0.00000493 | TIME: 0:00:04 |
[TRAIN F2] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.07772 (0.09405) | LR: 0.00000396 | TIME: 0:02:21 |
[TRAIN F2] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.12071 (0.09327) | LR: 0.00000308 | TIME: 0:04:41 |
[TRAIN F2] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.08491 (0.09438) | LR: 0.00000230 | TIME: 0:07:00 |
[TRAIN F2] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.11738 (0.09408) | LR: 0.00000162 | TIME: 0:09:21 |
[TRAIN F2] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.09810 (0.09320) | LR: 0.00000106 | TIME: 0:11:40 |
[TRAIN F2] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.05514 (0.09385) | LR: 0.00000061 | TIME: 0:14:01 |
[TRAIN F2] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.06334 (0.09348) | LR: 0.00000028 | TIME: 0:16:22 |
[TRAIN F2] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.10644 (0.09275) | LR: 0.00000008 | TIME: 0:18:43 |
[TRAIN F2] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.10342 (0.09301) | LR: 0.00000000 | TIME: 0:21:02 |
[TRAIN F2] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.07230 (0.09324) | LR: 0.00000000 | TIME: 0:21:20 |

VALID_LOOP
[VALID F2] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.15021 (0.15021) | TIME: 0:00:01 |
[VALID F2] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.09346 (0.11223) | TIME: 0:00:16 |
[VALID F2] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.09718 (0.10936) | TIME: 0:00:32 |
[VALID F2] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.10451 (0.10893) | TIME: 0:00:47 |
[VALID F2] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.11489 (0.10854) | TIME: 0:00:48 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.09324 |      0.10854 |  0.46723 | 0.493 | 0.456 | 0.426 | 0.478 | 0.488 | 0.463 | 0:22:08 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4672313928604126


----------------------------------- FOLD 2 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.46723     0.49252   0.45559       0.42574        0.47788    0.48824        0.46342

################################### END OF FOlD 2 ###################################


Date: 2022-11-19 20:28:56.435522+07:00 (GMT+7)
Mode: CV_MODE
Train_on: cuda, (AMP: True, GradScaler: True)
Model: allenai/longformer-base-4096
Model_config: LongformerConfig {
  "_name_or_path": "allenai/longformer-base-4096",
  "attention_mode": "longformer",
  "attention_probs_dropout_prob": 0.0,
  "attention_window": [
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "ignore_attention_mask": false,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 4098,
  "model_type": "longformer",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 1,
  "pooler_dropout": 0.0,
  "position_embedding_type": "absolute",
  "sep_token_id": 2,
  "transformers_version": "4.20.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Pooling_strategy: concat_attn_mean_pooling
Initailzation: None
AWP: True (adv_lr: 2e-05, adv_eps: 0.001) at epoch 1
SWA: False (swa_lr: 1e-06, anneal_strat: cos) at last 0.112
Multi_sample_dropout: True (p: [0.3, 0.3, 0.3, 0.3, 0.3])
Loss_fn: SmoothL1Loss()
Optimizer: AdamW
LR: (Backbone: 2e-05, LowerLayer: 3e-05)
LR_Scheduler: get_cosine_schedule_with_warmup {'num_warmup_steps': 91, 'num_training_steps': 1464}
Grad_clip_norm: False (max_norm: 10)
Number_of_batches: 8 (Gradient_accumulate: 1)
max_len: 1024

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 1/4 | STEP: 000/366 | LOSS: 2.33162 (2.33162) | LR: 0.00000033 | TIME: 0:00:04 |
[TRAIN F3] EPOCH: 1/4 | STEP: 040/366 | LOSS: 0.35103 (1.68234) | LR: 0.00001352 | TIME: 0:02:25 |
[TRAIN F3] EPOCH: 1/4 | STEP: 080/366 | LOSS: 0.26988 (0.97077) | LR: 0.00002670 | TIME: 0:04:44 |
[TRAIN F3] EPOCH: 1/4 | STEP: 120/366 | LOSS: 0.12095 (0.70791) | LR: 0.00002996 | TIME: 0:07:03 |
[TRAIN F3] EPOCH: 1/4 | STEP: 160/366 | LOSS: 0.16835 (0.56902) | LR: 0.00002981 | TIME: 0:09:24 |
[TRAIN F3] EPOCH: 1/4 | STEP: 200/366 | LOSS: 0.14302 (0.48601) | LR: 0.00002953 | TIME: 0:11:44 |
[TRAIN F3] EPOCH: 1/4 | STEP: 240/366 | LOSS: 0.10587 (0.42904) | LR: 0.00002913 | TIME: 0:14:05 |
[TRAIN F3] EPOCH: 1/4 | STEP: 280/366 | LOSS: 0.17841 (0.38891) | LR: 0.00002860 | TIME: 0:16:26 |
[TRAIN F3] EPOCH: 1/4 | STEP: 320/366 | LOSS: 0.12476 (0.35685) | LR: 0.00002797 | TIME: 0:18:47 |
[TRAIN F3] EPOCH: 1/4 | STEP: 360/366 | LOSS: 0.15730 (0.33353) | LR: 0.00002723 | TIME: 0:21:08 |
[TRAIN F3] EPOCH: 1/4 | STEP: 365/366 | LOSS: 0.08892 (0.33081) | LR: 0.00002713 | TIME: 0:21:25 |

VALID_LOOP
[VALID F3] EPOCH: 1/4 | STEP: 000/123 | LOSS: 0.11714 (0.11714) | TIME: 0:00:01 |
[VALID F3] EPOCH: 1/4 | STEP: 040/123 | LOSS: 0.11225 (0.11992) | TIME: 0:00:16 |
[VALID F3] EPOCH: 1/4 | STEP: 080/123 | LOSS: 0.17516 (0.12658) | TIME: 0:00:32 |
[VALID F3] EPOCH: 1/4 | STEP: 120/123 | LOSS: 0.07778 (0.12755) | TIME: 0:00:47 |
[VALID F3] EPOCH: 1/4 | STEP: 122/123 | LOSS: 0.10122 (0.12735) | TIME: 0:00:48 |

--------------------
EPOCH: 1/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 1/4     |      0.33081 |      0.12735 |  0.50638 | 0.524 | 0.524 | 0.472 | 0.544 | 0.526 | 0.449 | 0:22:13 |


[SAVED] EPOCH: 1 | MCRMSE: 0.5063843131065369

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 2/4 | STEP: 000/366 | LOSS: 0.10748 (0.10748) | LR: 0.00002711 | TIME: 0:00:04 |
[TRAIN F3] EPOCH: 2/4 | STEP: 040/366 | LOSS: 0.09071 (0.12716) | LR: 0.00002625 | TIME: 0:02:23 |
[TRAIN F3] EPOCH: 2/4 | STEP: 080/366 | LOSS: 0.12673 (0.12621) | LR: 0.00002529 | TIME: 0:04:44 |
[TRAIN F3] EPOCH: 2/4 | STEP: 120/366 | LOSS: 0.10105 (0.12391) | LR: 0.00002425 | TIME: 0:07:03 |
[TRAIN F3] EPOCH: 2/4 | STEP: 160/366 | LOSS: 0.15112 (0.12607) | LR: 0.00002313 | TIME: 0:09:23 |
[TRAIN F3] EPOCH: 2/4 | STEP: 200/366 | LOSS: 0.14135 (0.12558) | LR: 0.00002195 | TIME: 0:11:40 |
[TRAIN F3] EPOCH: 2/4 | STEP: 240/366 | LOSS: 0.12034 (0.12532) | LR: 0.00002070 | TIME: 0:13:59 |
[TRAIN F3] EPOCH: 2/4 | STEP: 280/366 | LOSS: 0.08966 (0.12498) | LR: 0.00001941 | TIME: 0:16:19 |
[TRAIN F3] EPOCH: 2/4 | STEP: 320/366 | LOSS: 0.13062 (0.12393) | LR: 0.00001808 | TIME: 0:18:40 |
[TRAIN F3] EPOCH: 2/4 | STEP: 360/366 | LOSS: 0.08338 (0.12312) | LR: 0.00001673 | TIME: 0:20:57 |
[TRAIN F3] EPOCH: 2/4 | STEP: 365/366 | LOSS: 0.11053 (0.12270) | LR: 0.00001656 | TIME: 0:21:15 |

VALID_LOOP
[VALID F3] EPOCH: 2/4 | STEP: 000/123 | LOSS: 0.11052 (0.11052) | TIME: 0:00:01 |
[VALID F3] EPOCH: 2/4 | STEP: 040/123 | LOSS: 0.07402 (0.10095) | TIME: 0:00:16 |
[VALID F3] EPOCH: 2/4 | STEP: 080/123 | LOSS: 0.14176 (0.10560) | TIME: 0:00:32 |
[VALID F3] EPOCH: 2/4 | STEP: 120/123 | LOSS: 0.07596 (0.10682) | TIME: 0:00:47 |
[VALID F3] EPOCH: 2/4 | STEP: 122/123 | LOSS: 0.04591 (0.10651) | TIME: 0:00:48 |

--------------------
EPOCH: 2/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 2/4     |       0.1227 |      0.10651 |  0.46281 | 0.500 | 0.455 | 0.428 | 0.459 | 0.489 | 0.445 | 0:22:03 |


[SAVED] EPOCH: 2 | MCRMSE: 0.46280717849731445

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 3/4 | STEP: 000/366 | LOSS: 0.10298 (0.10298) | LR: 0.00001652 | TIME: 0:00:04 |
[TRAIN F3] EPOCH: 3/4 | STEP: 040/366 | LOSS: 0.13538 (0.10501) | LR: 0.00001515 | TIME: 0:02:23 |
[TRAIN F3] EPOCH: 3/4 | STEP: 080/366 | LOSS: 0.10844 (0.10302) | LR: 0.00001378 | TIME: 0:04:44 |
[TRAIN F3] EPOCH: 3/4 | STEP: 120/366 | LOSS: 0.17986 (0.10338) | LR: 0.00001242 | TIME: 0:07:05 |
[TRAIN F3] EPOCH: 3/4 | STEP: 160/366 | LOSS: 0.08936 (0.10270) | LR: 0.00001108 | TIME: 0:09:26 |
[TRAIN F3] EPOCH: 3/4 | STEP: 200/366 | LOSS: 0.11763 (0.10086) | LR: 0.00000977 | TIME: 0:11:45 |
[TRAIN F3] EPOCH: 3/4 | STEP: 240/366 | LOSS: 0.08223 (0.10187) | LR: 0.00000851 | TIME: 0:14:07 |
[TRAIN F3] EPOCH: 3/4 | STEP: 280/366 | LOSS: 0.08530 (0.10217) | LR: 0.00000730 | TIME: 0:16:26 |
[TRAIN F3] EPOCH: 3/4 | STEP: 320/366 | LOSS: 0.06808 (0.10241) | LR: 0.00000616 | TIME: 0:18:47 |
[TRAIN F3] EPOCH: 3/4 | STEP: 360/366 | LOSS: 0.10592 (0.10272) | LR: 0.00000509 | TIME: 0:21:06 |
[TRAIN F3] EPOCH: 3/4 | STEP: 365/366 | LOSS: 0.08083 (0.10271) | LR: 0.00000496 | TIME: 0:21:24 |

VALID_LOOP
[VALID F3] EPOCH: 3/4 | STEP: 000/123 | LOSS: 0.12347 (0.12347) | TIME: 0:00:01 |
[VALID F3] EPOCH: 3/4 | STEP: 040/123 | LOSS: 0.06703 (0.10025) | TIME: 0:00:16 |
[VALID F3] EPOCH: 3/4 | STEP: 080/123 | LOSS: 0.12623 (0.10353) | TIME: 0:00:32 |
[VALID F3] EPOCH: 3/4 | STEP: 120/123 | LOSS: 0.07547 (0.10419) | TIME: 0:00:47 |
[VALID F3] EPOCH: 3/4 | STEP: 122/123 | LOSS: 0.03851 (0.10385) | TIME: 0:00:48 |

--------------------
EPOCH: 3/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 3/4     |      0.10271 |      0.10385 |  0.45697 | 0.494 | 0.455 | 0.419 | 0.450 | 0.479 | 0.445 | 0:22:12 |


[SAVED] EPOCH: 3 | MCRMSE: 0.4569704234600067

TRAIN_LOOP
AWP_ACTIVATED
[TRAIN F3] EPOCH: 4/4 | STEP: 000/366 | LOSS: 0.11185 (0.11185) | LR: 0.00000493 | TIME: 0:00:04 |
[TRAIN F3] EPOCH: 4/4 | STEP: 040/366 | LOSS: 0.09226 (0.09274) | LR: 0.00000396 | TIME: 0:02:21 |
[TRAIN F3] EPOCH: 4/4 | STEP: 080/366 | LOSS: 0.15399 (0.09414) | LR: 0.00000308 | TIME: 0:04:40 |
[TRAIN F3] EPOCH: 4/4 | STEP: 120/366 | LOSS: 0.07211 (0.09355) | LR: 0.00000230 | TIME: 0:07:01 |
[TRAIN F3] EPOCH: 4/4 | STEP: 160/366 | LOSS: 0.07537 (0.09322) | LR: 0.00000162 | TIME: 0:09:22 |
[TRAIN F3] EPOCH: 4/4 | STEP: 200/366 | LOSS: 0.14274 (0.09382) | LR: 0.00000106 | TIME: 0:11:43 |
[TRAIN F3] EPOCH: 4/4 | STEP: 240/366 | LOSS: 0.15032 (0.09445) | LR: 0.00000061 | TIME: 0:14:03 |
[TRAIN F3] EPOCH: 4/4 | STEP: 280/366 | LOSS: 0.09466 (0.09388) | LR: 0.00000028 | TIME: 0:16:23 |
[TRAIN F3] EPOCH: 4/4 | STEP: 320/366 | LOSS: 0.08324 (0.09397) | LR: 0.00000008 | TIME: 0:18:45 |
[TRAIN F3] EPOCH: 4/4 | STEP: 360/366 | LOSS: 0.09417 (0.09342) | LR: 0.00000000 | TIME: 0:21:06 |
[TRAIN F3] EPOCH: 4/4 | STEP: 365/366 | LOSS: 0.09611 (0.09365) | LR: 0.00000000 | TIME: 0:21:23 |

VALID_LOOP
[VALID F3] EPOCH: 4/4 | STEP: 000/123 | LOSS: 0.11877 (0.11877) | TIME: 0:00:01 |
[VALID F3] EPOCH: 4/4 | STEP: 040/123 | LOSS: 0.06419 (0.09913) | TIME: 0:00:16 |
[VALID F3] EPOCH: 4/4 | STEP: 080/123 | LOSS: 0.13448 (0.10226) | TIME: 0:00:32 |
[VALID F3] EPOCH: 4/4 | STEP: 120/123 | LOSS: 0.07926 (0.10311) | TIME: 0:00:47 |
[VALID F3] EPOCH: 4/4 | STEP: 122/123 | LOSS: 0.03656 (0.10280) | TIME: 0:00:48 |

--------------------
EPOCH: 4/4 SUMMARY
--------------------
| EPOCH   |   TRAIN_LOSS |   VALID_LOSS |   MCRMSE | COLS                                          | TIME    |
|---------|--------------|--------------|----------|-----------------------------------------------|---------|
| 4/4     |      0.09365 |       0.1028 |  0.45461 | 0.492 | 0.454 | 0.418 | 0.447 | 0.479 | 0.438 | 0:22:11 |


[SAVED] EPOCH: 4 | MCRMSE: 0.4546149969100952


----------------------------------- FOLD 3 RESULT -----------------------------------
  MCRMSE    cohesion    syntax    vocabulary    phraseology    grammar    conventions
--------  ----------  --------  ------------  -------------  ---------  -------------
 0.45461     0.49159   0.45414       0.41827        0.44679    0.47891          0.438

################################### END OF FOlD 3 ###################################


